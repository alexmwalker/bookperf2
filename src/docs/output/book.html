<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta http-equiv="Content-Style-Type" content="text/css" />
    <meta name="generator" content="pandoc" />
    <title>Backend Performance: SitePoint</title>

    <link rel="stylesheet" href="../assets/css/book.css" type="text/css" />
</head>

<body>
    <style>
        #cover {
            background: #A8CE35;
            background-size: 100% auto;
            color: #fff;
            font-family: Raleway, sans-serif;
            width: 8in;
            height: 10in;
            position: relative;
        }

        #cover div#cover-logo {
            background: url(../images/svg/sitepoint-white.svg) no-repeat;
            position: absolute;
            top: .75in;
            right: 4em;
            width: 2in;
            height: 0.5in;
        }

        #cover h1.title {
            font-size: 75pt;
            font-weight: normal;
            line-height: 60pt;
            color: #fff;
            letter-spacing: -2px;
            text-align: right;
            text-transform: uppercase;
            margin: 2.2in .2em 0 0;
            position: absolute;
        }

        #cover h3.tagline {
            font-size: 15pt;
            font-weight: 600;
            line-height: 1.5;
            color: #fff;
            position: absolute;
            text-transform: uppercase;
            top: 8.75in;
            width: 7.9in;
            text-align: center;
        }

        #cover span.sizeup {
            font-size: 110%;
        }

        #cover span.sizedown {
            font-size: 80%;
        }

        #cover div#hero {
            position: absolute;
            background: url(../images/svg/hero2.svg) no-repeat;
            top: 4.5in;
            right: 0;
            width: 6in;
            height: 4in;
            margin-right: 3rem;
        }
    </style>


    <section id="cover" class="set-cover">
        <div id="cover-logo"></div>
        <h1 class="title">
            <span class="sizeup">Back-end</span>
            <span class="sizedown">Performance</span>
        </h1>
        <h3 class="tagline">Because Every Second – Every Kilobyte – Matters</h3>
        <div id="hero"></div>
    </section>

    <div class="page-frontmatter front set-front-matter">
        <div class="title-name">
            <h2 id="maintitle">Back-end Performance </h2>
        </div>
        <p>Copyright © 2017 SitePoint Pty. Ltd.</p>
        <ul>
            <li>
                <strong>Product Manager:</strong> Simon Mackie</li>
            <li>
                <strong>English Editor:</strong> Ralph Mason</li>
            <li>
                <strong>Technical Editor:</strong> Darin Dimitrov</li>
            <li>
                <strong>Cover Designer:</strong> Alex Walker</li>
        </ul>
        <h2 id="notice-of-rights">Notice of Rights</h2>
        <p>All rights reserved. No part of this book may be reproduced, stored in a retrieval system or transmitted in any form
            or by any means, without the prior written permission of the publisher, except in the case of brief quotations
            embodied in critical articles or reviews.</p>
        <h2 id="notice-of-liability">Notice of Liability</h2>
        <p>The author and publisher have made every effort to ensure the accuracy of the information herein. However, the information
            contained in this book is sold without warranty, either express or implied. Neither the authors and SitePoint
            Pty. Ltd., nor its dealers or distributors will be held liable for any damages to be caused either directly or
            indirectly by the instructions contained in this book, or by the software or hardware products described herein.</p>
        <h2 id="trademark-notice">Trademark Notice</h2>
        <p>Rather than indicating every occurrence of a trademarked name as such, this book uses the names only in an editorial
            fashion and to the benefit of the trademark owner with no intention of infringement of the trademark.</p>
    </div>
    <div class="page-about front set-front-matter">
        <div class="small-centered">
            <p>
                <img src="../images/sitepoint-gray.svg" alt="SitePoint logo" width="160" />
                <br/> Published by SitePoint Pty. Ltd.</p>
            <p>48 Cambridge Street Collingwood
                <br/> VIC Australia 3066
                <br/> Web: www.sitepoint.com
                <br/> Email: books@sitepoint.com</p>
            <br/> Printed and bound in the United States of America</div>
        <h2 id="about-sitepoint">About SitePoint</h2>
        <p>SitePoint specializes in publishing fun, practical, and easy-to-understand content for web professionals. Visit
            <a class="uri" href="http://www.sitepoint.com/">http://www.sitepoint.com/</a> to access our blogs, books, newsletters, articles, and community forums. You’ll
            find a stack of information on JavaScript, PHP, Ruby, mobile development, design, and more.</p>
    </div>
    <div class="page-toc front set-front-matter">
        <h1 id="table-of-contents">Table of Contents</h1>
        <ul class="toc" id="insert-toc"></ul>
    </div>
    <div class="preface set-front-matter">
        <h1 id="preface">Preface</h1>
        <!-->
        React is a remarkable JavaScript library that's taken the development community by storm. In a nutshell, it's made it easier
        for developers to build interactive user interfaces for web, mobile and desktop platforms. One of its best features
        is its freedom from the problematic bugs inherent in MVC frameworks, where inconsistent views is a recurring problem
        for big projects. Today, thousands of companies worldwide are using React, including big names such as Netflix and
        AirBnB. React has become immensely popular, such that a number of apps have been ported to React --- including WhatsApp,
        Instagram and Dropbox. This book is a collection of articles, selected from SitePoint's [React Hub](https://www.sitepoint.com/javascript/react/),
        that will guide you through your first week with the amazingly flexible library. -->
        <h2 id="who-should-read-this-book-">Who Should Read This Book?</h2>
        <p>This book is for all Back-end Developers that want to build sites and apps that run faster. You’ll need to be familiar
            with HTML and CSS and have a reasonable level of understanding of JavaScript in order to follow the discussion.
            </p>
        <h2 id="conventions-used">Conventions Used</h2>
        <p>You’ll notice that we’ve used certain typographic and layout styles throughout this book to signify different types
            of information. Look out for the following items.</p>
        <h3 id="code-samples">Code Samples</h3>
        <p>Code in this book is displayed using a fixed-width font, like so: </p>
        <pre><code class="lang-html">&lt;h1&gt;A Perfect Summer&#39;s Day&lt;/h1&gt;
  &lt;p&gt;It was a lovely day for a walk in the park.
  The birds were singing and the kids were all back at school.&lt;/p&gt;
  </code></pre>
        <p>Where existing code is required for context, rather than repeat all of it, ⋮ will be displayed:</p>
        <pre><code class="lang-js">function animate() {
    ⋮
  new_variable = &quot;Hello&quot;;
  }
  </code></pre>
        <p>Some lines of code should be entered on one line, but we’ve had to wrap them because of page constraints. An ➥ indicates
            a line break that exists for formatting purposes only, and should be ignored:</p>
        <pre><code class="lang-js">URL.open(&quot;http://www.sitepoint.com/responsive-web-
  ➥design-real-user-testing/?responsive1&quot;);
  </code></pre>
        <h3 class="breakbefore" id="preface-tips">Tips, Notes, and Warnings</h3>
        <div class="box tip">
            <h4>Hey, You!</h4>
            <div class="body">
                <p>Tips provide helpful little pointers.</p>
            </div>
        </div>
        <div class="box note">
            <h4>Ahem, Excuse Me ...</h4>
            <div class="body">
                <p>Notes are useful asides that are related—but not critical—to the topic at hand. Think of them as extra tidbits
                    of information.</p>
            </div>
        </div>
        <div class="box attention">
            <h4>Make Sure You Always ...</h4>
            <div class="body">
                <p>... pay attention to these important points.</p>
            </div>
        </div>
        <div class="box warning">
            <h4>Watch Out!</h4>
            <div class="body">
                <p>Warnings highlight any gotchas that are likely to trip you up along the way.</p>
            </div>
        </div>
    </div>
    <!-- End front matter-->


    <div class="chapter">
        <div class="ch-head">Chapter</div>
        <h1 class="chaptertitle"> How to Optimize MySQL: Indexes, Slow Queries, Configuration</h1>
        <h3 class="author">Bruno Škvorc</h3>
        <p>MySQL is still the world&#39;s most popular relational database, and yet, it&#39;s still the most unoptimized - many
            people leave it at default values, not bothering to investigate further. In this article, we&#39;ll look at some
            MySQL optimization tips we&#39;ve covered previously, and combine them with novelties that came out since.</p>
        <h2 id="configuration-optimization">Configuration Optimization</h2>
        <p>The first - and most skipped! - performance upgrade every user of MySQL should do is tweak the configuration. 5.7
            (the current version) has much better defaults than its predecessors, but it&#39;s still easy to make improvements
            on top of those.</p>
        <p>We&#39;ll assume you&#39;re using a Linux-based host or a good
            <a href="http://www.sitepoint.com/re-introducing-vagrant-right-way-start-php/">Vagrant</a> box like our
            <a href="http://www.sitepoint.com/quick-tip-get-homestead-vagrant-vm-running/">Homestead Improved</a> so your configuration file will be in
            <code>/etc/mysql/my.cnf</code>. It&#39;s possible that your installation will actually load a secondary configuration
            file into that configuration file, so look into that - if the
            <code>my.cnf</code> file doesn&#39;t have much content, the file
            <code>/etc/mysql/mysql.conf.d/mysqld.cnf</code> might.</p>
        <h3 id="editing-configuration">Editing Configuration</h3>
        <p>You&#39;ll need to be comfortable with using the command line. Even if you haven&#39;t been exposed to it yet, now
            is as good a time as any.</p>
        <p>If you&#39;re editing locally on a Vagrant box, you can copy the file out into the main filesystem by copying it
            into the shared folder with
            <code>cp /etc/mysql/my.cnf /home/vagrant/Code</code> and editing it with a regular text editor, then copying it back
            into place when done. Otherwise, use a simple text editor like vim by executing
            <code>sudo vim /etc/mysql/my.cnf</code>.</p>
        <p>
            <em>Note: modify the above path to match the config file&#39;s real location - it&#39;s possible that it&#39;s actually
                in
                <code>/etc/mysql/mysql.conf.d/mysqld.cnf</code>
            </em>
        </p>
        <h3 id="manual-tweaks">Manual Tweaks</h3>
        <p>The following manual tweaks should be made out of the box. As per
            <a href="https://www.percona.com/blog/2016/10/12/mysql-5-7-performance-tuning-immediately-after-installation/">these tips</a>, add this to the config file under the
            <code>[mysqld]</code> section:</p>
        <pre><code class="lang-cnf">innodb_buffer_pool_size = 1G # (adjust value here, 50%-70% of total RAM)
innodb_log_file_size = 256M
innodb_flush_log_at_trx_commit = 1 # may change to 2 or 0
innodb_flush_method = O_DIRECT
</code></pre>
        <ul>
            <li>
                <code>innodb_buffer_pool_size</code> - the buffer pool is a storage area for caching data and indexes in memory.
                It&#39;s used to keep frequently accessed data in memory, and when you&#39;re running a dedicated or virtual
                server where the DB will often be the bottleneck, it makes sense to give this part of your app(s) the most
                RAM. Hence, we give it 50-70% of all RAM. There&#39;s a buffer pool sizing guide available in the
                <a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-buffer-pool-resize.html">MySQL docs</a>.</li>
            <li>the log file size is well explained
                <a href="https://www.percona.com/blog/2016/05/31/what-is-a-big-innodb_log_file_size/">here</a> but in a nutshell it&#39;s how much data to store in a log before wiping it. Note that a log in
                this case is not an error log or something you might be used to, but instead it indicates checkpoint time
                because with MySQL, writes happen in the background but still affect foreground performance. Big log files
                mean better performance because of fewer new and smaller checkpoints being created, but longer recovery time
                in case of a crash (more stuff needs to be re-written to the DB).</li>
            <li>
                <code>innodb_flush_log_at_trx_commit</code> is explained
                <a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_flush_log_at_trx_commit">here</a> and indicates what happens with the log file. With 1 we have the safest setting, because the log
                is flushed to disk after every transaction. With 0 or 2 it&#39;s less ACID, but more performant. The difference
                in this case isn&#39;t big enough to outweigh the stability benefits of the setting of 1.</li>
            <li>
                <code>innodb_flush_method</code> - to top things off in regards to flushing, this gets set to
                <code>O_DIRECT</code> to avoid double-buffering. This should always be done, unless the I/O system is very low
                performance. On most hosted servers like DigitalOcean droplets you&#39;ll have SSDs, so the I/O system will
                be high performance.</li>
        </ul>
        <p>There&#39;s another tool from Percona which can help us find the remaining problems automatically. Note that if we
            had run it without the above manual tweaks, only 1 out of 4 fixes would have been manually identified because
            the other 3 depend on user preference and the app&#39;s environment.</p>
        <p>
            <img src="../images/1509294383superhero-534120_1280.jpg" alt="Superhero speeding">
        </p>
        <h3 id="variable-inspector">Variable Inspector</h3>
        <p>To install the variable inspector on Ubuntu:</p>
        <pre><code class="lang-bash">wget https://repo.percona.com/apt/percona-release_0.1-4.$(lsb_release -sc)_all.deb
sudo dpkg -i percona-release_0.1-4.$(lsb_release -sc)_all.deb
sudo apt-get update
sudo apt-get install percona-toolkit
</code></pre>
        <p>For other systems, follow
            <a href="https://www.percona.com/downloads/percona-toolkit/LATEST/">instructions</a>.</p>
        <p>Then, run the toolkit with:</p>
        <pre><code class="lang-bash">pt-variable-advisor h=localhost,u=homestead,p=secret
</code></pre>
        <p>You should see output not unlike this one:</p>
        <pre><code># WARN delay_key_write: MyISAM index blocks are never flushed until necessary.

# NOTE max_binlog_size: The max_binlog_size is smaller than the default of 1GB.

# NOTE sort_buffer_size-1: The sort_buffer_size variable should generally be left at its default unless an expert determines it is necessary to change it.

# NOTE innodb_data_file_path: Auto-extending InnoDB files can consume a lot of disk space that is very difficult to reclaim later.

# WARN log_bin: Binary logging is disabled, so point-in-time recovery and replication are not possible.
</code></pre>
        <p>None of these are critical, they don&#39;t need to be fixed. The only one we could add would be binary logging for
            replication and snapshot purposes.</p>
        <p>
            <em>Note: the binlog size will default to 1G in newer versions and won&#39;t be noted by PT.</em>
        </p>
        <pre><code class="lang-cnf">max_binlog_size = 1G
log_bin = /var/log/mysql/mysql-bin.log
server-id=master-01
binlog-format = &#39;ROW&#39;
</code></pre>
        <ul>
            <li>the
                <code>max_binlog_size</code> setting determined how large binary logs will be. These are logs that log your transactions
                and queries and make checkpoints. If a transaction is bigger than max, then a log might be bigger than max
                when saved to disk - otherwise, MySQL will keep them at that limit.</li>
            <li>the
                <code>log_bin</code> option enables binary logging altogether. Without it, there&#39;s no snapshotting or replication.
                Note that this can be very strenuous on the disk space. Server ID is a necessary option when activating binary
                logging, so the logs know which server they came from (for replication) and the format is just the way in
                which the logs are written.</li>
        </ul>
        <p>As you can see, the new MySQL has sane defaults that make things nearly production ready. Of course, every app is
            different and has additional custom tweaks applicable.</p>
        <h3 id="mysql-tuner">MySQL Tuner</h3>
        <p>The
            <a href="https://raw.githubusercontent.com/major/MySQLTuner-perl/master/mysqltuner.pl">Tuner</a> will monitor a database in longer intervals (run it once per week or so on a live app) and suggest
            changes based on what it&#39;s seen in the logs.</p>
        <p>Install it by simply downloading it:</p>
        <pre><code class="lang-bash">wget https://raw.githubusercontent.com/major/MySQLTuner-perl/master/mysqltuner.pl
chmod +x mysqltuner.pl
</code></pre>
        <p>Running it with
            <code>./mysqltuner.pl</code> will ask you for admin username and password for the database, and output information
            from the quick scan. For example, here&#39;s my InnoDB section:</p>
        <pre><code class="lang-pl">[--] InnoDB is enabled.
[--] InnoDB Thread Concurrency: 0
[OK] InnoDB File per table is activated
[OK] InnoDB buffer pool / data size: 1.0G/11.2M
[!!] Ratio InnoDB log file size / InnoDB Buffer pool size (50 %): ⤶
    256.0M * 2/1.0G should be equal 25%
[!!] InnoDB buffer pool &lt;= 1G and Innodb_buffer_pool_instances(!=1).
[--] Number of InnoDB Buffer Pool Chunk : 8 for 8 Buffer Pool Instance(s)
[OK] Innodb_buffer_pool_size aligned with Innodb_buffer_pool_chunk_size⤶
 &amp; Innodb_buffer_pool_instances
[OK] InnoDB Read buffer efficiency: 96.65% (19146 hits/ 19809 total)
[!!] InnoDB Write Log efficiency: 83.88% (640 hits/ 763 total)
[OK] InnoDB log waits: 0.00% (0 waits / 123 writes)
</code></pre>
        <p>Again, it&#39;s important to note that this tool should be run once per week or so as the server has been running.
            Once a config value is changed and the server restarted, it should be run a week from that point then. It&#39;s
            a good idea to set up a cronjob to do this for you and send you the results periodically.</p>
        <hr>
        <p>Make sure you restart the mysql server after every configuration change:</p>
        <pre><code class="lang-bash">sudo service mysql restart
</code></pre>
        <h2 id="indexes">Indexes</h2>
        <p>Next up, let&#39;s focus on Indexes - the main pain point of many hobbyist DB admins! Especially those who immediately
            jump into ORMs and are thus never truly exposed to raw SQL.</p>
        <p>
            <em>Note: the terms keys and indexes can be used interchangeably.</em>
        </p>
        <p>You can compare MySQL indexes with the index in a book which lets you easily find the correct page that contains
            the subject you&#39;re looking for. If there weren’t any indexes, you&#39;d have to go through the whole book
            searching for pages that contain the subject.</p>
        <p>As you can imagine, it’s way faster to search by an index than having to go through each page. Therefore, adding
            indexes to your database is in general speeding up your select queries. However, the index also has to be created
            and stored. So the update and insert queries will be slower and it will cost you a bit more disk space. In general,
            you won’t notice the difference with updating and inserting if you have indexed your table correctly and therefore
            it’s advisable to add indexes at the right locations.</p>
        <p>Tables which only contain a few rows don’t really benefit from indexing. You can imagine that searching through 5
            pages is not much slower then first going to the index, getting the page number and then opening that particular
            page.</p>
        <p>So how do we find out which indexes to add, and which types of indexes exist?</p>
        <h3 id="unique-primary-indexes">Unique / Primary Indexes</h3>
        <p>Primary indexes are the main indexes of data which are the default way of addressing them. For a user account, that
            might be a user ID, or a username, even a main email. Primary indexes are unique. Unique indexes are indexes
            that cannot be repeated in a set of data.</p>
        <p>For example, if a user selected a specific username, no one else should be able to take it. Adding a &quot;unique&quot;
            index to the
            <code>username</code> column solves this problem. MySQL will complain if someone else tries to insert a row which has
            a username that already exists.</p>
        <pre><code class="lang-sql">...
ALTER TABLE `users`
ADD UNIQUE INDEX `username` (`username`);
...
</code></pre>
        <p>Primary keys/indexes are usually defined on table creation, and unique indexes are defined after the fact by altering
            the table.</p>
        <p>Both primary keys and unique keys can be made on a single column or multiple columns at once. For example, if you
            want to make sure only one username per country can be defined, you make a unique index on both of those columns,
            like so:</p>
        <pre><code class="lang-sql">    ...
    ALTER TABLE `users`
    ADD UNIQUE INDEX `usercountry` (`username`, `country`),
    ...
</code></pre>
        <p>Unique indexes are put onto columns which you&#39;ll address often. So if the user account is frequently requested
            and you have many user accounts in the database, that&#39;s a good use case.</p>
        <h3 id="regular-indexes">Regular Indexes</h3>
        <p>Regular indexes ease lookup. They&#39;re very useful when you need to find data by a specific column or combination
            of columns fast, but that data doesn&#39;t need to be unique.</p>
        <pre><code class="lang-sql">...
ALTER TABLE `users`
ADD INDEX `usercountry` (`username`, `country`),
...
</code></pre>
        <p>The above would make it faster to search for usernames per country.</p>
        <p>Indexes also help with sorting and grouping speed.</p>
        <h3 id="fulltext-indexes">Fulltext Indexes</h3>
        <p>FULLTEXT indexes are used for full-text searches. Only the InnoDB and MyISAM storage engines support FULLTEXT indexes
            and only for CHAR, VARCHAR, and TEXT columns.</p>
        <p>These indexes are very useful for all the text searching you might need to do. Finding words inside of bodies of
            text is FULLTEXT&#39;s specialty. Use these on posts, comments, descriptions, reviews, etc. if you often allow
            searching for them in your application.</p>
        <h3 id="descending-indexes">Descending Indexes</h3>
        <p>Not a special type, but an alteration. From version 8+, MySQL supports
            <a href="https://dev.mysql.com/doc/refman/8.0/en/descending-indexes.html">Descending indexes</a>, which means it can store indexes in descending order. This can come in handy when you
            have enormous tables that frequently need the last added data first, or prioritize entries that way. Sorting
            in descending order was always possible, but came at a small performance penalty. This further speeds things
            up.</p>
        <pre><code class="lang-sql">CREATE TABLE t (
    c1 INT, c2 INT,
    INDEX idx1 (c1 ASC, c2 ASC),
    INDEX idx2 (c1 ASC, c2 DESC),
    INDEX idx3 (c1 DESC, c2 ASC),
    INDEX idx4 (c1 DESC, c2 DESC)
);
</code></pre>
        <p>Consider applying DESC to an index when dealing with logs written in the database, posts and comments which are loaded
            last to first, and similar.</p>
        <h3 id="helper-tools-explain">Helper Tools: Explain</h3>
        <p>When looking at optimizing queries, the EXPLAIN tool will be priceless. Prefixing a simple query with
            <code>EXPLAIN</code> will process it in a very in-depth manner, analyze indexes in use, and show you the ratio of hits
            and misses. You&#39;ll notice how many rows it had to process to get the results you&#39;re looking for.</p>
        <pre><code class="lang-sql">EXPLAIN SELECT City.Name FROM City
JOIN Country ON (City.CountryCode = Country.Code)
WHERE City.CountryCode = &#39;IND&#39; AND Country.Continent = &#39;Asia&#39;
</code></pre>
        <p>You can further extend this with
            <code>EXTENDED</code>:</p>
        <pre><code class="lang-sql">EXPLAIN SELECT City.Name FROM City
JOIN Country ON (City.CountryCode = Country.Code)
WHERE City.CountryCode = &#39;IND&#39; AND Country.Continent = &#39;Asia&#39;
</code></pre>
        <p>See how to use this and apply the discoveries by reading
            <a href="https://www.sitepoint.com/using-explain-to-write-better-mysql-queries/">this excellent, detailed post</a>.</p>
        <h3 id="helper-tools-percona-for-duplicate-indexes">Helper Tools: Percona for Duplicate Indexes</h3>
        <p>The previously installed Percona Toolkit also has a tool for detecting duplicate indexes, which can come in handy
            when using third party CMSes or just checking if you accidentally added more indexes than needed. For example,
            the default WordPress installation has duplicate indexes in the
            <code>wp_posts</code> table:</p>
        <pre><code class="lang-bash">
pt-duplicate-key-checker h=localhost,u=homestead,p=secret

# ########################################################################
# homestead.wp_posts
# ########################################################################

# Key type_status_date ends with a prefix of the clustered index
# Key definitions:
#   KEY `type_status_date` (`post_type`,`post_status`,`post_date`,`ID`),
#   PRIMARY KEY (`ID`),
# Column types:
#      `post_type` varchar(20) collate utf8mb4_unicode_520_ci not null ⤶
#         default &#39;post&#39;
#      `post_status` varchar(20) collate utf8mb4_unicode_520_ci not null ⤶
#        default &#39;publish&#39;
#      `post_date` datetime not null default &#39;0000-00-00 00:00:00&#39;
#      `id` bigint(20) unsigned not null auto_increment
# To shorten this duplicate clustered index, execute:
ALTER TABLE `homestead`.`wp_posts` DROP INDEX `type_status_date`, ADD INDEX ⤶
`type_status_date` (`post_type`,`post_status`,`post_date`);
</code></pre>
        <p>As you can see by the last line, it also gives you advice on how to get rid of the duplicate indexes.</p>
        <h3 id="helper-tools-percona-for-unused-indexes">Helper Tools: Percona for Unused Indexes</h3>
        <p>Percona can also detect unused indexes. If you&#39;re logging slow queries (see the Bottlenecks section below), you
            can run the tool and it&#39;ll inspect if these logged queries are using the indexes in the tables involved with
            the queries.</p>
        <pre><code class="lang-bash">pt-index-usage /var/log/mysql/mysql-slow.log
</code></pre>
        <p>For detailed usage of this tool, see
            <a href="https://www.percona.com/doc/percona-toolkit/LATEST/pt-index-usage.html">here</a>.</p>
        <h2 id="bottlenecks">Bottlenecks</h2>
        <p>This section will explain how to detect and monitor for bottlenecks in a database.</p>
        <pre><code class="lang-cnf">slow_query_log  = /var/log/mysql/mysql-slow.log
long_query_time = 1
log-queries-not-using-indexes = 1
</code></pre>
        <p>The above should be added to the configuration. It&#39;ll monitor queries that are longer than 1 second, and those
            not using indexes.</p>
        <p>Once this log has some data, you can analyze it for index usage with the aforementioned
            <code>pt-index-usage</code> tool, or the
            <code>pt-query-digest</code> tool which produces results like these:</p>
        <pre><code class="lang-bash">pt-query-digest /var/log/mysql/mysql-slow.log

# 360ms user time, 20ms system time, 24.66M rss, 92.02M vsz
# Current date: Thu Feb 13 22:39:29 2014
# Hostname: *
# Files: mysql-slow.log
# Overall: 8 total, 6 unique, 1.14 QPS, 0.00x concurrency ________________
# Time range: 2014-02-13 22:23:52 to 22:23:59
# Attribute          total     min     max     avg     95%  stddev  median
# ============     ======= ======= ======= ======= ======= ======= =======
# Exec time            3ms   267us   406us   343us   403us    39us   348us
# Lock time          827us    88us   125us   103us   119us    12us    98us
# Rows sent             36       1      15    4.50   14.52    4.18    3.89
# Rows examine          87       4      30   10.88   28.75    7.37    7.70
# Query size         2.15k     153     296  245.11  284.79   48.90  258.32
# ==== ================== ============= ===== ====== ===== ===============
# Profile
# Rank Query ID           Response time Calls R/Call V/M   Item
# ==== ================== ============= ===== ====== ===== ===============
#    1 0x728E539F7617C14D  0.0011 41.0%     3 0.0004  0.00 SELECT blog_article
#    2 0x1290EEE0B201F3FF  0.0003 12.8%     1 0.0003  0.00 SELECT portfolio_item
#    3 0x31DE4535BDBFA465  0.0003 12.6%     1 0.0003  0.00 SELECT portfolio_item
#    4 0xF14E15D0F47A5742  0.0003 12.1%     1 0.0003  0.00 SELECT portfolio_category
#    5 0x8F848005A09C9588  0.0003 11.8%     1 0.0003  0.00 SELECT blog_category
#    6 0x55F49C753CA2ED64  0.0003  9.7%     1 0.0003  0.00 SELECT blog_article
# ==== ================== ============= ===== ====== ===== ===============
# Query 1: 0 QPS, 0x concurrency, ID 0x728E539F7617C14D at byte 736 ______
# Scores: V/M = 0.00
# Time range: all events occurred at 2014-02-13 22:23:52
# Attribute    pct   total     min     max     avg     95%  stddev  median
# ============ === ======= ======= ======= ======= ======= ======= =======
# Count         37       3
# Exec time     40     1ms   352us   406us   375us   403us    22us   366us
# Lock time     42   351us   103us   125us   117us   119us     9us   119us
# Rows sent     25       9       1       4       3    3.89    1.37    3.89
# Rows examine  24      21       5       8       7    7.70    1.29    7.70
# Query size    47   1.02k     261     262  261.25  258.32       0  258.32
# String:
# Hosts        localhost
# Users        *
# Query_time distribution
#   1us
#  10us
# 100us  ################################################################
#   1ms
#  10ms
# 100ms
#    1s
#  10s+
# Tables
#    SHOW TABLE STATUS LIKE &#39;blog_article&#39;\G
#    SHOW CREATE TABLE `blog_article`\G
# EXPLAIN /*!50100 PARTITIONS*/
SELECT b0_.id AS id0, b0_.slug AS slug1, b0_.title AS title2, b0_.excerpt AS ⤶
excerpt3, b0_.external_link AS external_link4, b0_.description AS description5, ⤶
b0_.created AS created6, b0_.updated AS updated7 FROM blog_article b0_ ORDER BY⤶ 
b0_.created DESC LIMIT 10
</code></pre>
        <p>If you&#39;d prefer to analyze these logs by hand, you can do so too - but first you need to export the log into
            a more &quot;analyzable&quot; format. This can be done with:</p>
        <pre><code class="lang-bash">mysqldumpslow /var/log/mysql/mysql-slow.log
</code></pre>
        <p>Additional parameters can further filter data and make sure only important things are exported. For example: the
            top 10 queries sorted by average execution time.</p>
        <pre><code class="lang-bash">mysqldumpslow -t 10 -s at /var/log/mysql/localhost-slow.log
</code></pre>
        <p>For other parameters, see the
            <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqldumpslow.html">docs</a>.</p>
        <h2 id="conclusion">Conclusion</h2>
        <p>In this comprehensive MySQL optimization post we looked at various techniques for making MySQL fly. We dealt with
            configuration optimization, we powered through some indexes, and we got rid of some bottlenecks!</p>
    </div>
    <div class="chapter">
        <div class="ch-head">Chapter</div>
        <h1 class="chaptertitle"> How to Read Big Files with PHP (Without Killing Your Server)</h1>
        <h3 class="author">Chris Pitt</h3>
        <p>It’s not often that we, as PHP developers, need to worry about memory management. The PHP engine does a stellar job
            of cleaning up after us, and the web server model of short-lived execution contexts means even the sloppiest
            code has no long-lasting effects.</p>
        <p>There are rare times when we may need to step outside of this comfortable boundary --- like when we&#39;re trying
            to run Composer for a large project on the smallest VPS we can create, or when we need to read large files on
            an equally small server.</p>
        <p>
            <img src="../images/1509321734fragment-898477_640.jpg" alt="Fragmented terrain">
        </p>
        <p>It’s the latter problem we&#39;ll look at in this tutorial.</p>
        <p>
            <em>The code for this tutorial can be found on
                <a href="https://github.com/sitepoint-editors/sitepoint-performant-reading-of-big-files-in-php">GitHub</a>.</em>
        </p>
        <h2 id="measuring-success">Measuring Success</h2>
        <p>The only way to be sure we’re making any improvement to our code is to measure a bad situation and then compare that
            measurement to another after we’ve applied our fix. In other words, unless we know how much a “solution” helps
            us (if at all), we can’t know if it really is a solution or not.</p>
        <p>There are two metrics we can care about. The first is CPU usage. How fast or slow is the process we want to work
            on? The second is memory usage. How much memory does the script take to execute? These are often inversely proportional
            --- meaning that we can offload memory usage at the cost of CPU usage, and vice versa.</p>
        <p>In an asynchronous execution model (like with multi-process or multi-threaded PHP applications), both CPU and memory
            usage are important considerations. In traditional PHP architecture, these
            <em>generally</em> become a problem when either one reaches the limits of the server.</p>
        <p>
            <em>It&#39;s impractical to measure CPU usage inside PHP. If that’s the area you want to focus on, consider using
                something like
                <code>top</code>, on Ubuntu or macOS. For Windows, consider using the Linux Subsystem, so you can use
                <code>top</code> in Ubuntu.</em>
        </p>
        <p>For the purposes of this tutorial, we’re going to measure memory usage. We’ll look at how much memory is used in
            “traditional” scripts. We’ll implement a couple of optimization strategies and measure those too. In the end,
            I want you to be able to make an educated choice.</p>
        <p>The methods we’ll use to see how much memory is used are:</p>
        <pre><code class="lang-php">// formatBytes is taken from the php.net documentation

memory_get_peak_usage();

function formatBytes($bytes, $precision = 2) {
    $units = array(&quot;b&quot;, &quot;kb&quot;, &quot;mb&quot;, &quot;gb&quot;, &quot;tb&quot;);

    $bytes = max($bytes, 0);
    $pow = floor(($bytes ? log($bytes) : 0) / log(1024));
    $pow = min($pow, count($units) - 1);

    $bytes /= (1 &lt;&lt; (10 * $pow));

    return round($bytes, $precision) . &quot; &quot; . $units[$pow];
}
</code></pre>
        <p>We’ll use these functions at the end of our scripts, so we can see which script uses the most memory at one time.</p>
        <h2 id="what-are-our-options-">What Are Our Options?</h2>
        <p>There are many approaches we could take to read files efficiently. But there are also two likely scenarios in which
            we could use them. We could want to read and process data all at the same time, outputting the processed data
            or performing other actions based on what we read. We could also want to transform a stream of data without ever
            really needing access to the data.</p>
        <p>Let’s imagine, for the first scenario, that we want to be able to read a file and create separate queued processing
            jobs every 10,000 lines. We’d need to keep at least 10,000 lines in memory, and pass them along to the queued
            job manager (whatever form that may take).</p>
        <p>For the second scenario, let’s imagine we want to compress the contents of a particularly large API response. We
            don’t care what it says, but we need to make sure it’s backed up in a compressed form.</p>
        <p>In both scenarios, we need to read large files. In the first, we need to know what the data is. In the second, we
            don’t care what the data is. Let’s explore these options…</p>
        <h2 id="reading-files-line-by-line">Reading Files, Line By Line</h2>
        <p>There are many functions for working with files. Let’s combine a few into a naive file reader:</p>
        <pre><code class="lang-php">// from memory.php

function formatBytes($bytes, $precision = 2) {
    $units = array(&quot;b&quot;, &quot;kb&quot;, &quot;mb&quot;, &quot;gb&quot;, &quot;tb&quot;);

    $bytes = max($bytes, 0);
    $pow = floor(($bytes ? log($bytes) : 0) / log(1024));
    $pow = min($pow, count($units) - 1);

    $bytes /= (1 &lt;&lt; (10 * $pow));

    return round($bytes, $precision) . &quot; &quot; . $units[$pow];
}

print formatBytes(memory_get_peak_usage());

// from reading-files-line-by-line-1.php

function readTheFile($path) {
    $lines = [];
    $handle = fopen($path, &quot;r&quot;);

    while(!feof($handle)) {
        $lines[] = trim(fgets($handle));
    }

    fclose($handle);
    return $lines;
}

readTheFile(&quot;shakespeare.txt&quot;);

require &quot;memory.php&quot;;
</code></pre>
        <p>We’re reading a text file containing the complete works of Shakespeare. The text file is about
            <strong>5.5MB</strong>, and the peak memory usage is
            <strong>12.8MB</strong>. Now, let’s use a generator to read each line:</p>
        <pre><code class="lang-php">// from reading-files-line-by-line-2.php

function readTheFile($path) {
    $handle = fopen($path, &quot;r&quot;);

    while(!feof($handle)) {
        yield trim(fgets($handle));
    }

    fclose($handle);
}

readTheFile(&quot;shakespeare.txt&quot;);

require &quot;memory.php&quot;;
</code></pre>
        <p>The text file is the same size, but the peak memory usage is
            <strong>393KB</strong>. This doesn’t mean anything until we do something with the data we’re reading. Perhaps we can
            split the document into chunks whenever we see two blank lines. Something like this:</p>
        <pre><code class="lang-php">// from reading-files-line-by-line-3.php

$iterator = readTheFile(&quot;shakespeare.txt&quot;);

$buffer = &quot;&quot;;

foreach ($iterator as $iteration) {
    preg_match(&quot;/\n{3}/&quot;, $buffer, $matches);

    if (count($matches)) {
        print &quot;.&quot;;
        $buffer = &quot;&quot;;
    } else {
        $buffer .= $iteration . PHP_EOL;
    }
}

require &quot;memory.php&quot;;
</code></pre>
        <p>Any guesses how much memory we’re using now? Would it surprise you to know that, even though we split the text document
            up into
            <strong>1,216</strong> chunks, we still only use
            <strong>459KB</strong> of memory? Given the nature of generators, the most memory we’ll use is that which we need to
            store the largest text chunk in an iteration. In this case, the largest chunk is
            <strong>101,985</strong> characters.</p>
        <p>
            <em>I’ve already written about the
                <a href="https://www.sitepoint.com/memory-performance-boosts-with-generators-and-nikiciter/">performance boosts of using generators</a> and
                <a href="https://github.com/nikic/iter">Nikita Popov’s Iterator library</a>, so go check that out if you’d like to see more!</em>
        </p>
        <p>Generators have other uses, but this one is demonstrably good for performant reading of large files. If we need to
            work on the data, generators are probably the best way.</p>
        <h2 id="piping-between-files">Piping Between Files</h2>
        <p>In situations where we don’t need to operate on the data, we can pass file data from one file to another. This is
            commonly called
            <strong>piping</strong> (presumably because we don’t see what’s inside a pipe except at each end … as long as it&#39;s
            opaque, of course!). We can achieve this by using stream methods. Let’s first write a script to transfer from
            one file to another, so that we can measure the memory usage:</p>
        <pre><code class="lang-php">// from piping-files-1.php

file_put_contents(
    &quot;piping-files-1.txt&quot;, file_get_contents(&quot;shakespeare.txt&quot;)
);

require &quot;memory.php&quot;;
</code></pre>
        <p>Unsurprisingly, this script uses slightly more memory to run than the text file it copies. That’s because it has
            to read (and keep) the file contents in memory until it has written to the new file. For small files, that may
            be okay. When we start to use bigger files, no so much…</p>
        <p>Let’s try streaming (or piping) from one file to another:</p>
        <pre><code class="lang-php">// from piping-files-2.php

$handle1 = fopen(&quot;shakespeare.txt&quot;, &quot;r&quot;);
$handle2 = fopen(&quot;piping-files-2.txt&quot;, &quot;w&quot;);

stream_copy_to_stream($handle1, $handle2);

fclose($handle1);
fclose($handle2);

require &quot;memory.php&quot;;
</code></pre>
        <p>This code is slightly strange. We open handles to both files, the first in read mode and the second in write mode.
            Then we copy from the first into the second. We finish by closing both files again. It may surprise you to know
            that the memory used is
            <strong>393KB</strong>.</p>
        <p>That seems familiar. Isn’t that what the generator code used to store when reading each line? That’s because the
            second argument to
            <code>fgets</code> specifies how many bytes of each line to read (and defaults to
            <code>-1</code> or until it reaches a new line).</p>
        <p>The third argument to
            <code>stream_copy_to_stream</code> is exactly the same sort of parameter (with exactly the same default).
            <code>stream_copy_to_stream</code> is reading from one stream, one line at a time, and writing it to the other stream.
            It skips the part where the generator yields a value, since we don’t need to work with that value.</p>
        <p>Piping this text isn’t useful to us, so let’s think of other examples which might be. Suppose we wanted to output
            an image from our CDN, as a sort of redirected application route. We could illustrate it with code resembling
            the following:</p>
        <pre><code class="lang-php">// from piping-files-3.php

file_put_contents(
    &quot;piping-files-3.jpeg&quot;, file_get_contents(
        &quot;https://github.com/assertchris/uploads/raw/master/rick.jpg&quot;
    )
);

// ...or write this straight to stdout, if we don&#39;t need the memory info

require &quot;memory.php&quot;;
</code></pre>
        <p>Imagine an application route brought us to this code. But instead of serving up a file from the local file system,
            we want to get it from a CDN. We may substitute
            <code>file_get_contents</code> for something more elegant (like
            <a href="http://docs.guzzlephp.org/en/stable/">Guzzle</a>), but under the hood it’s much the same.</p>
        <p>The memory usage (for this image) is around
            <strong>581KB</strong>. Now, how about we try to stream this instead?</p>
        <pre><code class="lang-php">// from piping-files-4.php

$handle1 = fopen(
    &quot;https://github.com/assertchris/uploads/raw/master/rick.jpg&quot;, &quot;r&quot;
);

$handle2 = fopen(
    &quot;piping-files-4.jpeg&quot;, &quot;w&quot;
);

// ...or write this straight to stdout, if we don&#39;t need the memory info

stream_copy_to_stream($handle1, $handle2);

fclose($handle1);
fclose($handle2);

require &quot;memory.php&quot;;
</code></pre>
        <p>The memory usage is slightly less (at
            <strong>400KB</strong>), but the result is the same. If we didn’t need the memory information, we could just as well
            print to standard output. In fact, PHP provides a simple way to do this:</p>
        <pre><code class="lang-php">$handle1 = fopen(
    &quot;https://github.com/assertchris/uploads/raw/master/rick.jpg&quot;, &quot;r&quot;
);

$handle2 = fopen(
    &quot;php://stdout&quot;, &quot;w&quot;
);

stream_copy_to_stream($handle1, $handle2);

fclose($handle1);
fclose($handle2);

// require &quot;memory.php&quot;;
</code></pre>
        <h3 id="other-streams">Other Streams</h3>
        <p>There are a few other streams we could pipe and/or write to and/or read from:</p>
        <ul>
            <li>
                <code>php://stdin</code> (read-only)</li>
            <li>
                <code>php://stderr</code> (write-only, like php://stdout)</li>
            <li>
                <code>php://input</code> (read-only) which gives us access to the raw request body</li>
            <li>
                <code>php://output</code> (write-only) which lets us write to an output buffer</li>
            <li>
                <code>php://memory</code> and
                <code>php://temp</code> (read-write) are places we can store data temporarily. The difference is that
                <code>php://temp</code> will store the data in the file system once it becomes large enough, while
                <code>php://memory</code> will keep storing in memory until that runs out.</li>
        </ul>
        <h2 id="filters">Filters</h2>
        <p>There’s another trick we can use with streams called
            <strong>filters</strong>. They’re a kind of in-between step, providing a tiny bit of control over the stream data without
            exposing it to us. Imagine we wanted to compress our
            <code>shakespeare.txt</code>. We might use the Zip extension:</p>
        <pre><code class="lang-php">// from filters-1.php

$zip = new ZipArchive();
$filename = &quot;filters-1.zip&quot;;

$zip-&gt;open($filename, ZipArchive::CREATE);
$zip-&gt;addFromString(&quot;shakespeare.txt&quot;, file_get_contents(&quot;shakespeare.txt&quot;));
$zip-&gt;close();

require &quot;memory.php&quot;;
</code></pre>
        <p>This is a neat bit of code, but it clocks in at around
            <strong>10.75MB</strong>. We can do better, with filters:</p>
        <pre><code class="lang-php">// from filters-2.php

$handle1 = fopen(
    &quot;php://filter/zlib.deflate/resource=shakespeare.txt&quot;, &quot;r&quot;
);

$handle2 = fopen(
    &quot;filters-2.deflated&quot;, &quot;w&quot;
);

stream_copy_to_stream($handle1, $handle2);

fclose($handle1);
fclose($handle2);

require &quot;memory.php&quot;;
</code></pre>
        <p>Here, we can see the
            <code>php://filter/zlib.deflate</code> filter, which reads and compresses the contents of a resource. We can then pipe
            this compressed data into another file. This only uses
            <strong>896KB</strong>.</p>
        <p>
            <em>I know this is not the same format, or that there are upsides to making a zip archive. You have to wonder though:
                if you could choose the different format and save 12 times the memory, wouldn’t you?</em>
        </p>
        <p>To uncompress the data, we can run the deflated file back through another zlib filter:</p>
        <pre><code class="lang-php">// from filters-2.php

file_get_contents(
    &quot;php://filter/zlib.inflate/resource=filters-2.deflated&quot;
);
</code></pre>
        <p>Streams have been extensively covered in “
            <a href="https://www.sitepoint.com/%EF%BB%BFunderstanding-streams-in-php/">Understanding Streams in PHP</a>” and “
            <a href="https://www.sitepoint.com/using-php-streams-effectively/">Using PHP Streams Effectively</a>”. If you’d like a different perspective, check those out!</p>
        <h2 id="customizing-streams">Customizing Streams</h2>
        <p>
            <code>fopen</code> and
            <code>file_get_contents</code> have their own set of default options, but these are completely customizable. To define
            them, we need to create a new stream context:</p>
        <pre><code class="lang-php">// from creating-contexts-1.php

$data = join(&quot;&amp;&quot;, [
    &quot;twitter=assertchris&quot;,
]);

$headers = join(&quot;\r\n&quot;, [
    &quot;Content-type: application/x-www-form-urlencoded&quot;,
    &quot;Content-length: &quot; . strlen($data),
]);

$options = [
    &quot;http&quot; =&gt; [
        &quot;method&quot; =&gt; &quot;POST&quot;,
        &quot;header&quot;=&gt; $headers,
        &quot;content&quot; =&gt; $data,
    ],
];

$context = stream_content_create($options);

$handle = fopen(&quot;https://example.com/register&quot;, &quot;r&quot;, false, $context);
$response = stream_get_contents($handle);

fclose($handle);
</code></pre>
        <p>In this example, we’re trying to make a
            <code>POST</code> request to an API. The API endpoint is secure, but we still need to use the
            <code>http</code> context property (as is used for
            <code>http</code> and
            <code>https</code>). We set a few headers and open a file handle to the API. We can open the handle as read-only since
            the context takes care of the writing.</p>
        <p>
            <em>There are loads of things we can customize, so it’s best to check out
                <a href="https://php.net/function.stream-context-create">the documentation</a> if you want to know more.</em>
        </p>
        <h2 id="making-custom-protocols-and-filters">Making Custom Protocols and Filters</h2>
        <p>Before we wrap things up, let’s talk about making custom protocols. If you look at
            <a href="https://php.net/manual/en/class.streamwrapper.php">the documentation</a>, you can find an example class to implement:</p>
        <pre><code class="lang-php">Protocol {
    public resource $context;
    public __construct ( void )
    public __destruct ( void )
    public bool dir_closedir ( void )
    public bool dir_opendir ( string $path , int $options )
    public string dir_readdir ( void )
    public bool dir_rewinddir ( void )
    public bool mkdir ( string $path , int $mode , int $options )
    public bool rename ( string $path_from , string $path_to )
    public bool rmdir ( string $path , int $options )
    public resource stream_cast ( int $cast_as )
    public void stream_close ( void )
    public bool stream_eof ( void )
    public bool stream_flush ( void )
    public bool stream_lock ( int $operation )
    public bool stream_metadata ( string $path , int $option , mixed $value )
    public bool stream_open ( string $path , string $mode , int $options ,
        string &amp;$opened_path )
    public string stream_read ( int $count )
    public bool stream_seek ( int $offset , int $whence = SEEK_SET )
    public bool stream_set_option ( int $option , int $arg1 , int $arg2 )
    public array stream_stat ( void )
    public int stream_tell ( void )
    public bool stream_truncate ( int $new_size )
    public int stream_write ( string $data )
    public bool unlink ( string $path )
    public array url_stat ( string $path , int $flags )
}
</code></pre>
        <p>We’re not going to implement one of these, since I think it is deserving of its own tutorial. There’s a lot of work
            that needs to be done. But once that work is done, we can register our stream wrapper quite easily:</p>
        <pre><code class="lang-php">if (in_array(&quot;highlight-names&quot;, stream_get_wrappers())) {
    stream_wrapper_unregister(&quot;highlight-names&quot;);
}

stream_wrapper_register(&quot;highlight-names&quot;, &quot;HighlightNamesProtocol&quot;);

$highlighted = file_get_contents(&quot;highlight-names://story.txt&quot;);
</code></pre>
        <p>Similarly, it’s also possible to create custom stream filters.
            <a href="https://php.net/manual/en/class.php-user-filter.php">The documentation</a> has an example filter class:</p>
        <pre><code class="lang-php">Filter {
    public $filtername;
    public $params
    public int filter ( resource $in , resource $out , int &amp;$consumed ,
        bool $closing )
    public void onClose ( void )
    public bool onCreate ( void )
    }
</code></pre>
        <p>This can be registered just as easily:</p>
        <pre><code class="lang-php">$handle = fopen(&quot;story.txt&quot;, &quot;w+&quot;);
stream_filter_append($handle, &quot;highlight-names&quot;, STREAM_FILTER_READ);
</code></pre>
        <p>
            <code>highlight-names</code> needs to match the
            <code>filtername</code> property of the new filter class. It’s also possible to use custom filters in a
            <code>php://filter/highligh-names/resource=story.txt</code> string. It&#39;s much easier to define filters than it
            is to define protocols. One reason for this is that protocols need to handle directory operations, whereas filters
            only need to handle each chunk of data.</p>
        <p>If you have the gumption, I strongly encourage you to experiment with creating custom protocols and filters. If you
            can apply filters to
            <code>stream_copy_to_stream</code> operations, your applications are going to use next to no memory even when working
            with obscenely large files. Imagine writing a
            <code>resize-image</code> filter or and
            <code>encrypt-for-application</code> filter.</p>
        <h2 id="summary">Summary</h2>
        <p>Though this isn’t a problem we frequently suffer from, it’s easy to mess up when working with large files. In asynchronous
            applications, it’s just as easy to bring the whole server down when we’re not careful about memory usage.</p>
        <p>This tutorial has hopefully introduced you to a few new ideas (or refreshed your memory about them), so that you
            can think more about how to read and write large files efficiently. When we start to become familiar with streams
            and generators, and stop using functions like
            <code>file_get_contents</code>: an entire category of errors disappear from our applications. That seems like a good
            thing to aim for!</p>
    </div>
    <div class="chapter">
        <div class="ch-head">Chapter</div>
        <h1 class="chaptertitle"> WordPress Optimization</h1>
        <h3 class="author">Tonino Jankov</h3>
        <p>According to
            <a href="https://builtwith.com">Builtwith.com</a>, WordPress
            <a href="https://trends.builtwith.com/cms">holds close to 50%</a> of the CMS share of the world&#39;s top 1,000,000 websites. As for the ecommerce sphere,
            we&#39;re at 33% with WooCommerce. And if we cast a wider net, percentages go higher. Although we may complain
            that WordPress can get bloated, resource-heavy, and its data model leaves a lot to be desired, there is no denying
            that WordPress is everywhere.</p>
        <p>
            <img src="../images/1509925595ecommerce-stats-1024x634.png" alt="Ecommerce software stats by builtwith.com">
        </p>
        <p>WordPress can thank its simplicity and a low barrier to entry for this pervasiveness. It&#39;s easy to set up, and
            requires next to no technical knowledge. Hosting for WordPress can be found for as little as a couple of dollars
            per month, and the basic setup takes just a half hour of clicking. Free themes for WordPress are galore, some
            with included WYSIWYG page builders.</p>
        <p>Many look down on it, but in many ways we can thank WordPress for the growth of the internet and PHP, and many internet
            professionals have WP&#39;s gentle learning curve to thank for their careers.</p>
        <p>But this ease of entry comes at a cost. Plenty of websites that proudly wear the WordPress badge were not done by
            professionals but by the cheapest developers. And often, it shows. Professional look and professional performance
            were afterthoughts.</p>
        <p>One of the main points of feedback the owner of an aspiring high-quality website will get from a grudging professional
            is that performance and a professional look and feel shouldn&#39;t be afterthoughts. You can&#39;t easily paint
            or stick them over a website. Professional websites should be premeditated.</p>
        <p>
            <img src="../images/1509925914lingscars.com_-1024x571.gif" alt="lingscars.com">
        </p>
        <p>A famous UK used car dealer, Ling&#39;s Cars, tried a unique way to make a kitsch marketing punchline. Unless you&#39;re
            REALLY sure about what you&#39;re doing, DO NOT try this at home.</p>
        <p>And this starts with…</p>
        <h2 id="choice-of-hosting">Choice of Hosting</h2>
        <p>Typically, new users will go with products that are on the low-cost side, with most of beginner-friendly bells and
            whistles. Considering the
            <a href="https://www.reviewhell.com/blog/endurance-international-group-eig-hosting/">shady business practices</a> by some big industry players in this arena, and the complaints and demands for site
            migration professionals coming from clients, this is a part of website setup that requires due attention.</p>
        <p>We can divide WordPress hosting vendors into a few tiers.</p>
        <p>Premium, WordPress-dedicated vendors like
            <a href="https://kinsta.com/pricing/">Kinsta</a> whose plans start at $100 per month, or even higher-grade managed hosting like
            <a href="https://vip.wordpress.com/">WordPress VIP</a> by Automattic, may be worth their salt, but also may be out of reach for many website owners.</p>
        <p>Medium tier
            <a href="https://getflywheel.com">Flywheel</a>,
            <a href="https://www.a2hosting.com/">A2 hosting</a>,
            <a href="https://www.siteground.com/">Siteground</a> and
            <a href="https://pantheon.io">Pantheon</a> are among those considered reliable and performance oriented, offering acceptable speed and a managed
            hosting service for those more price-conscious. Users here may get a bit less hand-holding, but these services
            usually strike an acceptable balance between a solid setup, price, and options for more advanced users. Not to
            forget, there is
            <a href="https://www.cloudways.com/en/">Cloudways</a>, which is a hybrid between VPS and managed hosting. Those with their audience in Europe may look
            into
            <a href="https://pilvia.com/pricing/">Pilvia</a>, as it offers a performant server stack and is pretty affordable.</p>
        <p>There&#39;s an interesting survey of customer satisfaction with more prominent hosting vendors,
            <a href="https://www.codeinwp.com/blog/wordpress-hosting-infographic/">published by Codeinwp</a>.</p>
        <p>For those of us not scared of the command line, there are VPS and dedicated-server vendors like
            <a href="https://www.digitalocean.com/">Digital Ocean</a>,
            <a href="https://www.vultr.com/">Vultr</a>,
            <a href="https://www.linode.com/">Linode</a>, Amazon&#39;s
            <a href="https://amazonlightsail.com/">Lightsail</a>,
            <a href="https://www.hetzner.de/">Hetzner</a> in Europe, and
            <a href="https://www.ovh.com/us/">OVH</a>. Hetzner is a German vendor known for its quality physical servers on offer, somewhat above the price
            of virtual servers, while OVH offers very
            <a href="https://www.ovh.com/us/vps/vps-ssd.xml">cost-efficient virtual servers</a>. For the price-conscious, OVH&#39;s subsidiary
            <a href="https://www.kimsufi.com/en/">Kimsufi</a> in Europe and Canada also offers bargain physical dedicated servers, and
            <a href="https://hostus.us/kvm-vps.html">Host US</a> has very affordable virtual servers.</p>
        <p>With managed hosting, things to look for are a good
            <strong>server stack</strong>, good CDN integration, and of course SSD storage. Guaranteed resources,
            <a href="https://www.a2hosting.com/web-hosting/compare">like with A2</a>, are a big plus. The next thing to look for is SSH-access. Tech-savvy users may profit from
            <a href="http://wp-cli.org/">WP-CLI</a> availability.</p>
        <p>When choosing a VPS, the thing to look for is XEN or KVM virtualization over OpenVZ, because it mitigates the overselling
            of resources, helping guarantee that the resources you bought are really yours. It also provides better security.</p>
        <p>
            <a href="https://easyengine.io/">Easy Engine</a> is software that can make your entire VPS/WordPress installation a one-hour job.</p>
        <p>Regarding the server stack, Nginx is preferred to Apache if we&#39;re pursuing performance, and PHP 7 is a must.
            If we really need Apache, using Nginx as a reverse proxy is a plus, but this setup can get complex.</p>
        <p>
            <a href="https://www.cloudways.com/blog/php-5-6-vs-php-7-symfony-benchmarks/">Tests performed</a> give PHP 7 a big edge over the previous version.
            <a href="http://blogs.fasthosts.co.uk/web-design/php-7-performance-improvements/">According to fasthosts.co.uk</a>:</p>
        <blockquote>
            <p>WordPress 4.1 executed 95% more requests per second on PHP 7 compared to PHP 5.6.</p>
        </blockquote>
        <p>When choosing your hosting, be aware of negative experiences with some providers that have
            <a href="https://www.reviewhell.com/blog/endurance-international-group-eig-hosting/">become notorious</a>.</p>
        <h2 id="software-considerations">Software Considerations</h2>
        <p>Things that usually slow down WordPress websites are bulky, bloated front ends with a lot of static resources and
            database queries. These issues arise from the choice of theme (and its page builders, huge sliders, etc) ---
            which not only slow down initial loading due to many requests and overall size, but often slow down the browser
            due to a lot of JavaScript, and stuff to render, making it unresponsive.</p>
        <p>The golden rule here is:
            <strong>don&#39;t use it unless there&#39;s a good reason to.</strong>
        </p>
        <p>This may seem like a rule coming from the mouth of Homer Simpson, but if you can skip any of the bells and whistles,
            do so. Be conservative. If you must add some shiny functionality or JS eye candy, always prefer those tailored
            and coded as specifically as possible for your exact need. If you&#39;re a skilled coder, and the project justifies
            the effort, code it yourself with minimalism in mind.</p>
        <p>Review all the plugins your website can&#39;t live without --- and remove the others.</p>
        <p>And most importantly:
            <strong>back up your website before you begin pruning!</strong>
        </p>
        <h3 id="data-model">Data model</h3>
        <p>If you have a theme where you use a lot of custom posts or fields, be warned that a lot of these will slow down your
            database queries. Keep your data model as simple as possible, and if not, consider that WordPress&#39; original
            intended purpose was as a blogging engine. If you need a lot more than that, you may want to consider some of
            the MVC web frameworks out there that will give you greater control over your data model and the choice of database.</p>
        <p>In WordPress
            <strong>we can</strong> build a rich custom data model by using
            <a href="https://codex.wordpress.org/Post_Types">custom post types</a>,
            <a href="https://codex.wordpress.org/Taxonomies">custom taxonomies</a> and
            <a href="https://codex.wordpress.org/Custom_Fields">custom fields</a>, but be conscious of performance and complexity costs.</p>
        <p>If you know your way around the code, inspect your theme to find unnecessary database queries. Every individual database
            trip spends precious milliseconds in your
            <a href="https://en.wikipedia.org/wiki/Time_to_first_byte">TTFB</a>, and megabytes of your server&#39;s memory. Remember that
            <a href="https://wordpress.stackexchange.com/questions/110845/what-exactly-defines-a-main-loop-and-a-secondary-loop">secondary loops</a> can be costly --- so be warned when using widgets and plugins that show extra posts, like
            in sliders or widget areas. If you must use them,
            <a href="https://code.tutsplus.com/tutorials/how-to-code-multiple-loops-while-only-querying-the-database-once--cms-25703">consider fetching all your posts in a single query</a>, where it may otherwise slow down your website. There&#39;s
            <a href="https://github.com/birgire/wp-combine-queries">a GitHub repo</a> for those not wanting to code from scratch.</p>
        <h3 id="meta-queries-can-be-expensive">Meta queries can be expensive</h3>
        <p>Using
            <a href="https://codex.wordpress.org/Custom_Fields">custom fields</a> to fetch posts by some criteria can be a great tool to develop sophisticated things with WP.
            This is an example of a
            <a href="http://www.epicwebs.co.uk/wordpress-tutorials/how-to-use-wordpress-meta-queries-effectively/">meta query</a>, and
            <a href="https://wordpress.stackexchange.com/questions/276310/slow-meta-query-with-multi-meta-keys">here</a> you can find some elaboration on its costs. Summary:
            <strong>post meta wasn&#39;t built for filtering, taxonomies were</strong>.</p>
        <p>
            <code>get_post_meta</code>
            <a href="https://developer.wordpress.org/reference/functions/get_post_meta/">is a function</a> typically called to fetch custom fields, and it can be called with just the post ID as an argument,
            in which case it fetches
            <strong>all</strong> the post&#39;s meta fields in an array, or it can have a custom field&#39;s name as a second argument,
            in which case it returns just the specified field.</p>
        <p>If using
            <code>get_post_meta()</code>for a certain post multiple times on a single page or request (for multiple custom fields),
            be aware that
            <a href="https://wordpress.stackexchange.com/a/167151/73217">this
                <em>won&#39;t</em> incur extra cost</a>, because the first time this function is called,
            <a href="https://core.trac.wordpress.org/browser/tags/4.0/src/wp-includes/meta.php#L474">
                <em>all</em> the post meta gets cached</a>.</p>
        <h3 id="database-hygiene">Database hygiene</h3>
        <p>Installing and deleting various plugins, and changing different themes over the lifetime of your website, often clutters
            your database with
            <em>a lot</em> of data that isn&#39;t needed. It&#39;s completely possible to discover --- upon inspecting why a
            WordPress website is sluggish, or why it won&#39;t even load, due to exhausted server memory --- that the database
            has grown to hundreds and hundreds of megabytes, or over a gigabyte, with no content that explains it.</p>
        <p>
            <strong>wp-options</strong> is where a lot of orphaned data usually gets left behind. This includes, but is not limited
            to, various
            <strong>
                <a href="https://codex.wordpress.org/Transients_API">transients</a>
            </strong> (
            <a href="https://salferrarello.com/delete-transients-plugin-deactivation/">this post</a> warns of best practices regarding deletion of transients in plugins). Transients are a form of
            cache, but as with any other caching, if misused, it can do more harm than good. If your server environment provides
            it,
            <a href="https://developer.wordpress.org/cli/commands/transient/">wp-cli has a command set dedicated to transients management</a>, including deletion. If not, there are
            <a href="https://wordpress.org/plugins/artiss-transient-cleaner/">plugins</a> in the WordPress plugins repo that can delete expired transients, but which offer less control.</p>
        <p>If deleting transients still leaves us with a bloated database without any tangible cause,
            <a href="https://wordpress.org/plugins/wp-sweep/">WP-Sweep</a> is an excellent free tool that can do the job of cleaning up the database. Another one to consider
            is
            <a href="https://wordpress.org/plugins/wp-optimize/">WP Optimize</a>.</p>
        <p>
            <strong>Before doing any kind of database cleanup, it&#39;s strongly recommended that you back up your database!</strong>
        </p>
        <p>One of the plugins that comes in very handy for profiling of the whole WordPress request lifecycle is
            <a href="https://wordpress.org/plugins/debug-objects/">Debug Objects</a>. It offers an inspection of all the transients, shortcodes, classes, styles and scripts, templates
            loaded, db queries, and hooks.</p>
        <p>
            <img src="../images/1509925426debug-objects-1024x349.gif" alt="Debug Objects plugin output">
        </p>
        <p>After ensuring a sane, performance-oriented setup --- considering our server stack in advance, eliminating the possible
            bloat created by theme choice and plugins and widgets overload --- we should try to identify bottlenecks.</p>
        <p>If we test our website in a tool like
            <a href="https://tools.pingdom.com/">Pingdom Speed Test</a>, we&#39;ll get a waterfall chart of all the resources loaded in the request:</p>
        <p>
            <img src="../images/1509925983pingdom-waterfall-1024x404.png" alt="Pingdom Waterfall Chart">
        </p>
        <p>This gives us details about the request-response lifecycle, which we can analyze for bottlenecks. For instance:</p>
        <ul>
            <li>If the pink DNS time above is too big, it could mean we should consider caching our DNS records for a longer
                period. This is done by increasing the
                <strong>TTL</strong> setting in our domain management/registrar dashboard.</li>
            <li>If the
                <strong>SSL</strong> part is taking too long, we may want to consider enabling HTTP/2 to benefit from
                <a href="https://en.wikipedia.org/wiki/Application-Layer_Protocol_Negotiation">ALPN</a>, adjusting our cache-control headers, and finally switching to a CDN service. “
                <a href="https://medium.baqend.com/hosting-lessons-learned-6010992eb257">Web Performance in a Nutshell: HTTP/2, CDNs and Browser Caching</a>” is a thorough article on this topic,
                as is “
                <a href="https://www.keycdn.com/blog/https-performance-overhead/">Analyzing HTTPS Performance Overhead</a>” by KeyCDN.</li>
            <li>
                <strong>Connect</strong>,
                <strong>Send</strong>, and
                <strong>Receive</strong> parts usually depend on network latency, so these can be improved by hosting close to your
                intended audience, making sure your host has a fast uplink, and using a CDN. For these items, you may want
                to consider a
                <a href="https://en.wikipedia.org/wiki/Ping_(networking_utility">ping tool</a>) too (not to be confused with the Pingdom tools mentioned above), to make sure your server
                is responsive.</li>
            <li>The
                <strong>Wait</strong> part --- the yellow part of the waterfall --- is the time your server infrastructure takes
                to produce or return the requested website. If this part takes too much time, you may want to return to our
                previous topics of optimizing the server, WordPress installation, and database stack. Or you may consider
                various layers of caching.</li>
        </ul>
        <p>To get a more extensive testing and hand-holding tips on improving the website, there&#39;s a little command line
            utility called
            <strong>
                <a href="https://github.com/sitespeedio/coach">webcoach</a>
            </strong>. In an environment with NodeJS and npm installed (like
            <a href="http://www.sitepoint.com/quick-tip-get-homestead-vagrant-vm-running/">Homestead Improved</a>), installing it is simple:</p>
        <pre><code class="lang-bash">npm install webcoach -g
</code></pre>
        <p>After it&#39;s been installed, we can get detailed insights and advice on how to improve our website&#39;s various
            aspects, including performance:</p>
        <p>
            <img src="../images/1509926340webcoach.gif" alt="webcoach command">
        </p>
        <h2 id="caching">Caching</h2>
        <p>Caching can make all the difference when managing a WordPress website. There are a few layers and possible ways of
            caching.</p>
        <h3 id="page-caching">Page Caching</h3>
        <p>Page caching is caching of the entire HTML output of a web application.</p>
        <p>If we can, we should try to test the server-level solutions first, like NGINX caching, or Varnish, or caching systems
            offered by managed host vendors like Kinsta, Siteground, and others.</p>
        <p>
            <img src="../images/1509926398sg-optimizer-1024x113.png" alt="Siteground Optimizer">
        </p>
        <p>If this doesn&#39;t turn out to be as helpful as we&#39;d like, we may want to consider plugins like
            <a href="https://wordpress.org/plugins/wp-super-cache/">WP Super Cache</a>,
            <a href="https://wordpress.org/plugins/wp-fastest-cache/">WP Fastest Cache</a>, or the overhauled W3 Total Cache
            <a href="https://github.com/szepeviktor/w3-total-cache-fixed">from GitHub</a>. All of these can improve performance, but usually require some experimenting. Badly configured
            caching solutions can actually harm the site&#39;s performance. W3TC, for example --- at least before the overhaul
            --- is known to be maybe the best free caching solution, doing real wonders … when it works. When it doesn&#39;t,
            it can take your website offline.</p>
        <p>
            <a href="https://wp-rocket.me/pricing/">WP Rocket</a> is known to be maybe the most praised of the premium caching solutions.</p>
        <p>Page caching can improve performance drastically, serving entire websites from RAM, but be aware that it can introduce
            complications if you have a dynamic website with a cart, or parts that depend on cookies or a personalized front
            end. It can serve one user&#39;s UI parts to another user, so it usually needs to be tested before being put
            into production. This especially applies to solutions on non-managed servers, like Varnish or Cloudflare page
            cache.</p>
        <h3 id="fragment-caching">Fragment caching</h3>
        <p>Fragment caching is a solution to think about when dynamic, cookie-dependent websites become hard to cache with a
            full-page approach, or when we are caching Ajax requests. A good introduction is available
            <a href="http://www.sitecrafting.com/blog/speed-up-your-wordpress-sites-with-fragment-caching-transients-api/">here</a>.</p>
        <h3 id="object-cache">Object cache</h3>
        <p>
            <a href="https://www.scalewp.io/object-caching/">Object cache</a> means compiling and storing in-memory all the database queries and PHP objects. Some caching
            plugins try to manage object-cache back ends for us. The back ends used are usually
            <a href="https://www.wptutor.io/web/php/apcu-php-7">APCu</a>, Memcached and Redis. They need to be
            <a href="https://guides.wp-bullet.com/install-apcu-object-cache-for-php7-for-wordpress-ubuntu-16-04/">installed on the server</a>.</p>
        <p>To go deeper into benchmarking our PHP code and the performance of our object caching, a valuable tool which requires
            shell access and
            <a href="http://wp-cli.org/">wp-cli</a> installed is the
            <a href="https://github.com/wp-cli/profile-command">profile command</a>. We can install it with:</p>
        <pre><code class="lang-bash">wp package install git@github.com:wp-cli/profile-command.git
</code></pre>
        <p>or</p>
        <pre><code class="lang-bash">wp package install wp-cli/profile-command
</code></pre>
        <p>
            <em>(You may need to add the
                <code>--allow-root</code> flag, depending on the installation.)</em>
        </p>
        <p>Then we can profile the whole load cycle, or drill down to specific hooks, files and classes, their load time and
            caching ratio.</p>
        <p>
            <img src="../images/1509926450wp-profile-1024x626.gif" alt="wp profile command">
        </p>
        <h3 id="browser-caching">Browser caching</h3>
        <p>Browser caching means forcing visitors&#39; browsers to keep static files in their cache, so they don&#39;t need
            to get them from our server on repeat visits.
            <em>
                <code>cache-control</code>
            </em> and
            <em>
                <code>expires</code>
            </em> headers are used here. Caching plugins often manage browser caching and setting the headers. Technumero
            <a href="https://technumero.com/how-to-leverage-browser-caching-wordpress/">made a guide</a> that goes into more depth.</p>
        <h2 id="static-files">Static Files</h2>
        <p>Static files are images, stylesheets, JS code, fonts, media files, etc. We should make sure we compress them, and
            that we&#39;re leveraging HTTP/2 to serve these files if possible. If our managed hosting doesn&#39;t support
            HTTP/2, or moving our unmanaged VPS to HTTP/2 is out of reach, the easiest way is to implement a CDN into our
            stack. CDNs serve our static files from data centers closest to our audience. This decreases latency and usually
            means taking advantage of their highly tuned infrastructure.</p>
        <p>
            <a href="https://wordpress.org/plugins/autoptimize/">Autooptimize</a> is one plugin that can help manipulate our static assets, and reduce the number of requests,
            concatenating JS and stylesheet files, minifying them and thus shrinking the page output.</p>
        <p>Regarding media files, we should consider compressing/encoding our videos to reduce their size, and serving them
            through providers like YouTube, to reduce the strain on our servers. Cloud storage providers like Amazon S3 are
            another good option. Video hosting is out of scope of this article, but WPMUDEV made
            <a href="https://premium.wpmudev.org/blog/video-wordpress/">a handy guide</a> about the topic.</p>
        <p>Regarding images, these are often too big for the web. Sometimes the only solution that will allow our server to
            breathe --- and might take a lengthy amount of time --- is batch compression through the shell. Imagemagick on
            Linux has a useful
            <code>convert</code> tool that allows us to batch compress our images. This example does it recursively with all JPGs
            in a folder, reducing JPEG quality to 80%, along with other small enhancements, and resizing the images (it should
            be self-explanatory):</p>
        <pre><code class="lang-bash">for file in *.jpg; do convert &quot;${file}&quot; -resize 20% -sharpen 3 ⤶ 
-sharpen 2 -brightness-contrast &quot;0x26&quot; -quality 80 thumb.&quot;${file}&quot;; done
</code></pre>
        <p>WP Bullet has
            <a href="https://guides.wp-bullet.com/category/image-optimization/">two excellent guides to batch compress JPG and PNG files</a>.</p>
        <p>Apart from that, there&#39;s the
            <a href="https://imagify.io/">Imagify</a> service and the accompanying WordPress plugin to reduce image sizes,
            <a href="https://wordpress.org/plugins/ewww-image-optimizer/">EWWW image optimizer</a>, and others…</p>
        <h2 id="other-random-tips">Other random tips</h2>
        <ul>
            <li>
                <p>
                    <strong>Memory</strong>:
                    <a href="https://docs.woocommerce.com/document/increasing-the-wordpress-memory-limit/">make sure your installation has enough</a>.</p>
            </li>
            <li>
                <p>
                    <strong>XML-RPC and login page</strong> can often suffer from automated, scripted brute-force attacks --- even
                    if one isn&#39;t a big fish. Even without breaking in, they can waste the CPU cycles. We should try to
                    stop them on the server-level, before our WordPress installation is loaded. If we don&#39;t need the
                    access to
                    <code>xml-rpc.php</code>, we can put this snippet inside our virtual host block on nginx:</p>
            </li>
        </ul>
        <pre><code class="lang-cnf">location = /xmlrpc.php {    
    deny all;    
}
</code></pre>
        <p>In Apache:</p>
        <pre><code class="lang-cnf">&lt;Files xmlrpc.php&gt;
    deny from all
&lt;/Files&gt;
</code></pre>
        <p>Plugins like
            <a href="https://wordpress.org/plugins/better-wp-security/">iThemes Security</a>,
            <a href="https://wordpress.org/plugins/wps-hide-login/">WPS Hide login</a> and others can help with this and with changing our login page URL.</p>
        <p>If you&#39;re under brute force attacks and you aren&#39;t protected by a CDN like Cloudflare --- or a managed host&#39;s
            protection --- consider a firewall like
            <a href="https://www.digitalocean.com/community/tutorials/how-to-protect-wordpress-with-fail2ban-on-ubuntu-14-04">fail2ban</a> (you should probably have a firewall in place whether under attack or not).</p>
        <ul>
            <li>
                <p>
                    <strong>
                        <a href="https://developer.wordpress.org/plugins/javascript/heartbeat-api/">WordPress Heartbeat</a>
                    </strong>: polling the server while WordPress dashboard is open can slow your server, and make your dashboard
                    unresponsive. Especially if it&#39;s open in multiple browser tabs or by several users. The
                    <a href="https://wordpress.org/plugins/heartbeat-control/">HeartBeat</a> plugin can help solve that.</p>
            </li>
            <li>
                <p>
                    <strong>MAX_INPUT_VARS</strong>: when saving posts with lots of meta fields, or variable products with WooCommerce,
                    we may reach a limit of maximum allowed request variables (variables sent by complex WooCommerce products
                    can go into thousands). This can bring down your server.
                    <a href="https://www.a2hosting.com/kb/developer-corner/php/using-php.ini-directives/php-max-input-vars-directive">Here&#39;s how to fix it</a>.</p>
            </li>
            <li>
                <p>If your WordPress installation with a big database --- especially a WooCommerce installation --- starts having
                    issues with speed that you can&#39;t solve otherwise, consider
                    <a href="https://wordpress.org/plugins/elasticpress/">ElasticPress</a>. Some have had luck with it.</p>
            </li>
            <li>
                <p>If you use WordFence, make sure to turn off the Live View feature. It can bring to a halt even a VPS with
                    couple of gigabytes of memory.</p>
            </li>
            <li>
                <p>If you&#39;re logging access to your website --- particularly in
                    <code>debug.log</code> in your
                    <code>wp-content</code> directory --- mind its size. It can grow to the level of gigabytes and crash your server.</p>
            </li>
            <li>
                <p>If you have system crashes on your server/hosting,
                    <a href="http://www.inmotionhosting.com/support/website/what-is/what-are-core-dumps">core dumps</a> will fill up your storage. Have someone analyze the reasons of those crashes, and then
                    delete those files. You&#39;ll recognize them by patterns that look like
                    <code>core.XXXXXX</code>.</p>
            </li>
        </ul>
        <p>All this said, a repeat warning is in order:
            <strong>before you do any changes to your website, back it up!</strong>
        </p>
        <h2 id="conclusion">Conclusion</h2>
        <p>I hope this compilation of WordPress optimization tips will come in handy. As sites grow in size, the tricks are
            harder and harder to retroactively apply. That&#39;s why it&#39;s best to start early, and strive for a maximum
            effect: apply as many of these techniques
            <em>before</em> you launch, and you&#39;ll not only have a smooth launch, but also a performant app from day 1 ---
            surely a fantastic experience for all new users.</p>
        <p>Make sure you also check out SitePoint&#39;s
            <a href="https://www.sitepoint.com/retrofit-your-website-as-a-progressive-web-app/">PWA guide</a>. Making your WP site as PWA as possible from day 0 will help users install it on the home screens
            of their devices, priming them for repeat visits.</p>
    </div>
    <div class="chapter">
        <div class="ch-head">Chapter</div>
        <h1 class="chaptertitle"> How to Optimize SQL Queries for Faster Sites</h1>
        <h3 class="author">Iain Poulson</h3>
        <p>
            <em>This article was originally published on the
                <a href="https://deliciousbrains.com/sql-query-optimization/">Delicious Brains blog</a>, and is republished here with permission.</em>
        </p>
        <p>You know that a fast site == happier users, improved ranking from Google, and increased conversions. Maybe you even
            think your WordPress site is as fast as it can be: you&#39;ve looked at site performance, from the
            <a href="https://deliciousbrains.com/hosting-wordpress-setup-secure-virtual-server/">best practices of setting up a server</a>, to
            <a href="https://deliciousbrains.com/finding-bottlenecks-wordpress-code/">troubleshooting slow code</a>, and
            <a href="https://deliciousbrains.com/wp-offload-s3/doc/why-use-a-cdn/">offloading your images to a CDN</a>, but is that everything?</p>
        <p>With dynamic, database-driven websites like WordPress, you might still have one problem on your hands: database queries
            slowing down your site.</p>
        <p>In this post, I’ll take you through how to identify the queries causing bottlenecks, how to understand the problems
            with them, along with quick fixes and other approaches to speed things up. I’ll be using an actual query we recently
            tackled that was slowing things down on the customer portal of
            <a href="https://deliciousbrains.com">deliciousbrains.com</a>.</p>
        <h2 id="identification">Identification</h2>
        <p>The first step in fixing slow SQL queries is to find them. Ashley has
            <a href="https://deliciousbrains.com/finding-bottlenecks-wordpress-code/#query-monitor">sung the praises</a> of the debugging plugin
            <a href="https://wordpress.org/plugins/query-monitor/">Query Monitor</a> on the blog before, and it’s the database queries feature of the plugin that really makes it
            an invaluable tool for identifying slow SQL queries. The plugin reports on all the database queries executed
            during the page request. It allows you to filter them by the code or component (the plugin, theme or WordPress
            core) calling them, and highlights duplicate and slow queries:</p>
        <p>
            <img src="../images/1510526259query_monitor-1440x494-1024x351.png" alt="Query Monitor results">
        </p>
        <p>If you don’t want to install a debugging plugin on a production site (maybe you’re worried about adding some performance
            overhead) you can opt to turn on the
            <a href="https://dev.mysql.com/doc/refman/5.7/en/slow-query-log.html">MySQL Slow Query Log</a>, which logs all queries that take a certain amount of time to execute. This is relatively
            simple to
            <a href="https://www.a2hosting.co.uk/kb/developer-corner/mysql/enabling-the-slow-query-log-in-mysql">configure</a> and
            <a href="https://dev.mysql.com/doc/refman/5.7/en/log-destinations.html">set up</a> where to log the queries to. As this is a server-level tweak, the performance hit will be less that
            a debugging plugin on the site, but should be turned off when not using it.</p>
        <h2 id="understanding">Understanding</h2>
        <p>Once you have found an expensive query that you want to improve, the next step is to try to understand what is making
            the query slow. Recently during development to our site, we found a query that was taking around 8 seconds to
            execute!</p>
        <pre><code class="lang-sql">SELECT
l.key_id,
    l.order_id,
    l.activation_email,
    l.licence_key,
    l.software_product_id,
    l.software_version,
    l.activations_limit,
    l.created,
    l.renewal_type,
    l.renewal_id,
    l.exempt_domain,
    s.next_payment_date,
    s.status,
    pm2.post_id AS &#39;product_id&#39;,
    pm.meta_value AS &#39;user_id&#39;
FROM
    oiz6q8a_woocommerce_software_licences l
        INNER JOIN
    oiz6q8a_woocommerce_software_subscriptions s ON s.key_id = l.key_id
        INNER JOIN
    oiz6q8a_posts p ON p.ID = l.order_id
        INNER JOIN
    oiz6q8a_postmeta pm ON pm.post_id = p.ID
        AND pm.meta_key = &#39;_customer_user&#39;
        INNER JOIN
    oiz6q8a_postmeta pm2 ON pm2.meta_key = &#39;_software_product_id&#39;
        AND pm2.meta_value = l.software_product_id
WHERE
    p.post_type = &#39;shop_order&#39;
        AND pm.meta_value = 279
ORDER BY s.next_payment_date
</code></pre>
        <p>We use WooCommerce and a customized version of the WooCommerce Software Subscriptions plugin to run our plugins store.
            The purpose of this query is to get all subscriptions for a customer where we know their customer number. WooCommerce
            has a somewhat complex data model, in that even though an order is stored as a custom post type, the id of the
            customer (for stores where each customer gets a WordPress user created for them) is not stored as the
            <code>post_author</code>, but instead as a piece of post meta data. There are also a couple of joins to custom tables
            created by the software subscriptions plugin. Let’s dive in to understand the query more.</p>
        <h3 id="mysql-is-your-friend">MySQL is your Friend</h3>
        <p>MySQL has a handy statement
            <a href="https://dev.mysql.com/doc/refman/5.7/en/describe.html">
                <code>DESCRIBE</code>
            </a> which can be used to output information about a table’s structure such as its columns, data types, defaults.
            So if you execute
            <code>DESCRIBE wp_postmeta;</code> you will see the following results:</p>
        <table>

            <thead>

                <tr>

                    <th>Field</th>

                    <th>Type</th>

                    <th>Null</th>

                    <th>Key</th>

                    <th>Default</th>

                    <th>Extra</th>

                </tr>

            </thead>

            <tbody>

                <tr>

                    <td>meta_id</td>

                    <td>bigint(20) unsigned</td>

                    <td>NO</td>

                    <td>PRI</td>

                    <td>NULL</td>

                    <td>auto_increment</td>

                </tr>

                <tr>

                    <td>post_id</td>

                    <td>bigint(20) unsigned</td>

                    <td>NO</td>

                    <td>MUL</td>

                    <td>0</td>

                </tr>

                <tr>

                    <td>meta_key</td>

                    <td>varchar(255)</td>

                    <td>YES</td>

                    <td>MUL</td>

                    <td>NULL</td>

                </tr>

                <tr>

                    <td>meta_value</td>

                    <td>longtext</td>

                    <td>YES</td>

                    <td>NULL</td>

                </tr>

            </tbody>

        </table>

        <p>That’s cool, but you may already know about it. But did you know that the
            <code>DESCRIBE</code> statement prefix can actually be used on
            <code>SELECT</code>,
            <code>INSERT</code>,
            <code>UPDATE</code>,
            <code>REPLACE</code> and
            <code>DELETE</code> statements? This is more commonly known by its synonym
            <a href="https://dev.mysql.com/doc/refman/5.7/en/explain.html">
                <code>EXPLAIN</code>
            </a> and will give us detailed information about how the statement will be executed.</p>
        <p>Here are the results for our slow query:</p>
        <table>

            <thead>

                <tr>

                    <th>id</th>

                    <th>select_type</th>

                    <th>table</th>

                    <th>type</th>

                    <th>possible_keys</th>

                    <th>key</th>

                    <th>key_len</th>

                    <th>ref</th>

                    <th>rows</th>

                    <th>Extra</th>

                </tr>

            </thead>

            <tbody>

                <tr>

                    <td>1</td>

                    <td>SIMPLE</td>

                    <td>pm2</td>

                    <td>ref</td>

                    <td>meta_key</td>

                    <td>meta_key</td>

                    <td>576</td>

                    <td>const</td>

                    <td>28</td>

                    <td>Using where; Using temporary; Using filesort</td>

                </tr>

                <tr>

                    <td>1</td>

                    <td>SIMPLE</td>

                    <td>pm</td>

                    <td>ref</td>

                    <td>post_id,meta_key</td>

                    <td>meta_key</td>

                    <td>576</td>

                    <td>const</td>

                    <td>37456</td>

                    <td>Using where</td>

                </tr>

                <tr>

                    <td>1</td>

                    <td>SIMPLE</td>

                    <td>p</td>

                    <td>eq_ref</td>

                    <td>PRIMARY,type_status_date</td>

                    <td>PRIMARY</td>

                    <td>8</td>

                    <td>deliciousbrainsdev.pm.post_id</td>

                    <td>1</td>

                    <td>Using where</td>

                </tr>

                <tr>

                    <td>1</td>

                    <td>SIMPLE</td>

                    <td>l</td>

                    <td>ref</td>

                    <td>PRIMARY,order_id</td>

                    <td>order_id</td>

                    <td>8</td>

                    <td>deliciousbrainsdev.pm.post_id</td>

                    <td>1</td>

                    <td>Using index condition; Using where</td>

                </tr>

                <tr>

                    <td>1</td>

                    <td>SIMPLE</td>

                    <td>s</td>

                    <td>eq_ref</td>

                    <td>PRIMARY</td>

                    <td>PRIMARY</td>

                    <td>8</td>

                    <td>deliciousbrainsdev.l.key_id</td>

                    <td>1</td>

                    <td>NULL</td>

                </tr>

            </tbody>

        </table>

        <p>At first glance, this isn’t very easy to interpret. Luckily the folks over at SitePoint have put together a
            <a href="https://www.sitepoint.com/using-explain-to-write-better-mysql-queries/">comprehensive guide to understanding the statement</a>.</p>
        <p>The most important column is
            <code>type</code>, which describes how the tables are joined. If you see
            <code>ALL</code> then that means MySQL is reading the whole table from disk, increasing I/O rates and putting load
            on the CPU. This is know as a “full table scan” (more on that later).</p>
        <p>The
            <code>rows</code> column is also a good indication of what MySQL is having to do, as this shows how many rows it has
            looked in to find a result.</p>
        <p>
            <code>Explain</code> also gives us more information we can use to optimize. For example, the pm2 table (wp_postmeta),
            it is telling us we are
            <code>Using filesort</code>, because we are asking the results to be sorted using an
            <code>ORDER BY</code> clause on the statement. If we were also grouping the query we would be adding overhead to the
            execution.</p>
        <h3 id="visual-investigation">Visual Investigation</h3>
        <p>
            <a href="https://www.mysql.com/products/workbench/">MySQL Workbench</a> is another handy, free tool for this type of investigation. For databases running on MySQL
            5.6 and above, the results of
            <code>EXPLAIN</code> can be outputted as JSON, and MySQL Workbench turns that JSON into a visual execution plan of
            the statement:</p>
        <p>
            <img src="../images/1510526322mysql_workbench_visual-1440x922-1024x656.png" alt="MySQl Workbench Visual Results">
        </p>
        <p>It automatically draws your attention to issues by coloring parts of the query by cost. We can see straight away
            that join to the
            <code>wp_woocommerce_software_licences</code> (alias l) table has a serious issue.</p>
        <h2 id="solving">Solving</h2>
        <p>That part of the query is performing a full table scan, which you
            <a href="https://dev.mysql.com/doc/refman/5.7/en/table-scan-avoidance.html">should try to avoid</a>, as it uses a non-indexed column
            <code>order_id</code> as the join between the
            <code>wp_woocommerce_software_licences</code> table to the
            <code>wp_posts</code> table. This is a common issue for slow queries and one that can be solved easily.</p>
        <h3 id="indexes">Indexes</h3>
        <p>
            <code>order_id</code> is a pretty important piece of identifying data in the table, and if we are querying like this
            we really should have an
            <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-indexes.html">index</a> on the column, otherwise MySQL will literally scan each row of the table until it finds the rows needed.
            Let’s add an index and see what that does:</p>
        <pre><code class="lang-sql">CREATE INDEX order_id ON wp_woocommerce_software_licences(order_id)
</code></pre>
        <p>
            <img src="../images/1510526377index_results-1024x75.png" alt="SQL results with index">
        </p>
        <p>Wow, we’ve managed to shave over 5 seconds off the query by adding that index, good job!</p>
        <h3 id="know-your-query">Know your Query</h3>
        <p>Examine the query --- join by join, subquery by subquery. Does it do things it doesn’t need to? Can optimizations
            be made?</p>
        <p>In this case we join the licenses table to the posts table using the
            <code>order_id</code>, all the while restricting the statement to post types of
            <code>shop_order</code>. This is to enforce data integrity to make sure we are only using the correct order records.
            However, it is actually a redundant part of the query. We know that it’s a safe bet that a software license row
            in the table has an
            <code>order_id</code> relating to the WooCommerce order in the posts table, as this is enforced in PHP plugin code.
            Let’s remove the join and see if that improves things:</p>
        <p>
            <img src="../images/1510526429redundant_results-1440x90-1024x64.png" alt="Query results without redundant join">
        </p>
        <p>That’s not a huge saving, but the query is now under 3 seconds.</p>
        <h3 id="cache-all-the-things-">Cache All The Things!</h3>
        <p>If your server doesn’t have
            <a href="https://dev.mysql.com/doc/refman/5.7/en/query-cache.html">MySQL query caching</a> on by default then it is worth turning on. This means MySQL will keep a record of all
            statements executed with the result, and if an identical statement is subsequently executed the cached results
            are returned. The cache doesn’t get stale, as MySQL flushes the cache when tables are changed.</p>
        <p>Query Monitor found our query to be running 4 times on a single page load, and although it’s good to have MySQL query
            caching on, duplicate reads to the database in one request should really be avoided full stop. Static caching
            in your PHP code is a simple and very effective way to solve this issue. Basically you are fetching the results
            of a query from the database the first time it is requested and storing them in static property of a class, and
            then subsequent calls will return the results from the static property:</p>
        <pre><code class="lang-php">class WC_Software_Subscription {

    protected static $subscriptions = array();

    public static function get_user_subscriptions( $user_id ) {
        if ( isset( static::$subscriptions[ $user_id ] ) ) {
            return static::$subscriptions[ $user_id ];
        }

        global $wpdb;

        $sql = &#39;...&#39;;

        $results = $wpdb-&gt;get_results( $sql, ARRAY_A );

        static::$subscriptions[ $user_id ] = $results;

        return $results;
    }
}
</code></pre>
        <p>The cache has a lifespan of the request, more specifically that of the instantiated object. If you are looking at
            persisting query results across requests, then you would need to implement a persistent
            <a href="https://codex.wordpress.org/Class_Reference/WP_Object_Cache">Object Cache</a>. However, your code would need to be responsible for setting the cache, and invalidating the
            cache entry when the underlying data changes.</p>
        <h3 id="thinking-outside-the-box">Thinking Outside the Box</h3>
        <p>There are other approaches we can take to try and speed up query execution that involve a bit more work than just
            tweaking the query or adding an index. One of the slowest parts of our query is the work done to join the tables
            to go from customer id to product id, and we have to do this for every customer. What if we did all that joining
            just once, so we could just grab the customer’s data when we need it?</p>
        <p>You could denormalize the data by creating a table that stores the license data, along with the user id and product
            id for all licenses and just query against that for a specific customer. You would need to rebuild the table
            using
            <a href="https://dev.mysql.com/doc/refman/5.7/en/create-trigger.html">MySQL triggers</a> on
            <code>INSERT/UPDATE/DELETE</code> to the licenses table (or others depending on how the data could change) but this
            would significantly improve the performance of querying that data.</p>
        <p>Similarly, if a number of joins slow down your query in MySQL, it might be quicker to break the query into two or
            more statements and execute them separately in PHP and then collect and filter the results in code. Laravel does
            something similar by
            <a href="https://laravel.com/docs/5.5/eloquent-relationships#eager-loading">eager loading</a> relationships in Eloquent.</p>
        <p>WordPress can be prone to slower queries on the
            <code>wp_posts</code> table, if you have a large amount of data, and many different custom post types. If you are finding
            querying for your post type slow, then consider moving away from the custom post type storage model and to a
            <a href="https://deliciousbrains.com/creating-custom-table-php-wordpress/">custom table</a>.</p>
        <h2 id="results">Results</h2>
        <p>With these approaches to query optimization we managed to take our query down from 8 seconds down to just over 2
            seconds, and reducing the amount of times it was called from 4 to 1. As a note, those query times were recorded
            when running on our development environment and would be quicker on production.</p>
        <p>I hope this has been a helpful guide to tracking down slow queries and fixing them up. Query optimization might seem
            like a scary task, but as soon as you try it out and have some quick wins you’ll start to get the bug and want
            to improve things even further.</p>
    </div>
    <div class="chapter">
        <div class="ch-head">Chapter</div>
        <h1 class="chaptertitle"> HTTP/2: Background, Performance Benefits and Implementations</h1>
        <h3 class="author">Tonino Jankov</h3>
        <p>On top of the infrastructure of the internet --- or the physical network layers --- sits the Internet Protocol, as
            part of the TCP/IP, or transport layer. It&#39;s the fabric underlying all or most of our internet communications.</p>
        <p>A higher level protocol layer that we use on top of this is the
            <em>application layer</em>. On this level, various applications use different protocols to connect and transfer information.
            We have SMTP, POP3, and IMAP for sending and receiving emails, IRC and XMPP for chatting, SSH for remote sever
            access, and so on.</p>
        <p>The best-known protocol among these, which has become synonymous with the use of the internet, is HTTP (hypertext
            transfer protocol). This is what we use to access websites every day. It was devised by Tim Berners-Lee at CERN
            as early as 1989. The specification for version 1.0 was released in 1996 (RFC 1945), and 1.1 in 1999.</p>
        <p>The HTTP specification is maintained by the World Wide Web Consortium, and can be found at
            <a href="http://www.w3.org/standards/techs/HTTP">http://www.w3.org/standards/techs/HTTP</a>.</p>
        <p>The first generation of this protocol --- versions 1 and 1.1 --- dominated the web up until 2015, when HTTP/2 was
            released and the industry --- web servers and browser vendors --- started adopting it.</p>
        <h2 id="http-1">HTTP/1</h2>
        <p>HTTP is a
            <em>stateless</em> protocol, based on a
            <em>request-response</em> structure, which means that the client makes requests to the server, and these requests
            are atomic: any single request isn&#39;t aware of the previous requests. (This is why we use cookies --- to bridge
            the gap between multiple requests in one user session, for example, to be able to serve an authenticated version
            of the website to logged in users.)</p>
        <p>Transfers are typically initiated by the client --- meaning the user&#39;s browser --- and the servers usually just
            respond to these requests.</p>
        <p>We could say that the current state of HTTP is pretty &quot;dumb&quot;, or better, low-level, with lots of &quot;help&quot;
            that needs to be given to the browsers and to the servers on how to communicate efficiently. Changes in this
            arena are not that simple to introduce, with so many existing websites whose functioning depends on backward
            compatibility with any introduced changes. Anything being done to improve the protocol has to be done in a seamless
            way that won&#39;t disrupt the internet.</p>
        <p>In many ways, the current model has become a bottleneck with this strict request-response, atomic, synchronous model,
            and progress has mostly taken the form of hacks, spearheaded often by the industry leaders like Google, Facebook
            etc. The usual scenario, which is being improved on in various ways, is for the visitor to request a web page,
            and when their browser receives it from the server, it parses the HTML and finds other resources necessary to
            render the page, like CSS, images, and JavaScript. As it encounters these resource links, it stops loading everything
            else, and requests specified resources from the server. It doesn&#39;t move a millimeter until it receives this
            resource. Then it requests another, and so on.</p>
        <p>
            <img src="../images/1509358979top-benchmarks.png" alt="Average number of requests in the world&#39;s top websites">
        </p>
        <p>The number of requests needed to load world&#39;s biggest websites is often in couple of hundreds.</p>
        <p>This includes a lot of waiting, and a lot of round trips during which our visitor sees only a white screen or a half-rendered
            website. These are wasted seconds. A lot of available bandwidth is just sitting there unused during these request
            cycles.</p>
        <p>
            <a href="https://www.sitepoint.com/what-is-a-cdn-and-how-does-it-work">CDNs</a> can alleviate a lot of these problems, but even they are nothing but hacks.</p>
        <p>As Daniel Stenberg (one of the people working on HTTP/2 standardization) from Mozilla
            <a href="https://bagder.gitbooks.io/HTTP2-explained/content/en/part2.html">has pointed out</a>, the first version of the protocol is having a hard time fully leveraging the capacity of
            the underlying transport layer, TCP. Users who have been working on optimizing website loading speeds know this
            often requires some creativity, to put it mildly.</p>
        <p>Over time, internet bandwidth speeds have drastically increased, but HTTP/1.1-era infrastructure didn&#39;t utilize
            this fully. It still struggled with issues like
            <a href="https://en.wikipedia.org/wiki/HTTP_pipelining">HTTP pipelining</a> --- pushing more resources over the same TCP connection. Client-side support in browsers
            has been dragging the most, with Firefox and Chrome disabling it by default, or not supporting it at all, like
            IE, Firefox version 54+, etc. This means that even small resources require opening a new TCP connection, with
            all the bloat that goes with it --- TCP handshakes, DNS lookups, latency… And due to
            <a href="https://www.wikiwand.com/en/Head-of-line_blocking">head-of-line blocking</a>, the loading of one resource results in blocking all other resources from loading.</p>
        <p>
            <img src="../images/1510041715http-pipelining.png" alt="HTTP pipelining">
        </p>
        <p>A synchronous, non-pipelined connection vs a pipelined one, showing possible savings in load time.</p>
        <p>Some of the optimization sorcery web developers have to resort to under the HTTP/1 model to optimize their websites
            include
            <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Images/Implementing_image_sprites_in_CSS">image sprites</a>, CSS and JavaScript concatenation, sharding (distributing visitors&#39; requests for resources
            over more than one domain or subdomain), and so on.</p>
        <p>The improvement was due, and it had to solve these issues in a seamless, backward-compatible way so as not to interrupt
            the workings of the existing web.</p>
        <h2 id="spdy">SPDY</h2>
        <p>In 2009, Google announced a project that would become a draft proposal of a new-generation protocol,
            <a href="https://www.chromium.org/spdy/spdy-whitepaper">SPDY</a> (pronounced
            <em>speedy</em>), adding support to Chrome, and pushing it to all of its web services in subsequent years. Then followed
            Twitter and server vendors like Apache, nginx with their support, Node.js, and later came Facebook, WordPress.com,
            and most CDN providers.</p>
        <p>SPDY introduced
            <strong>multiplexing</strong> --- sending multiple resources in parallel, over a single TCP connection. Connections are
            encrypted by default, and data is compressed. First, preliminary tests in the
            <a href="https://www.chromium.org/spdy/spdy-whitepaper">SPDY white paper</a> performed on the top 25 sites showed speed improvements from 27% to over 60%.</p>
        <p>After it proved itself in production,
            <a href="https://bagder.gitbooks.io/HTTP2-explained/content/en/part4.html">SPDY version 3 became basis for the first draft of HTTP/2</a>, made by the Hypertext Transfer Protocol working
            group httpbis in 2015.</p>
        <p>HTTP/2 aims to address the issues ailing the first version of the protocol --- latency issues --- by:</p>
        <ul>
            <li>compressing HTTP headers</li>
            <li>implementing
                <a href="https://en.wikipedia.org/wiki/HTTP/2_Server_Push">server push</a>
            </li>
            <li>
                <a href="https://en.wikipedia.org/wiki/Multiplexing">multiplexing</a> requests over a single connection.</li>
        </ul>
        <p>It also aims to solve head-of-line blocking. The data it transfers is in
            <a href="https://HTTP2.github.io/faq/#why-is-HTTP2-binary">binary format, improving its efficiency</a>, and it requires encryption by default (or at least, this is a requirement
            imposed by major browsers).</p>
        <p>Header compression is performed with the HPACK algorithm,
            <a href="https://blog.cloudflare.com/hpack-the-silent-killer-feature-of-HTTP-2/">solving the vulnerability in SPDY</a>, and reducing web
            <strong>request</strong> sizes by half.</p>
        <p>
            <strong>Server push</strong> is one of the features that aims to solve wasted waiting time, by serving resources to the
            visitor&#39;s browser before the browser requires it. This reduces the round trip time, which is a big bottleneck
            in website optimization.</p>
        <p>Due to all these improvements, the difference in loading time that HTTP/2 brings to the table can be seen on
            <a href="https://imagekit.io/demo/HTTP2-vs-HTTP1">this example page</a> by imagekit.io.</p>
        <p>Savings in loading time become more apparent the more resources a website has.</p>
        <h2 id="how-to-see-if-a-website-is-serving-resources-over-http-2">How to See If a Website Is Serving Resources over HTTP/2</h2>
        <p>In major browsers, like Firefox or Chrome, we can check a website&#39;s support for the HTTP/2 protocol in the inspector
            tool, by opening the
            <em>Network</em> tab and right-clicking the strip above the resources list. Here we can enable the
            <em>Protocol</em> item.</p>
        <p>
            <img src="../images/1509358982thenextweb.enable.short_.gif" alt="enabling protocol inspection in browser&#39;s inspection tool">
        </p>
        <p>Another way is to install a little JavaScript-based tool that allows us to inspect HTTP/2 support through the command
            line (assuming we have Node.js and npm installed):</p>
        <pre><code class="lang-bash">npm install -g is-HTTP2-cli
</code></pre>
        <p>After installation, we should be able to use it like this:</p>
        <pre><code class="lang-bash">is-HTTP2 www.google.com

✓ HTTP/2 supported by www.google.com
Supported protocols: grpc-exp h2 HTTP/1.1
</code></pre>
        <h2 id="implementations">Implementations</h2>
        <p>At the time of writing, all major browsers
            <a href="https://en.wikipedia.org/wiki/Comparison_of_web_browsers#Protocol_support">support HTTP/2</a>, albeit requiring all the HTTP/2 requests be encrypted, which the HTTP/2 specification itself
            doesn&#39;t require.</p>
        <h3 id="servers">Servers</h3>
        <p>
            <strong>Apache 2.4 supports it</strong> with its
            <a href="https://HTTPd.apache.org/docs/2.4/howto/HTTP2.html">mod_HTTP2</a> module which should be production-ready by now. Apache needs to be built with it by adding the
            <code>--enable-HTTP2</code> argument to the
            <code>./configure</code> command. We also need to be sure to have at least version 1.2.1 of the
            <code>libngHTTP2</code> library installed. In the case of the system having trouble finding it, we can provide the
            path to
            <code>./configure</code> by adding
            <code>--with-ngHTTP2=&lt;path&gt;</code>.</p>
        <p>The next step would be to load the module by adding the directive to Apache&#39;s configuration:</p>
        <pre><code>LoadModule HTTP2_module modules/mod_HTTP2.so
</code></pre>
        <p>Then, we would add
            <code>Protocols h2 h2c HTTP/1.1</code> to our virtual host block and reload the server. Apache&#39;s documentation
            warns us of the caveats when enabling HTTP/2:</p>
        <blockquote>
            <p>Enabling HTTP/2 on your Apache Server has impact on the resource consumption and if you have a busy site, you
                may need to consider carefully the implications.</p>
            <p>The first noticeable thing after enabling HTTP/2 is that your server processes will start additional threads.
                The reason for this is that HTTP/2 gives all requests that it receives to its own
                <strong>Worker</strong> threads for processing, collects the results and streams them out to the client.</p>
        </blockquote>
        <p>
            <em>You can read more about the Apache configuration
                <a href="https://HTTPd.apache.org/docs/2.4/howto/HTTP2.html">here</a>.</em>
        </p>
        <p>
            <strong>nginx</strong>
            <a href="https://www.nginx.com/blog/nginx-1-9-5/">has supported HTTP/2 since version 1.9.5</a>, and we enable it by simply adding the
            <em>http2</em> argument to our virtual host specification:</p>
        <pre><code>server {
    listen 443 ssl http2 default_server;

    ssl_certificate    server.crt;
    ssl_certificate_key server.key;
</code></pre>
        <p>Then reload nginx.</p>
        <p>Unfortunately,
            <strong>server push</strong> at the time of writing is not officially implemented, but
            <a href="https://trac.nginx.org/nginx/roadmap">it has been added to development roadmap</a>, scheduled to be released next year. For the more adventurous ones,
            there is an
            <a href="https://github.com/ghedo/HTTP2-push-nginx-module">unofficial nginx module</a> that adds support for HTTP/2 server push.</p>
        <p>
            <strong>LiteSpeed and OpenLiteSpeed</strong>
            <a href="https://www.litespeedtech.com/products/litespeed-web-server/features/HTTP-2-support">also boast</a> support for HTTP/2.</p>
        <p>One caveat
            <strong>before</strong> activating HTTP/2 on the server side is to make sure that we have SSL support. This means that
            all the virtual hosts snippets we mentioned above --- for Apache and for nginx --- need to go into the SSL-version
            virtual host blocks, listening on port 443. Once we have Apache or nginx installed, and we have configured regular
            virtual hosts, getting the LetsEncrypt SSL certificate, and installing it on any of the major Linux distributions
            should be a matter of just couple of lines of code.
            <a href="https://certbot.eff.org/">Certbot</a> is a command-line tool that automates the whole process.</p>
        <h2 id="conclusion">Conclusion</h2>
        <p>In this article, I&#39;ve provided a bird&#39;s-eye overview of HTTP/2, the new and evolving specification of a second-generation
            web protocol.</p>
        <p>The full list of implementations of the new generation of HTTP can be found
            <a href="https://github.com/HTTP2/HTTP2-spec/wiki/Implementations">here</a>.</p>
        <p>For the less tech-savvy, perhaps the shortest path to transitioning to this new protocol would be to simply implement
            a
            <a href="https://www.sitepoint.com/what-is-a-cdn-and-how-does-it-work">CDN</a> into the web stack, as CDNs were among the earliest adopters of HTTP/2.</p>
    </div>
    <div class="chapter">
        <div class="ch-head">Chapter</div>
        <h1 class="chaptertitle"> Apache vs Nginx Performance: Optimization Techniques</h1>
        <h3 class="author">Tonino Jankov</h3>
        <p>Some years ago, the
            <a href="https://httpd.apache.org/">Apache Foundation&#39;s web server</a>, known simply as &quot;Apache&quot;, was so ubiquitous that it became
            synonymous with the term &quot;web server&quot;. Its daemon process on Linux systems has the name
            <em>httpd</em> (meaning simply
            <em>http process</em>) --- and comes preinstalled in major Linux distributions.</p>
        <p>It was initially released in 1995, and, to
            <a href="https://en.wikipedia.org/wiki/Apache_HTTP_Server">quote Wikipedia</a>,
            <em>&quot;it played a key role in the initial growth of the World Wide Web&quot;</em>. It is still the most-used
            web server software
            <a href="https://w3techs.com/blog/entry/fact_20170828">according to W3techs</a>. However, according to those reports which
            <a href="https://w3techs.com/technologies/history_overview/web_server/ms/y">show some trends of the last decade</a> and
            <a href="https://w3techs.com/technologies/comparison/ws-apache,ws-microsoftiis,ws-nginx">comparisons to other solutions</a>, its market share is decreasing. The reports given by
            <a href="https://news.netcraft.com/archives/2017/09/11/september-2017-web-server-survey.html">Netcraft</a> and
            <a href="https://trends.builtwith.com/web-server">Builtwith</a> differ a bit, but all agree on a trending decline of Apache&#39;s market share and the growth of
            Nginx.</p>
        <p>
            <strong>Nginx</strong> --- pronounced
            <em>engine x</em> --- was released in 2004 by
            <a href="https://www.wikiwand.com/en/Igor_Sysoev">Igor Sysoev</a>, with the explicit intent to outperform Apache. Nginx&#39;s website has
            <a href="https://www.nginx.com/blog/nginx-vs-apache-our-view/">an article worth reading</a> which compares these two technologies. At first, it was mostly used as a supplement
            to Apache, mostly for serving static files, but it has been steadily growing, as it has been evolving to deal
            with the full spectrum of web server tasks.</p>
        <p>It is often used as a
            <a href="https://www.nginx.com/resources/admin-guide/reverse-proxy/">reverse proxy</a>,
            <a href="http://nginx.org/en/docs/http/load_balancing.html">load balancer</a>, and for
            <a href="https://www.nginx.com/blog/nginx-caching-guide/">HTTP caching</a>. CDNs and video streaming providers use it to build their content delivery systems where performance
            is critical.</p>
        <p>Apache has been around for a long time, and it has a
            <a href="https://www.wikiwand.com/en/List_of_Apache_modules">big choice of modules</a>. Managing Apache servers is known to be user-friendly.
            <a href="http://howtolamp.com/lamp/httpd/2.4/dso/">Dynamic module loading</a> allows for different modules to be compiled and added to the Apache stack without
            recompiling the main server binary. Oftentimes, modules will be in Linux distro repositories, and after installing
            them through system package managers, they can be gracefully added to the stack with commands like
            <a href="http://manpages.ubuntu.com/manpages/yakkety/man8/a2enmod.8.html">a2enmod</a>. This kind of flexibility has yet to be seen with Nginx. When we look at a
            <a href="http://nginx.org/en/docs/http/ngx_http_v2_module.html">guide for setting up Nginx for HTTP/2</a>, modules are something Nginx needs to be built with --- configured
            for at build-time.</p>
        <p>One other feature that has contributed to Apache&#39;s market rule is the
            <a href="http://www.htaccess-guide.com/">.htaccess file</a>. It is Apache&#39;s silver bullet, which made it a go-to solution for the shared hosting environments,
            as it allows controlling the server configuration on a directory level. Every directory on a server served by
            Apache can have its own
            <code>.htaccess</code> file.</p>
        <p>Nginx not only has no equivalent solution, but
            <a href="https://www.nginx.com/resources/wiki/start/topics/examples/likeapache-htaccess/">discourages</a> such usage due to performance hits.</p>
        <p>
            <img src="../images/1510712027server-share-netcraft.png" alt="Server share stats, by Netcraft">
        </p>
        <p>
            <em>Server vendors market share 1995–2005.
                <a href="http://www.netcraft.com/">Data by Netcraft</a>
            </em>
        </p>
        <p>
            <a href="https://www.litespeedtech.com/products/litespeed-web-server">LiteSpeed</a>, or LSWS, is one server contender that has a level of flexibility that can compare to Apache, while
            not sacrificing performance. It supports Apache-style
            <code>.htaccess</code>,
            <code>mod_security</code> and
            <code>mod_rewrite</code>, and it&#39;s worth considering for shared setups. It was planned as a drop-in replacement
            for Apache, and it works with cPanel and Plesk. It&#39;s been supporting HTTP/2 since 2015.</p>
        <p>
            <a href="https://www.hivelocity.net/kb/what-is-litespeed/">LiteSpeed has three license tiers</a>, OpenLiteSpeed, LSWS Standard and LSWS Enterprise. Standard and Enterprise
            <a href="https://www.interserver.net/tips/kb/litespeed-cache-lscache-details-advantages/">come with an optional</a> caching solution comparable to Varnish, LSCache,
            <a href="https://www.litespeedtech.com/support/wiki/doku.php/litespeed_wiki:cache">which is built into the server itself</a>, and can be controlled, with rewrite rules, in
            <code>.htaccess</code> files (per directory).
            <a href="https://www.litespeedtech.com/products/litespeed-web-server/features/anti-ddos-advances">It also comes with some DDOS-mitigating &quot;batteries&quot; built in</a>. This, along with its event-driven
            architecture, makes it a solid contender, targeting primarily
            <a href="https://www.a2hosting.com/litespeed-hosting">performance-oriented hosting providers</a>, but it could be worth setting up even for smaller servers or websites.</p>
        <h2 id="hardware-considerations">Hardware Considerations</h2>
        <p>When optimizing our system, we cannot emphasize enough giving due attention to our hardware setup. Whichever of these
            solutions we choose for our setup, having enough RAM is critical. When a web server process, or an interpreter
            like PHP, don&#39;t have enough RAM, they start swapping, and swapping effectively means using the hard disk
            to supplement RAM memory. The effect of this is increased latency every time this memory is accessed. This takes
            us to the second point --- the hard disk space. Using fast SSD storage is another critical factor of our website
            speed. We also need to mind the CPU availability, and the physical distance of our server&#39;s data centers
            to our intended audience.</p>
        <p>To dive in deeper into the hardware side of performance tuning,
            <a href="https://blogs.dropbox.com/tech/2017/09/optimizing-web-servers-for-high-throughput-and-low-latency/">Dropbox has a good article</a>.</p>
        <h2 id="monitoring">Monitoring</h2>
        <p>One practical way to monitor our current server stack performance, per process in detail, is
            <a href="http://hisham.hm/htop/">htop</a>, which works on Linux, Unix and macOS, and gives us a colored overview of our processes.</p>
        <p>
            <img src="../images/1510712087htop-1024x667.gif" alt="HTOP">
        </p>
        <p>Other monitoring tools are
            <a href="https://newrelic.com/">New Relic</a>, a premium solution with a comprehensive set of tools, and
            <a href="https://my-netdata.io/">Netdata</a>, an open-source solution which offers great extensibility, fine-grained metrics and a customizable
            web dashboard, suitable for both little VPS systems and monitoring a network of servers. It can send alarms for
            any application or system process via email, Slack, pushbullet, Telegram, Twilio etc.</p>
        <p>
            <img src="../images/1510712717netdata-sm-1024x697.gif" alt="Netdata dashboard">
        </p>
        <p>
            <strong>
                <a href="https://www.cyberciti.biz/faq/how-to-install-and-use-monit-on-ubuntudebian-linux-server/">Monit</a>
            </strong> is another, headless, open-source tool which can monitor the system, and can be configured to alert us, or restart
            certain processes, or reboot the system when some conditions are met.</p>
        <h2 id="testing-the-system">Testing the System</h2>
        <p>
            <a href="https://httpd.apache.org/docs/2.4/programs/ab.html">AB</a> --- Apache Benchmark --- is a simple load-testing tool by Apache Foundation, and
            <a href="https://www.joedog.org/siege-home/">Siege</a> is another load-testing program. This
            <a href="https://kalamuna.atlassian.net/wiki/spaces/KALA/pages/16023587/Testing+With+Apache+Benchmark+and+Siege">article</a> explains how to set them both up, and
            <a href="https://blog.getpolymorph.com/7-tips-for-heavy-load-testing-with-apache-bench-b1127916b7b6">here</a> we have some more advanced tips for AB, while an in-depth look at Siege can be found
            <a href="https://www.sitepoint.com/web-app-performance-testing-siege-plan-test-learn/">here</a>.</p>
        <p>If you prefer a web interface, there is
            <a href="https://locust.io/">Locust</a>, a Python-based tool that comes in very handy for testing website performance.</p>
        <p>
            <img src="../images/1510712839locust-i-1024x699.gif" alt="Locust installation">
        </p>
        <p>After we install Locust, we need to
            <a href="https://docs.locust.io/en/latest/writing-a-locustfile.html">create a locustfile</a> in the directory from which we will launch it:</p>
        <pre><code class="lang-python">    from locust import HttpLocust, TaskSet, task

    class UserBehavior(TaskSet):
        @task(1)
        def index(self):
            self.client.get(&quot;/&quot;)

        @task(2)
        def shop(self):
            self.client.get(&quot;/?page_id=5&quot;)

        @task(3)
        def page(self):
            self.client.get(&quot;/?page_id=2&quot;)

    class WebsiteUser(HttpLocust):
        task_set = UserBehavior
        min_wait = 300
        max_wait = 3000
</code></pre>
        <p>Then we simply launch it from the command line:</p>
        <pre><code class="lang-bash">    locust --host=https://my-website.com
</code></pre>
        <p>One warning with these load-testing tools: they have the effect of a DDoS attack, so it&#39;s recommended you limit
            testing to your own websites.</p>
        <h2 id="tuning-apache">Tuning Apache</h2>
        <h3 id="apache-s-mpm-modules">Apache&#39;s mpm modules</h3>
        <p>Apache dates to 1995 and the early days of the internet, when an accepted way for servers to operate was to spawn
            a new process on each incoming TCP connection and to reply to it. If more connections came in, more worker processes
            were created to handle them. The costs of spawning new processes were high, and Apache developers devised a
            <em>prefork</em> mode, with a pre-spawned number of processes. Embedded dynamic language interpreters within each
            process (like
            <a href="https://stackoverflow.com/questions/2712825/what-is-mod-php">mod_php</a>) were still costly, and
            <a href="https://serverfault.com/questions/823121/why-is-apache-spawning-so-many-processes/823162">server crashes</a> with Apache&#39;s default setups became common. Each process was only able to handle a single
            incoming connection.</p>
        <p>This model is known as _
            <a href="http://httpd.apache.org/docs/2.4/mod/prefork.html">mpm_prefork_module</a>_ within Apache&#39;s
            <a href="https://httpd.apache.org/docs/trunk/mpm.html#dynamic">MPM</a> (Multi-Processing Module) system.
            <a href="http://httpd.apache.org/docs/2.4/mod/prefork.html">According to Apache&#39;s website</a>, this mode requires little configuration, because it is self-regulating,
            and
            <em>most important is that the
                <code>MaxRequestWorkers</code> directive be big enough to handle as many simultaneous requests as you expect to
                receive, but small enough to ensure there&#39;s enough physical RAM for all processes</em>.</p>
        <p>
            <img src="../images/1510712932libapache2-mod-php7-1024x655.gif" alt="libapache2-mod-php7 mpm_prefork HTOP report">
        </p>
        <p>
            <em>A small Locust load test that shows spawning of huge number of Apache processes to handle the incoming traffic.</em>
        </p>
        <p>We may add that this mode is maybe the biggest cause of Apache&#39;s bad name. It can get resource-inefficient.</p>
        <p>Version 2 of Apache brought another two MPMs that try to solve the issues that
            <em>prefork</em> mode has. These are
            <a href="http://httpd.apache.org/docs/2.4/mod/worker.html">
                <em>worker module</em>
            </a>, or _mpm_worker
            <em>module</em>, and
            <a href="http://httpd.apache.org/docs/2.4/mod/event.html">
                <em>event module</em>
            </a>.</p>
        <p>Worker module is not process-based anymore; it&#39;s a hybrid process-thread based mode of operation. Quoting
            <a
                href="http://httpd.apache.org/docs/2.4/mod/worker.html">Apache&#39;s website</a>,</p>
        <blockquote>
            <p>a single control process (the parent) is responsible for launching child processes. Each child process creates
                a fixed number of server threads as specified in the
                <code>ThreadsPerChild</code> directive, as well as a listener thread which listens for connections and passes them
                to a server thread for processing when they arrive.</p>
        </blockquote>
        <p>This mode is more resource efficient.</p>
        <p>2.4 version of Apache brought us the third MPM ---
            <a href="http://httpd.apache.org/docs/2.4/mod/event.html">event module</a>. It is based on worker MPM, and added a separate listening thread that manages dormant keepalive
            connections after the HTTP request has completed. It&#39;s a non-blocking, asynchronous mode with a smaller memory
            footprint. More about version 2.4 improvements
            <a href="https://www.slideshare.net/jimjag/apachecon-2017-whats-new-in-httpd-24">here</a>.</p>
        <p>We have loaded a testing WooCommerce installation with around 1200 posts on a virtual server and tested it on Apache
            2.4 with the default, prefork mode, and mod_php.</p>
        <p>First we tested it with
            <a href="https://packages.debian.org/sid/amd64/libapache2-mod-php7.0">libapache2-mod-php7</a> and mpm_prefork_module at
            <a href="https://tools.pingdom.com">https://tools.pingdom.com</a>:</p>
        <p>
            <img src="../images/1510712991h2-mpm-prefork-1024x325.png" alt="mpm prefork test">
        </p>
        <p>Then, we went for testing the event MPM module.</p>
        <p>We had to add
            <code>multiverse</code> to our
            <code>/etc/apt/sources.list</code>:</p>
        <pre><code class="lang-bash">deb http://archive.ubuntu.com/ubuntu xenial main restricted universe multiverse
deb http://archive.ubuntu.com/ubuntu xenial-updates main restricted universe multiverse
deb http://security.ubuntu.com/ubuntu xenial-security main restricted universe multiverse
deb http://archive.canonical.com/ubuntu xenial partner
</code></pre>
        <p>Then we did
            <code>sudo apt-get update</code>and installed
            <code>libapache2-mod-fastcgi</code> and php-fpm:</p>
        <pre><code class="lang-bash">sudo apt-get install libapache2-mod-fastcgi php7.0-fpm
</code></pre>
        <p>Since php-fpm is a service separate from Apache, it needed a restart:</p>
        <pre><code class="lang-bash">sudo service start php7.0-fpm
</code></pre>
        <p>Then we disabled the prefork module, and enabled the event mode and proxy_fcgi:</p>
        <pre><code class="lang-bash">sudo a2dismod php7.0 mpm_prefork
sudo a2enmod mpm_event proxy_fcgi
</code></pre>
        <p>We added this snippet to our Apache virtual host:</p>
        <pre><code class="lang-cnf">&amp;lt;filesmatch &quot;\.php$&quot;&amp;gt;
    SetHandler &quot;proxy:fcgi://127.0.0.1:9000/&quot;
&amp;lt;/filesmatch&amp;gt;
</code></pre>
        <p>This port needs to be consistent with php-fpm configuration in
            <code>/etc/php/7.0/fpm/pool.d/www.conf</code>. More about the php-fpm setup
            <a href="https://wiki.apache.org/httpd/PHP-FPM">here</a>.</p>
        <p>Then we tuned the mpm_event configuration in
            <code>/etc/apache2/mods-available/mpm_event.conf</code>, keeping in mind that our mini-VPS resources for this test
            were constrained --- so we merely reduced some default numbers. Details about every directive on Apache&#39;s
            <a href="http://httpd.apache.org/docs/current/mod/mpm_common.html">website</a>, and tips specific to the event mpm
            <a href="http://httpd.apache.org/docs/2.4/mod/event.html">here</a>. Keep in mind that started
            <em>servers</em> consume an amount of memory regardless of how busy they are. The
            <code>MaxRequestWorkers</code> directive sets the limit on the number of simultaneous requests allowed: setting
            <code>MaxConnectionsPerChild</code> to a value other than zero is important, because it prevents a possible memory
            leak.</p>
        <pre><code class="lang-cnf">&lt;ifmodule mpm_event_module&gt;
        StartServers              1
        MinSpareThreads          30
        MaxSpareThreads          75
        ThreadLimit              64
        ThreadsPerChild          30
        MaxRequestWorkers        80
        MaxConnectionsPerChild   80
&lt;/ifmodule&gt;
</code></pre>
        <p>Then we restarted the server with
            <code>sudo service apache2 restart</code> (if we change some directives, like ThreadLimit, we will need to stop and
            start the service explicitly, with
            <code>sudo service apache2 stop; sudo service apache2 start</code>).</p>
        <p>Our tests on
            <a href="https://tools.pingdom.com">Pingdom</a> now showed page load time reduced by more than half:</p>
        <p>
            <img src="../images/1510713042h2-proxy-fcgi-edited-1024x324.png" alt="Pingdom test">
        </p>
        <h3 id="other-tips-for-tuning-apache-">Other tips for tuning Apache:</h3>
        <p>
            <strong>Disabling .htaccess</strong>: htaccess allows setting specific configuration for every single directory in our
            server root, without restarting. So, traversing all the directories, looking for the .htaccess files, on every
            request, incurs a performance penalty.</p>
        <p>Quote from the Apache docs:</p>
        <blockquote>
            <p>In general, you should only use
                <code>.htaccess</code> files when you don’t have access to the main server configuration file.
                <em> … in general, use of
                    <code>.htaccess</code> files should be avoided when possible. Any configuration that you would consider putting
                    in a
                    <code>.htaccess</code> file, can just as effectively be made in a
                    <code>&lt;directory&gt;</code> section in your main server configuration file.</em>
            </p>
        </blockquote>
        <p>The solution is to disable it in
            <code>/etc/apache2/apache2.conf</code>:</p>
        <pre><code class="lang-cnf">AllowOverride None
</code></pre>
        <p>If we need it for the specific directories, we can then enable it within sections in our virtual host files:</p>
        <pre><code class="lang-cnf">AllowOverride All
</code></pre>
        <p>Further tips include:</p>
        <ul>
            <li>
                <p>
                    <strong>
                        <a href="http://httpd.apache.org/docs/current/mod/mod_expires.html">Control the browser cache with mod_expires</a>
                    </strong> --- by setting the expires headers.</p>
            </li>
            <li>
                <p>Keep
                    <strong>HostNameLookups</strong> turned off ---
                    <code>HostNameLookups Off</code> is the default since Apache 1.3, but make sure it stays off, because it can
                    incur a performance penalty.</p>
            </li>
            <li>
                <p>
                    <strong>Apache2buddy</strong> is a simple script that we can run and get tips for tuning our system:
                    <code>curl -sL https://raw.githubusercontent.com/richardforth/apache2buddy/master/apache2buddy.pl | perl</code>
                </p>
            </li>
        </ul>
        <p>
            <img src="../images/1510713088apache2buddy-1024x609.gif" alt="apache2buddy">
        </p>
        <h2 id="nginx">Nginx</h2>
        <p>Nginx is an
            <a href="https://www.nginx.com/blog/inside-nginx-how-we-designed-for-performance-scale/">event-driven</a> and non-blocking web server. To quote one poster
            <a href="https://news.ycombinator.com/item?id=8343350">on Hacker News</a>,</p>
        <blockquote>
            <p>Forking processes is incredibly expensive compared to an event loop. Event-based HTTP servers inevitably won.</p>
        </blockquote>
        <p>This statement sparked quite a debate on Hacker News, but from our experience, just switching from a mpm_prefork
            Apache to Nginx can often mean saving the website from crashing. Simple switching to Nginx is very often a cure
            in itself.</p>
        <p>
            <img src="../images/1510713144nginx-arch-1024x610.png" alt="Nginx architecture">
        </p>
        <p>A more thorough visual explanation of Nginx architecture can be found
            <a href="https://www.nginx.com/resources/library/infographic-inside-nginx/">here</a>.</p>
        <h3 id="nginx-settings">Nginx settings</h3>
        <p>Nginx recommends pinning the number of workers to number of PC cores (just like we did with Apache&#39;s mpm_event
            configuration), by setting
            <code>worker_processes</code> to
            <code>auto</code> (default is 1) in
            <code>/etc/nginx/nginx.conf</code>.</p>
        <p>
            <code>worker_connections</code> sets the number of connections every worker process can handle. The default is 512,
            but it can usually be increased.</p>
        <p>
            <strong>
                <a href="https://en.wikipedia.org/wiki/Keepalive">Keepalive connections</a>
            </strong> are a server aspect that impacts performance, which
            <a href="https://www.nginx.com/blog/http-keepalives-and-web-performance/">isn&#39;t usually visible in benchmarks</a>.</p>
        <p>
            <img src="../images/1510713195keepalive-firefox-1024x615.gif" alt="keepalive.cf">
        </p>
        <p>
            <a href="https://www.nginx.com/blog/http-keepalives-and-web-performance/">According to the Nginx website</a>,</p>
        <blockquote>
            <p>HTTP keepalive connections are a necessary performance feature that reduce latency and allow web pages to load
                faster.</p>
        </blockquote>
        <p>Establishing new TCP connections
            <a href="https://en.wikipedia.org/wiki/Handshaking">can be costly</a> --- not to mention when there is
            <em>HTTPS</em> encryption involved. The
            <a href="https://http2.github.io/">HTTP/2</a> protocol mitigates this with its
            <a href="https://en.wikipedia.org/wiki/Multiplexing">multiplexing features</a>. Reusing an existing connection can reduce request times.</p>
        <p>Apache&#39;s mpm_prefork and mpm_worker suffer from concurrency limitations that contrast the keepalive event loop.
            This is somewhat fixed in Apache 2.4, in mpm_event module, and comes as the only, default mode of operation in
            Nginx. Nginx workers can handle thousands of incoming connections simultaneously, and if it&#39;s used as a reverse
            proxy or a load balancer, Nginx then uses a local pool of keepalive connections, without TCP connection overhead.</p>
        <p>
            <a href="http://nginx.org/en/docs/http/ngx_http_core_module.html?&amp;_ga=2.26969269.942121935.1510206018-994710012.1508256997#keepalive_requests">
                <code>keepalive_requests</code>
            </a> is a setting that regulates the number of requests a client can make over a single keepalive connection.
            <a href="http://nginx.org/en/docs/http/ngx_http_core_module.html?&amp;_ga=2.191644834.942121935.1510206018-994710012.1508256997#keepalive_timeout">
                <code>keepalive_timeout</code>
            </a> sets the time an idle keepalive connection stays open.</p>
        <p>
            <a href="http://nginx.org/en/docs/http/ngx_http_upstream_module.html?&amp;_ga=2.203640216.942121935.1510206018-994710012.1508256997#keepalive">
                <code>keepalive</code>
            </a> is a setting pertaining to an Nginx connection to an upstream server --- when it acts as a proxy or load balancer.
            This means the number of idle keepalive upstream connections per worker process.</p>
        <p>Enabling the upstream keepalive connections requires putting these directives into the Nginx main configuration:</p>
        <pre><code class="lang-cnf">proxy_http_version 1.1;
proxy_set_header Connection &quot;&quot;;
</code></pre>
        <p>Nginx upstream connections are managed by
            <a href="http://nginx.org/en/docs/http/ngx_http_upstream_module.html">ngx_http_upstream_module</a>.</p>
        <p>If our front-end application keeps polling our back-end application for updates, increasing the
            <code>keepalive_requests</code> and
            <code>keepalive_timeout</code> will limit the number of connections that need to be established. The
            <code>keepalive</code> directive shouldn&#39;t be too large, to allow for other connections to reach our upstream server.</p>
        <p>The tuning of these settings is done on a per-case basis, and needs to be tested. That is maybe one reason why
            <code>keepalive</code> doesn&#39;t have a default setting.</p>
        <h4 id="using-unix-sockets">Using unix sockets</h4>
        <p>By default, Nginx uses a separate PHP process to which it forwards PHP file requests. In this, it acts as a proxy
            (just like Apache when we set it up with php7.0-fpm).</p>
        <p>Often our virtual host setup with Nginx will look like this:</p>
        <pre><code class="lang-cnf">location ~ \.php$ {
    fastcgi_param REQUEST_METHOD $request_method;
    fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
    fastcgi_pass 127.0.0.1:9000;
}
</code></pre>
        <p>Since FastCGI is a different protocol from HTTP, the first two lines are forwarding some arguments and headers to
            php-fpm, while the third line specifies the way to proxy our request --- over a local network socket.</p>
        <p>This is practical for multi-server setups, since we can also specify remote servers to proxy requests to.</p>
        <p>But if we&#39;re hosting our whole setup on a single system, we should use a Unix socket to connect to the listening
            php process:</p>
        <pre><code class="lang-cnf">fastcgi_pass unix:/var/run/php7.0-fpm.sock;
</code></pre>
        <p>Unix sockets are
            <a href="https://stackoverflow.com/questions/257433/postgresql-unix-domain-sockets-vs-tcp-sockets/257479#257479">considered to have better performance than TCP</a>, and this setup is considered safer. You can find more details
            about this setup in this
            <a href="https://support.rackspace.com/how-to/install-nginx-and-php-fpm-running-on-unix-file-sockets/">article by Rackspace</a>.</p>
        <p>This tip regarding Unix sockets is also applicable for Apache. More details
            <a href="https://wiki.apache.org/httpd/PHP-FPM">here</a>.</p>
        <p>
            <strong>gzip_static</strong>: the accepted wisdom around web server performance is to compress our static assets. This
            often means we&#39;ll try to compromise, and try to compress only files that are above some threshold, because
            compressing resources on the fly, with every request, can be expensive. Nginx has a
            <code>gzip_static</code> directive that allows us to serve gzipped versions of files --- with extension .gz --- instead
            of regular resources:</p>
        <pre><code class="lang-cnf">location /assets {
    gzip_static on;
}
</code></pre>
        <p>This way, Nginx will try to serve
            <code>style.css.gz</code> instead of
            <code>style.css</code> (we need to take care of the gzipping ourselves, in this case).</p>
        <p>This way, the CPU cycles won&#39;t be wasted through on-the-fly compression for every request.</p>
        <h3 id="caching-with-nginx">Caching with Nginx</h3>
        <p>The story about Nginx wouldn&#39;t be complete without mentioning how to cache content. Nginx caching is so efficient
            that many sysadmins don&#39;t think that separate layers for
            <a href="https://en.wikipedia.org/wiki/Web_cache">HTTP caching</a> --- like
            <a href="https://varnish-cache.org/">Varnish</a> --- make much sense. Perhaps it is less elaborate, but
            <em>
                <a href="https://speakerdeck.com/lukasa/simplicity-is-a-feature">simplicity is a feature</a>.</em> Enabling caching with Nginx is rather simple.</p>
        <pre><code class="lang-cnf">proxy_cache_path /path/to/cache levels=1:2 keys_zone=my_cache:10m max_size=10g
    inactive=60m;
</code></pre>
        <p>This is a directive we place in our virtual host file,
            <strong>outside</strong> of the
            <code>server</code> block. The
            <code>proxy_cache_path</code> argument can be any path we want to store our cache.
            <code>levels</code> designates how many levels of directories Nginx should store cached content in. For performance
            reasons, two levels are usually okay. Recursing through the directories can be costly. The
            <code>keys_zone</code> argument is a name for a shared memory zone used for storing the cache keys, and
            <code>10m</code> is room for those keys in memory (10MB is usually enough; this isn&#39;t the room for actual cached
            content).
            <code>max_size</code> is optional, and sets the upper limit for the cached content --- here 10GB. If this isn&#39;t
            specified, it will take up all the available space.
            <code>inactive</code> specifies how long the content can stay in the cache without being requested, before it gets
            deleted by Nginx.</p>
        <p>Having set this up, we would add the following line, with the name of our memory zone to either
            <code>server</code> or
            <code>location</code> block:</p>
        <pre><code class="lang-cnf">proxy_cache my_cache;
</code></pre>
        <p>An extra layer of fault-tolerance with Nginx can be achieved by telling it to serve the items from cache when it
            encounters a server error on the origin, or the upstream server, or when the server is down:</p>
        <pre><code class="lang-cnf">proxy_cache_use_stale error timeout http_500 http_502 http_503 http_504;
</code></pre>
        <p>More details about the
            <code>server</code> or
            <code>location</code> block directives to further tune Nginx caching can be found
            <a href="https://www.nginx.com/blog/nginx-caching-guide/">here</a>.</p>
        <p>
            <code>proxy_cache_*</code> directives are for static assets, but we usually want to cache the dynamic output of our
            web apps --- whether it&#39;s a CMS or something else. In this case, we&#39;ll use the
            <code>fastcgi_cache_*</code> directive instead of
            <code>proxy_cache_*</code>:</p>
        <pre><code class="lang-cnf">fastcgi_cache_path /var/run/nginx-cache levels=1:2 keys_zone=my_cache:10m inactive=60m;
fastcgi_cache_key &quot;$scheme$request_method$host$request_uri&quot;;
fastcgi_cache_use_stale error timeout invalid_header http_500;
fastcgi_ignore_headers Cache-Control Expires Set-Cookie;
add_header NGINX_FASTCGI_CACHE $upstream_cache_status;
</code></pre>
        <p>The last line above will set response headers to inform us whether the content was delivered from the cache or not.</p>
        <p>Then, in our server or location block, we can set some exceptions to caching --- for example, when the query string
            is present in the request URL:</p>
        <pre><code class="lang-cnf">if ($query_string != &quot;&quot;) {
    set $skip_cache 1;
}
</code></pre>
        <p>Also, in our
            <code>\.php</code> block, inside
            <code>server</code>, in case of PHP, we would add something like:</p>
        <pre><code class="lang-cnf">location ~ \.php$ {
    try_files $uri =404;
    include fastcgi_params;

    fastcgi_read_timeout 360s;
    fastcgi_buffer_size 128k;
    fastcgi_buffers 4 256k;
    fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;

    fastcgi_pass unix:/run/php/php7.0-fpm.sock;

        fastcgi_index index.php;
        fastcgi_cache_bypass $skip_cache;
        fastcgi_no_cache $skip_cache;
        fastcgi_cache my_cache;
        fastcgi_cache_valid  60m;
}
</code></pre>
        <p>Above, the
            <code>fastcgi_cache*</code> lines, and
            <code>fastcgi_no_cache</code>, regulate caching and exclusions. Detailed reference of all these directives can be found
            <a href="http://nginx.org/en/docs/dirindex.html">on the Nginx docs website</a>.</p>
        <p>To learn more, the people over at Nginx have provided a
            <a href="https://www.nginx.com/resources/webinars/installing-tuning-nginx/">free webinar</a> on this topic, and there&#39;s a
            <a href="https://www.nginx.com/resources/library/">number of ebooks</a> available.</p>
        <h2 id="conclusion">Conclusion</h2>
        <p>We&#39;ve tried to introduce some techniques that will help us improve our web server&#39;s performance, and the
            theory behind those techniques. But this topic is in no way exhausted: we still haven&#39;t covered reverse-proxy
            setups that consist of both Apache and Nginx, or multi-server setups. Achieving the top results with both these
            servers is a matter of testing and analyzing specific, real-life cases. It&#39;s kind of a never-ending topic.</p>
    </div>
    <div class="chapter">
        <div class="ch-head">Chapter</div>
        <h1 class="chaptertitle"> An In-depth Walkthrough of Supercharging Apps with Blackfire</h1>
        <h3 class="author">Reza Lavaryan</h3>
        <p>No one hates robust and scalable applications, especially when the database is growing quickly, and millions of requests
            need to be served on a daily basis. Profiling is a form of program analysis to measure the time and resources
            consumed by the program. With profiling, we can spot the performance bottlenecks in the code and do something
            about them. There is a variety of profiling tools out there, each taking a different approach.</p>
        <p>
            <img src="../images/1452685677article_thumb-1024x549.jpg" alt="Supercharged development">
        </p>
        <p>There are two main types of profiling methods:
            <strong>Sampling</strong> and
            <strong>Instrumentation</strong>.</p>
        <p>In the sampling approach, the profiler takes samples of the
            <strong>call stack</strong> or the
            <strong>Memory</strong> at certain intervals, and updates its statistics. This method has a lesser effect on performance
            because it doesn’t modify the code at all. However, sampling has its overhead, which can be tuned by increasing
            the sampling frequency.</p>
        <p>In the instrumentation approach, profiling instructions are inserted into the code either by the programmer or automatically
            by the profiler (at bytecode level). This approach has a significant performance impact on the application but
            provides precise details of what exactly is happening in the code at runtime.</p>
        <p>
            <a href="https://blackfire.io/">Blackfire.io</a> is the new generation of web profilers, which takes the automatic instrumentation approach,
            but without imposing a performance impact on our application. It’s been developed by
            <a href="https://sensiolabs.com/">Sensio Labs</a>, the team behind the
            <a href="http://sitepoint.com/tag/symfony">Symfony Framework</a>.</p>
        <p>What makes Blackfire special is that it helps us
            <strong>continuously</strong> test our application’s performance without adding a single line of code.</p>
        <p>We can profile any PHP script using its fancy Google Chrome extension, or its command line tool.</p>
        <p>Blackfire is easy to install as it is supported by many cloud server providers and VM boxes, including Homestead.
            In this tutorial, we’re going to learn how we can use Blackfire to build faster applications. As usual, we’ll
            use
            <a href="http://www.sitepoint.com/quick-tip-get-homestead-vagrant-vm-running/">Homestead Improved</a> to set up our development environment.</p>
        <h2 id="getting-started">Getting Started</h2>
        <p>Once the VM is booted up and we’ve managed to ssh into the system using
            <code>vagrant ssh</code>, we can actually start using Blackfire!</p>
        <p>But wait, first we need to create a Blackfire account
            <a href="https://blackfire.io/signup">here</a>. If we already have one, we can proceed by putting our Blackfire credentials inside
            <code>homestead.yaml</code> file, which is located in the root directory of our Vagrant box.</p>
        <p>To get our Blackfire credentials, we log in to
            <a href="http://blackfire.io">Blackfire</a>, click on the profile photo at top right side of the page, and click on
            <a href="https://blackfire.io/account/credentials">My Credentials</a>.</p>
        <p>The credentials are divided into two categories:
            <code>Client Credentials</code> and
            <code>Server Credentials</code>.</p>
        <p>
            <img src="../images/1452685666figure_01-1024x328.png" alt="My Credentials">
        </p>
        <p>We need to uncomment the Blackfire settings in our
            <code>Homestead.yaml</code> file and put the credentials into place:</p>
        <p>
            <strong>homestead.yml</strong>
        </p>
        <pre><code>blackfire:
    - id: &quot;Server Id here&quot;
        token: &quot;Server token here&quot;
        client-id: &quot;Client Id here&quot;
        client-token: &quot;Client token here&quot;
</code></pre>
        <h2 id="building-blocks-of-blackfire">Building Blocks of Blackfire</h2>
        <p>Blackfire is made of five main components:</p>
        <ul>
            <li>
                <strong>The Probe</strong> is a PHP extension, which instruments the application and collects the performance related
                information (currently works on
                <strong>Linux</strong> and
                <strong>macOS</strong>)</li>
            <li>
                <strong>The Agent</strong> is a server-side daemon that aggregates and forwards the profile information to
                <a href="http://blackfire.io">Blackfire</a>.</li>
            <li>
                <strong>The Companion</strong> is Google Chrome extension used to run the profiler from the browser; it can be installed
                from
                <a href="https://blackfire.io/docs/integrations/chrome">this URL</a>.</li>
            <li>
                <strong>The Client</strong> is the command line equivalent of the Companion, which we use to profile APIs, web services,
                web pages, etc.</li>
            <li>
                <strong>The Web-based Interface</strong> compares and visualizes the profile information in graph diagrams and tabular
                formats.</li>
        </ul>
        <blockquote>
            <p>The Probe, the Agent, the Client are pre-installed if we’re using the Homestead Improved Vagrant box.</p>
        </blockquote>
        <h2 id="some-terms-to-know-before-we-start">Some Terms to Know before We Start</h2>
        <ul>
            <li>
                <p>
                    <strong>Reference Profile:</strong> We usually need to run our first profile as a
                    <strong>reference profile</strong>. This profile will be the performance
                    <strong>baseline</strong> of our application. We can compare any profile with the reference, to measure the performance
                    achievements.</p>
            </li>
            <li>
                <p>
                    <strong>Exclusive Time:</strong> The amount of time spent on a function/method to be executed, without considering
                    the time spent for its external calls.</p>
            </li>
            <li>
                <p>
                    <strong>Inclusive Time:</strong> The total time spent to execute a function including all the external calls.</p>
            </li>
            <li>
                <p>
                    <strong>Hot Paths:</strong> Hot Paths are the parts of our application that were most active during the profile.
                    These could be the parts that consumed more memory or took more CPU time.</p>
            </li>
        </ul>
        <h2 id="profiling-a-script">Profiling a Script</h2>
        <p>In this section, let’s profile a simple PHP script to see how fast or slow it is. To have realistic profile results,
            we’re going to write a small PHP script which contains database interactions and function calls. The script inserts
            1,000 rows of random user data to a database table.</p>
        <h3 id="generating-dummy-data">Generating Dummy Data</h3>
        <p>To generate the dummy data, we use
            <a href="https://github.com/fzaninotto/Faker">Faker</a>, a handy library to generate random data about almost anything. Inside the VM and inside the project
            folder, we install it by executing:</p>
        <pre><code class="lang-bash">composer require fzanintto/faker
</code></pre>
        <p>Next, we create a data provider script which populates a JSON file with dummy data. We’ll use this JSON file within
            our main PHP script.</p>
        <p>Why aren&#39;t we doing this in our main script? If we use
            <code>Faker</code> within our main script, the profile result will also include all the operations of
            <code>Faker</code> library. This will make our profile analysis more complicated, while we need something more bare-bones
            for this tutorial.</p>
        <p>We name the file
            <code>UserProviderJSON.php</code>:</p>
        <pre><code class="lang-php">&lt;?php

require_once(&#39;vendor/autoload.php&#39;);

$num = isset($_GET[&#39;num&#39;]) ? $_GET[&#39;num&#39;] : 1000;
$data = [];

$faker = Faker\Factory::create();

if(!file_exists(&#39;data&#39;)) {
    mkdir(&#39;data&#39;);
}

for ($i = 0; $i &lt; $num; $i++) {
    $data[] = [&#39;name&#39; =&gt; $faker-&gt;name, &#39;email&#39; =&gt; $faker-&gt;email, ⤶
        &#39;city&#39; =&gt; $faker-&gt;city,];
}

file_put_contents(&#39;data/users.json&#39;, json_encode($data));

echo &#39;JSON file generated.&#39;;
</code></pre>
        <p>The data provider script consists of a function which generates an array of dummy data, converts the array to JSON
            format, and saves it as a file.</p>
        <p>We can then run the data provider script with
            <code>php UserProviderJSON.php</code>.</p>
        <p>As a result, a file named
            <code>users.json</code> will be created within
            <code>data</code> directory of our project’s root directory. This file should contain 1,000 entries of random user
            information in JSON format.</p>
        <h3 id="setting-up-the-mysql-database">Setting up the MySQL Database</h3>
        <p>If everything has gone well so far, we can create the MySQL database to store the data.</p>
        <p>Then, we run MySQL’s command-line client:</p>
        <pre><code class="lang-bash">mysql -h localhost -u homestead -psecret
</code></pre>
        <p>Now, we create a database called
            <code>blackfire_tutorial</code>:</p>
        <pre><code class="lang-mysql">CREATE DATABASE blackfire_tutorial;
USE blackfire_tutorial;
</code></pre>
        <p>And the table:</p>
        <pre><code class="lang-mysql">CREATE TABLE IF NOT EXISTS `sample_users` (
    `id` int(11) NOT NULL,
    `name` varchar(255) DEFAULT NULL,
    `email` varchar(255) DEFAULT NULL,
    `city` varchar(255) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
</code></pre>
        <h3 id="writing-the-main-script">Writing the Main Script</h3>
        <p>Let’s name it
            <code>benchmark-before.php</code>:</p>
        <pre><code class="lang-php">&lt;?php

$db = new PDO(&#39;mysql:host=localhost;dbname=blackfire_tutorial;charset=utf8&#39;, ⤶
    &#39;homestead&#39;, &#39;secret&#39;);

function SaveCustomers ($db) {

    // Providing data
    $users = userProvider();

    for ($i = 0; $i &lt; count($users); $i++) {

        foreach($users[$i] as $key =&gt; $value) {
            $$key = addslashes($value);
        }

        $db-&gt;exec(&quot;INSERT INTO sample_users (name, email, city) VALUES ⤶
            (&#39;$name&#39;, &#39;$email&#39;, &#39;$city&#39;)&quot;);

    }
}

function userProvider () {

    return json_decode(file_get_contents(&#39;data/users.json&#39;), true);

}

// Storing data
saveCustomers($db);
echo &#39;Users imported successfully.&#39;;
</code></pre>
        <p>In the preceding code, we create a function named
            <code>saveCustomers()</code>, which accepts a PDO object as the argument.
            <code>saveCustomers()</code> calls
            <code>userProvider()</code> to load the contents of
            <code>data/users.json</code> into an array.</p>
        <p>Consequently, it iterates over the array’s elements and inserts them into the database one by one.</p>
        <p>To run the script, we access it via the
            <code>/benchmark-before.php</code> relative URL. If everything is in order, the MySQL table will be filled with one
            thousand rows of random user information.</p>
        <p>To check if the operation was successful:</p>
        <pre><code class="lang-sql">mysql -h localhost -u homestead -psecret
use blackfire_tutorial;
select count(id) as rows from sample_users;
</code></pre>
        <p>The expected output should be as follows:</p>
        <pre><code class="lang-bash">    +-------+
    | rows  |
    +-------+
    | 1000  |
    +-------+
    1 row in set (0.01 sec)
</code></pre>
        <h3 id="running-the-first-profile">Running the First Profile</h3>
        <p>We will profile the script at its current state and set it as the
            <strong>reference profile</strong>, then we’ll do some micro-optimizations and run the profile again.</p>
        <p>To profile the page, on the
            <code>benchmark-before.php</code> page, we click on the Blackfire icon in the browser toolbar. Then in
            <code>Compare With</code> select box, we select
            <code>Create New Reference</code> and click on
            <code>Profile</code>.</p>
        <p>
            <img src="../images/1452685658figure_02-1024x256.png" alt="Running the first profile">
        </p>
        <p>
            <em>Note: To use Companion Chrome Extension, first, we need to make sure we’re logged into
                <a href="http://blackfire.io">Blackfire.io</a>.</em>
        </p>
        <p>If everything is properly set up, we should see the Blackfire toolbar at the top of the page.</p>
        <p>To see the profile details, we click on
            <code>View profile</code>, on the right side of the Blackfire toolbar.</p>
        <h3 id="analyzing-the-profile-information">Analyzing the Profile Information</h3>
        <p>After clicking on
            <code>View profile</code>, we’ll be redirected to the web interface, where we can find all the details about this profile
            run:</p>
        <p>
            <img src="../images/1452685650figure_03-1024x508.png" alt="Web interface">
        </p>
        <p>The web interface consists of several parts:</p>
        <h4 id="toolbar">Toolbar</h4>
        <p>The toolbar contains a summary of the profile information including wall time, I/O time, memory usage, CPU time,
            etc. For our first profile, the toolbar contains the following information:</p>
        <p>
            <img src="../images/1452685642figure_04-1024x37.png" alt="blackfire_toolbar">
        </p>
        <pre><code>    | Wall Time | I/O    | CPU Time | SQL Queries      |
    |-----------|--------|----------|------------------|
    | 578 ms    | 541 ms | 36.8 ms  | 556 s / 1000 rq  |
</code></pre>
        <h4 id="call-graph-diagram">Call Graph Diagram</h4>
        <p>This section visualizes the execution flow of our code in a graph. Each node in the graph represents a function/method
            in our application with the amount of time it took to execute.</p>
        <p>
            <img src="../images/1452685636figure_05.png" alt="Call graph diagram">
        </p>
        <p>All
            <strong>nodes</strong> in the graph are color-coded. The rule is simple:
            <strong>The darker this red color is, the more active this node is during each request.</strong>
        </p>
        <p>Colored borders show the application’s
            <strong>hot paths</strong> (the parts of our application that were most active during the profile) while the colored
            backgrounds show the most
            <strong>intense</strong> nodes. In many cases, hot paths don’t imply a performance bottleneck as some routines normally
            do the heavy lifting in our application. However, it is a good place to start when trying to locate the bottlenecks.</p>
        <p>By having a quick glance at the graph, we can spot the most active nodes. In our case,
            <code>saveCustomers()</code> has used
            <strong>99.27%</strong> (including external calls) of the total time.</p>
        <p>
            <strong>PDO::exec</strong> (called by
            <code>saveCustomers()</code>) is the most intense node in the graph, as we’ve called this method one thousand times
            from
            <code>saveCustomers()</code>. It has taken
            <strong>92.56%</strong> of the total time spent, according to the graph!</p>
        <h4 id="functions-list">Functions List</h4>
        <p>
            <img src="../images/1452685630figure_06.png" alt="Functions list">
        </p>
        <p>By clicking on each node, we can see all the details about it (on the left panel) including the number of calls and
            spent time.</p>
        <p>As an example, we click on the
            <code>saveCustomers()</code> node. According to the screenshot above, we can see that this function has been called
            <strong>once</strong> (1 caller), and it has four
            <strong>callees</strong> (external function/method calls).</p>
        <p>There are also several horizontal bar charts, showing the inclusive/exclusive times for this node in each
            <strong>dimension</strong> (wall time, I/O time, Memory, CPU time, etc.). The darker shade shows the exclusive time while
            the lighter shade shows the inclusive time. If we move the mouse over the bars, we can see the time/percentage
            for each of these dimensions.</p>
        <p>We can also see the
            <strong>caller(s)</strong> of this function by clicking on the button underneath the
            <strong>1 Callers (1 calls)</strong> label.</p>
        <p>If we scroll a bit down, we can also see the
            <strong>callee(s)</strong>. Above the
            <strong>4 callees</strong> label, there are several buttons next to each other with
            <strong>different</strong> widths. By clicking on each of these buttons, we’ll see the performance information for each
            external call from
            <code>saveCustomer()</code>. For instance,
            <code>userProvider()</code> is one of the callees of
            <code>saveCustomers()</code> .</p>
        <p>
            <img src="../images/1452685624figure_07.png" alt="Callees">
        </p>
        <h4 id="blackfire-metrics">Blackfire Metrics</h4>
        <p>Blackfire provides some metrics out of the box allowing us to evaluate our application’s performance from other perspectives
            as well, like SQL queries, PDO connections, HTTP response size, the number of compiled and executed files, just
            to name a few.</p>
        <p>
            <img src="../images/1452685618figure_09.png" alt="Metrics">
        </p>
        <p>These metrics can be used in continuous performance testing. We’ll get to that shortly.</p>
        <h3 id="optimizing-the-existing-code">Optimizing the Existing Code</h3>
        <p>Okay, now let’s do some micro-optimizations on the existing script to improve the performance a bit. In the existing
            code, we’re making one database request per entry, meaning 1,000 database interactions during each request. What
            if we could reduce this number to only
            <strong>one</strong> request? Consider the following statement:</p>
        <pre><code class="lang-mysql">INSERT INTO persons (name, email, city) VALUES (?, ?, ?),(?, ?, ?),(?, ?, ?),(?, ?, ?)⤶
...(?, ?, ?)
</code></pre>
        <p>In the above statement, all the values are concatenated as groups of parentheses (separated by commas). This statement
            will obviously use more memory, but will be much faster than the existing one. Let’s see it in action:</p>
        <pre><code class="lang-php">&lt;?php

$db = new PDO(&#39;mysql:host=localhost;dbname=blackfire_tutorial;charset=utf8&#39;, ⤶
&#39;homestead&#39;, &#39;secret&#39;);

function SaveCustomers($db) {

    // Providing data
    $users = userProvider();

    $params = [];
    $num = count($users);
    $placeholders = rtrim(str_repeat(&#39;(?, ?, ?), &#39;, $num), &#39;, &#39;);

    for ($i = 0; $i &lt; $num; $i++) {

        $params[] = $users[$i][&#39;name&#39;];
        $params[] = $users[$i][&#39;email&#39;];
        $params[] = $users[$i][&#39;city&#39;];

    }

    $q = $db-&gt;prepare(&#39;INSERT INTO sample_users (name, email, city) VALUES &#39; ⤶
    . $placeholders);
    $q-&gt;execute($params);

    unset($params);
    unset($placeholders);
}

function userProvider () {

    return json_decode(file_get_contents(&#39;data/users.json&#39;), true);

}

//Saving data
saveCustomers($db);
echo &#39;Users imported successfully.&#39;;
</code></pre>
        <p>We call this file
            <code>benchmark-after.php</code> and run it in the browser. Now, we run the profiler again. This time in the
            <strong>Compare with</strong> select box, we choose our reference profile.</p>
        <p>
            <img src="../images/1452685612figure_10.png" alt="Choosing profile">
        </p>
        <p>When the profiling is done, we click on
            <code>View Comparisons</code> to go to the web interface.</p>
        <p>
            <img src="../images/1452685605figure_11-1024x36.png" alt="Second profile toolbar">
        </p>
        <p>As we can see, the web interface is a bit different now since we’re comparing two different profiles. By taking a
            quick look at the toolbar, we can see that the wall time has been significantly optimized by
            <strong>98%</strong> (from
            <strong>578 ms</strong> to
            <strong>14 ms</strong>) and the number of database requests is reduced to only one! The memory usage has been increased
            by
            <strong>97%</strong> though (2.42 MB).</p>
        <p>If the performance has been increased when compared to the reference profile, respective values on the toolbar should
            be colored in green. Red means the performance has been reduced (in our case the Memory usage):</p>
        <pre><code>    | Wall Time | I/O    | CPU Time | Memory | SQL Queries      |
    |-----------|--------|----------|--------|------------------|
    |   -98%    |  -98%  |   -89%   |  +117% |  -99% / -999 rq  |
</code></pre>
        <p>The toolbar’s information in the second profile run displays percentage differences (between the current profile
            and the reference). To see the information of the current profile, toggle the
            <code>Delta</code> switch in right side of the toolbar:</p>
        <p>
            <img src="../images/1452685598figure_12.png" alt="Delta switch">
        </p>
        <p>By looking at the graph, we can also see that the performance has improved. The nodes in this diagram are colored
            blue. The negative value in each node means the amount of time we have saved.</p>
        <p>
            <img src="../images/1452685593figure_13.png" alt="Blue graph diagram">
        </p>
        <p>In our case
            <strong>PDO::exec</strong> has been impacted most (time reduced by 555 ms). By clicking the node, we can see its details
            in the left pane. The performance information of this profile and the reference profile are displayed
            <strong>side by side</strong>.</p>
        <p>By clicking on the metric tab, we can see the improvements from other perspectives. For example, in our last profile
            run, the
            <code>PDO Queries</code> has been reduced to only one.</p>
        <p>
            <img src="../images/1452685588figure_14.png" alt="Query metrics">
        </p>
        <p>Well, this wasn’t a lesson in performance tuning, but good enough to scratch the surface of Blackfire.</p>
        <h2 id="using-the-command-line-interface">Using the Command Line Interface</h2>
        <p>Along with the Companion, Blackfire provides a nifty command line utility called
            <code>blackfire</code> allowing us to profile any PHP script including web pages, web services, API calls or command-line
            scripts right from the terminal.</p>
        <h3 id="profiling-http-requests">Profiling HTTP Requests</h3>
        <p>To profile a web page from the command line, we use the
            <code>curl</code> sub-command followed by the page URL:</p>
        <pre><code class="lang-bash">blackfire curl http://192.168.10.10/benchmark-before.php
</code></pre>
        <p>As a result, Blackfire outputs some performance related information along with the URL to the web interface:</p>
        <pre><code>Profile URL: https://blackfire.io/profiles/b8fceed1-06be-4a0f-b28f-7841457e0837/graph
    Total time:   628 ms
    CPU time:    74 ms
            I/O:   554 ms
        Memory:  1.23 MB
        Network:      n/a
            SQL:   570 ms  1000 rq
</code></pre>
        <p>To have more precise results, we can take several
            <strong>samples</strong> of the same request by passing the
            <code>--sample</code> option, followed by the number of samples that we want. Blackfire takes 10 samples by default
            so don’t be surprised if your database table contains 11,000 rows after the first profile run.</p>
        <pre><code class="lang-bash">blackfire --sample 15 curl http://192.168.10.10/benchmark-before.php
</code></pre>
        <p>We can also create a new reference profile just the way we did using Companion:</p>
        <pre><code class="lang-bash">blackfire --new-reference curl http://192.168.10.10/benchmark-before.php
</code></pre>
        <p>Or compare it against a previously created reference profile. To do this, we pass
            <code>--reference</code> followed by the reference profile id:</p>
        <pre><code class="lang-bash">blackfire --reference=7  curl http://192.168.10.10/benchmark-after.php
</code></pre>
        <p>
            <em>The reference profile id is available in the web interface, or as part of the profile output when using
                <code>--new-reference</code> option.</em>
        </p>
        <h3 id="profiling-cli-scripts">Profiling CLI Scripts</h3>
        <p>By using the
            <code>blackfire</code> utility, we can profile any command-line script as well. This is possible via the
            <code>run</code> sub-command:</p>
        <pre><code class="lang-bash">blackfire run php benchmark-before.php
</code></pre>
        <p>All the options used with the
            <code>curl</code> sub-command can also be used with
            <code>run</code>.</p>
        <h2 id="performance-tests">Performance Tests</h2>
        <p>Another great feature of Blackfire is its continuous performance testing. As mentioned earlier, Blackfire provides
            a variety of metrics out of the box which we can use to write performance tests. This feature is only available
            to premium users, but it is also available as a two-week trial. Assertions can be on time dimensions or other
            dimensions like the number of database requests, memory usage or response size.</p>
        <p>All the tests should be in
            <code>.blackfire.yml</code> within our project’s root directory.</p>
        <p>A Blackfire test is like the following code:</p>
        <pre><code class="lang-yml">tests:
    &quot;Pages should be fast enough&quot;:
        path: &quot;/benchmark-before.php&quot; # run the assertions for all HTTP requests
        assertions:
            - &quot;main.wall_time &lt; 100ms&quot; # wall clock time is less than 100ms
</code></pre>
        <p>As we can see, all the tests should be under the
            <code>tests</code> main key.</p>
        <p>A test is composed of the following components:</p>
        <ul>
            <li>A name (in the above example:
                <strong>Pages should be fast enough</strong>)</li>
            <li>A regular expression (path) that all HTTP request must
                <strong>match</strong>, for the test to be executed.</li>
            <li>A set of
                <strong>assertions</strong> which consist of
                <strong>metrics</strong> and
                <strong>assertion values</strong>.</li>
        </ul>
        <p>Each time the profiler is run for a project containing the
            <code>.blackfire.yml</code> file, Blackfire automatically runs all the tests and reflects the result in the web interface
            (
            <strong>Assertion</strong> tab in the left panel).</p>
        <p>
            <img src="../images/1452685582figure_15.png" alt="Assertion test">
        </p>
        <p>In the above example, the test is run for
            <code>benchmark-before.php</code>.
            <code>main.wall_time</code> is a Blackfire metric for the total time required to execute the script. In the above assertion,
            we check if it’s less than
            <code>100ms</code>:</p>
        <p>Here’s another example with more assertions from
            <a href="https://blackfire.io/docs/cookbooks/tests">Blackfire’s documentation</a>:</p>
        <pre><code class="lang-yml">tests:
    &quot;Homepage should not hit the DB&quot;:
        path: &quot;/&quot;     # only apply the assertions for the homepage
        assertions:
            - &quot;metrics.sql.queries.count == 0&quot;      # no SQL statements executed
            - &quot;main.peak_memory &lt; 10mb&quot;             # memory does not exceed 10mb
            - &quot;metrics.output.network_out &lt; 100kb&quot;  # response size is less than 100kb
</code></pre>
        <p>The above test is run for the home page (
            <code>/</code>). In the assertions, we make sure that no database request is made on the home page, memory usage does
            not exceed
            <code>10 MB</code> and the response size is less than
            <code>100 KB</code>.</p>
        <p>To learn more about assertions, refer to the
            <a href="https://blackfire.io/docs/reference-guide/assertions">Assertion reference</a>.</p>
        <p>We can also have
            <strong>custom metrics</strong> in our assertions, which is fully covered in the
            <a href="https://blackfire.io/docs/reference-guide/metrics#metrics-custom-metrics">documentation</a>.</p>
        <p>
            <em>Note: to validate the tests, we can use Blackfire’s
                <a href="https://blackfire.io/docs/validator">Validator</a>.</em>
        </p>
        <h2 id="wrapping-up">Wrapping Up</h2>
        <p>
            <a href="http://blackfire.io">Blackfire.io</a> is a powerful web profiler which instruments applications without adding a single line of code.
            It consists of five main components: the Probe, The Agent, The Companion, CLI tool and the web interface. The
            Probe and the Agent are responsible for instrumenting the code and forwarding the profile results to the Blackfire
            server.</p>
        <p>We can profile an application either by using Companion or
            <code>blackfire</code> command line utility.</p>
        <p>Blackfire provides a web interface which visualizes the details of the profile result. We can set a profile as a
            reference, and then use that as a performance baseline to compare against future profiles.</p>
    </div>
    <div class="chapter">
        <div class="ch-head">Chapter</div>
        <h1 class="chaptertitle"> How to Boost Your Server Performance with Varnish</h1>
        <h3 class="author">Tonino Jankov</h3>
        <p>Varnish Cache is an
            <a href="https://en.wikipedia.org/wiki/Web_accelerator">HTTP accelerator</a> and
            <a href="https://en.wikipedia.org/wiki/Reverse_proxy">reverse proxy</a> developed by Danish consultant and FreeBSD core developer
            <a href="https://www.wikiwand.com/en/Poul-Henning_Kamp">Poul-Henning Kamp</a>, along with other developers at Norwegian
            <a href="https://www.redpill-linpro.com/">Linpro AS</a>. It was
            <a href="https://varnish-cache.org/lists/pipermail/varnish-announce/2006-September/000638.html">released</a> in 2006.</p>
        <p>
            <a href="http://royal.pingdom.com/2012/07/11/how-popular-is-varnish/">According to Pingdom.com</a>, a company focused on web performance, in 2012 Varnish was already famous among
            the world&#39;s top websites for its capacity to speed up web delivery, and it was being used by sites such as
            Wired, SlideShare, Zappos, SoundCloud, Weather.com, Business Insider, Answers.com, Urban Dictionary, MacRumors,
            DynDNS, OpenDNS, Lonely Planet, Technorati, ThinkGeek and Economist.com.</p>
        <p>It is licensed under a two-clause
            <a href="https://www.wikiwand.com/en/BSD_license">BSD license</a>. Varnish has a premium tier,
            <a href="https://www.varnish-software.com/pricing/varnish-plus/">Varnish Plus</a>, focused on enterprise customers, which offers some
            <a href="https://www.varnish-software.com/products/varnish-plus/">extra features, modules, and support</a>.</p>
        <p>Although there are other solutions that also
            <a href="https://deliciousbrains.com/page-caching-varnish-vs-nginx-fastcgi-cache/">shine</a>, Varnish is still a go-to solution that can dramatically improve website speed, reduce the strain on
            the web application server&#39;s CPU, and even serve as a
            <a href="https://www.htpcguides.com/wordpress-ddos-attack-protection-varnish-4-firewall/">protection layer from DDoS</a> attacks.
            <a href="https://www.keycdn.com/support/using-a-varnish-cdn-stack-with-keycdn/">KeyCDN recommends</a> deploying it on the origin server stack.</p>
        <p>Varnish can sit on a dedicated machine in case of more demanding websites, and make sure that the origin servers
            aren&#39;t affected by the flood of requests.</p>
        <p>At the time of this writing (November 2017), Varnish is at version
            <a href="https://varnish-cache.org/docs/5.2/whats-new/changes-5.2.html">5.2</a>.</p>
        <h2 id="how-it-works">How It Works</h2>
        <p>
            <a href="https://en.wikipedia.org/wiki/Cache_(computing">Caching</a>) in general works by keeping the pre-computed outputs of an application in memory, or on the disk,
            so that expensive computations don&#39;t have to be computed over and over on every request.
            <a href="https://en.wikipedia.org/wiki/Web_cache">Web Cache</a> can be on the client (browser cache), or on the server. Varnish falls into the second category.
            It is usually configured so that it listens for requests on the standard HTTP port (80), and then serves the
            requested resource to the website visitor.</p>
        <p>
            <img src="../images/1511065496varnish-miss-1024x420.gif" alt="Varnish cache miss">
        </p>
        <p>The first time a certain URL and path are requested, Varnish has to request it from the origin server in order to
            serve it to the visitor. This is called a CACHE MISS, which can be read in HTTP response headers, depending on
            the Varnish setup.</p>
        <p>According to the
            <a href="https://www.varnish-software.com/wiki/faq/index.html">docs</a>:</p>
        <blockquote>
            <p>when an object, any kind of content i.e. an image or a page, is not stored in the cache, then we have what is
                commonly known as a cache miss, in which case Varnish will go and fetch the content from the web server,
                store it and deliver a copy to the user and retain it in cache to serve in response to future requests.</p>
        </blockquote>
        <p>When a particular URL or a resource is cached by Varnish and stored in memory, it can be served directly from server
            RAM; it doesn&#39;t need to be computed every time. Varnish will start delivering a CACHE HIT in a matter of
            microseconds.</p>
        <p>
            <img src="../images/1511065546varnish-hit-1024x416.gif" alt="Varnish cache hit">
        </p>
        <p>This means that neither our origin server or our web application, including its database, are touched by future requests.
            They won&#39;t even be aware of the requests loaded on cached URLs.</p>
        <p>The origin server --- or servers, in case we
            <a href="http://opentsdb.net/docs/build/html/user_guide/utilities/varnish.html">use Varnish as a load balancer</a> --- are configured to listen on some non-standard port, like 8888, and Varnish
            is
            <a href="http://devdocs.magento.com/guides/v2.1/config-guide/varnish/config-varnish-configure.html">made aware of their address and port</a>.</p>
        <h2 id="varnish-features">Varnish Features</h2>
        <p>
            <strong>Varnish is
                <a href="https://book.varnish-software.com/4.0/chapters/Tuning.html#threading-model">threaded</a>.</strong> It&#39;s been
            <a href="https://kly.no/posts/2010_01_26__Varnish_best_practices__.html">reported</a> that Varnish was able to handle over 200,000 requests per second on a single instance. If properly
            configured, the only bottlenecks of your web app will be network throughput and the amount of RAM. (This shouldn&#39;t
            be an unreasonable requirement, because it just needs to keep computed web pages in memory, so for most websites,
            a couple of gigabytes should be sufficient.)</p>
        <p>
            <strong>Varnish is extendable via
                <a href="https://info.varnish-software.com/blog/varnish-modules-vmods-overview">VMODS</a>
            </strong>. These are modules that can use standard C libraries and extend Varnish functionality. There are community-contributed
            VMODS listed
            <a href="https://varnish-cache.org/vmods/">here</a>. They range from header manipulation to Lua scripting, throttling of requests, authentication, and so
            on.</p>
        <p>
            <strong>Varnish has its own domain-specific language,
                <a href="http://book.varnish-software.com/4.0/chapters/VCL_Basics.html">VCL</a>
            </strong>. VCL provides comprehensive configurability. With a full-page caching server like Varnish, there are a lot of
            intricacies that need to be solved.</p>
        <p>
            <img src="../images/1511065588vcl-scheme.gif" alt="vcl scheme">
        </p>
        <p>When we cache a dynamic website with dozens or hundreds of pages and paths, with GET query parameters, we&#39;ll
            want to exclude some of them from cache, or set different cache-expiration rules. Sometimes we&#39;ll want to
            cache certain Ajax requests, or exclude them from the cache. This varies from project to project, and can&#39;t
            be tailored in advance.</p>
        <p>Sometimes we&#39;ll want Varnish to decide what to do with the request depending on request headers. Sometimes we&#39;ll
            want to pass requests directly to the back end with a certain cookie set.</p>
        <p>To quote the
            <a href="http://book.varnish-software.com/4.0/chapters/VCL_Basics.html#summary-of-vcl-basics">Varnish book</a>,</p>
        <blockquote>
            <p>VCL provides subroutines that allow you to affect the handling of any single request almost anywhere in the execution
                chain.</p>
        </blockquote>
        <p>Purging the cache often needs to be done dynamically --- triggered by publishing articles or updating the website.
            Purging also needs to be done as atomically as possible --- meaning it should target the smallest possible scope,
            like a single resource or path.</p>
        <p>This means that specific rules need to be defined, with their order of priority in mind. Some examples can be found
            in the Varnish book (which is available to read
            <a href="http://book.varnish-software.com/4.0/chapters/VCL_Basics.html">online</a> or as a
            <a href="https://info.varnish-software.com/the-varnish-book">downloadable PDF</a>).</p>
        <p>Varnish has a set of
            <a href="http://book.varnish-software.com/4.0/chapters/Appendix_B__Varnish_Programs.html">tools for monitoring and administering the server</a>:</p>
        <ul>
            <li>
                <p>There&#39;s
                    <code>varnishtop</code>, which lets us monitor requested URLs and their frequency.</p>
            </li>
            <li>
                <p>
                    <code>varnishncsa</code> can be used to print the
                    <em>Varnish Shared memory Log</em> (VSL): it dumps everything pointing to a certain domain and subdomains.</p>
            </li>
            <li>
                <p>
                    <code>varnishhist</code> reads the VSL and presents a live histogram showing the distribution of the last number
                    of requests, giving an overview of server and back-end performance.</p>
            </li>
            <li>
                <p>
                    <code>varnishtest</code> is used to test VCL configuration files and develop VMODS.</p>
            </li>
            <li>
                <p>
                    <code>varnishstat</code> displays statistics about our varnishd instance:</p>
                <p>
                    <img src="../images/1511065626varnishstat-1024x622.gif" alt="varnishstat">
                </p>
            </li>
            <li>
                <p>
                    <code>varnishlog</code> is used to get data about specific clients and requests.</p>
            </li>
        </ul>
        <p>
            <a href="https://www.varnish-software.com/">Varnish Software</a> offers a set of commercial, paid solutions either built on top of Varnish cache, or extending
            its usage and helping with monitoring and management:
            <a href="https://www.varnish-software.com/products/varnish-api-engine/">Varnish Api Engine</a>,
            <a href="https://www.varnish-software.com/products/varnish-extend/">Varnish Extend</a>,
            <a href="https://www.varnish-software.com/products/akamai-connector-for-varnish/">Akamai Connector for Varnish</a>,
            <a href="https://www.varnish-software.com/plus/varnish-administration-console/">Varnish Administration Console (VAC)</a>, and
            <a href="https://www.varnish-software.com/plus/varnish-custom-statistics/">Varnish Custom Statistics (VCS)</a>.</p>
        <h2 id="installing-varnish">Installing Varnish</h2>
        <p>The Varnish docs
            <a href="https://varnish-cache.org/docs/5.0/installation/install.html">cover installation on various systems</a>. We&#39;ll go with Ubuntu 16.04 LTS in this post.</p>
        <p>
            <a href="https://packagecloud.io/varnishcache/varnish5/install#manual-deb">Packagecloud.io</a> has instructions for updating the Ubuntu repositories and installing Varnish version 5:</p>
        <pre><code class="lang-bash">curl -L https://packagecloud.io/varnishcache/varnish5/gpgkey | sudo apt-key add -
sudo apt-get update
sudo apt-get install -y apt-transport-https
</code></pre>
        <p>Then we add the following lines to the newly created file
            <code>/etc/apt/sources.list.d/varnishcache_varnish5.list</code>:</p>
        <pre><code class="lang-bash">deb https://packagecloud.io/varnishcache/varnish5/ubuntu/ xenial main
deb-src https://packagecloud.io/varnishcache/varnish5/ubuntu/ xenial main
</code></pre>
        <p>Then we run:</p>
        <pre><code class="lang-bash">sudo apt-get update
sudo apt-get install varnish
</code></pre>
        <p>We can test a brand-new WordPress installation running on Nginx. First, we change Nginx&#39;s default listening port
            from 80 to 8080 --- which is the port Varnish expects the back end to be running on --- by adding the following
            lines to the Nginx virtual host, inside the server clause:</p>
        <pre><code class="lang-cnf">server {       
    listen 127.0.0.1:8080 default_server;
    listen [::]:8080 default_server;
</code></pre>
        <p>Then we configure Varnish: we edit
            <code>/etc/default/varnish</code>, replacing port 6081 with 80 (the default web port):</p>
        <pre><code class="lang-cnf">DAEMON_OPTS=&quot;-a :80 \
    -T localhost:6082 \
    -f /etc/varnish/default.vcl \
    -S /etc/varnish/secret \
    -s malloc,256m&quot;
</code></pre>
        <p>We also need to change
            <code>/lib/systemd/system/varnish.service</code>, making the same replacement:</p>
        <pre><code class="lang-cnf">[Service]
Type=simple
LimitNOFILE=131072
LimitMEMLOCK=82000
ExecStart=/usr/sbin/varnishd -j unix,user=vcache -F -a :80 -T localhost:6082 ⤶
-f /etc/varnish/default.vcl -S /etc/varnish/secret -s malloc,256m
ExecReload=/usr/share/varnish/reload-vcl
ProtectSystem=full
ProtectHome=true
PrivateTmp=true
PrivateDevices=true
</code></pre>
        <p>Then we restart Nginx and Varnish:</p>
        <pre><code class="lang-bash">sudo service nginx restart
sudo /etc/init.d/varnish restart
</code></pre>
        <p>
            <em>Warning: due to some peculiarities, Varnish usually must be restarted --- or started this way, not with
                <code>service varnish start</code> --- in order to read all the config files we edited.</em>
        </p>
        <p>We tested the website speed and responsiveness with
            <a href="https://locust.io">Locust</a> and
            <a href="https://tools.pingdom.com">Pingdom Tools</a>.</p>
        <p>Once the cache was warmed up, the difference was impressive, despite Nginx being well known for its speed:
            <em>the average number of requests per second was multiplied by three to four times</em>, and response time were
            greatly reduced. Load times were a bit higher due to network latency, since we tested the website hosted in California
            from a workstation in Europe.</p>
        <p>
            <strong>Locust results for Nginx:</strong>
        </p>
        <p>
            <img src="../images/1511065686locust-nginx.jpg" alt="locust nginx">
        </p>
        <p>
            <strong>Locust results for Nginx + Varnish:</strong>
        </p>
        <p>
            <img src="../images/1511065729locust-varnish.jpg" alt="Locust varnish">
        </p>
        <p>Pingdom results were also good.</p>
        <p>
            <strong>Pingdom results for Nginx stack, tested from California:</strong>
        </p>
        <p>
            <img src="../images/1511065769pingdom-nginx.jpg" alt="Pingdom nginx ">
        </p>
        <p>
            <strong>Pingdom results for Nginx + Varnish, California:</strong>
        </p>
        <p>
            <img src="../images/1511065810pingdom-varnish-cal-1024x327.jpg" alt="Pingdom varnish">
        </p>
        <p>Notice also the
            <a href="https://en.wikipedia.org/wiki/Time_to_first_byte">TTFB</a> for each case.</p>
        <p>
            <strong>Nginx alone:</strong>
        </p>
        <p>
            <img src="../images/1511065860ttfb-nginx-cal.jpg" alt="TTFB nginx">
        </p>
        <p>
            <strong>Nginx + Varnish:</strong>
        </p>
        <p>
            <img src="../images/1511065893ttfb-varnish.jpg" alt="TTFB varnish">
        </p>
        <p>Even if we neglect the pink part, which is the DNS lookup, there is still an obvious difference.</p>
        <h2 id="simplicity-of-setup">Simplicity of Setup</h2>
        <p>Varnish doesn&#39;t care what&#39;s listening on port 8080 (we can change this default port as well, if required).
            This means that setting up Apache, or some other application server, should be just as straightforward: all we
            need to do is to configure them to listen on port 8080 instead of 80.</p>
        <h3 id="setting-up-varnish-with-nodejs">Setting up Varnish with NodeJS</h3>
        <p>On our existing server, where we had already installed Varnish, setting up a hello-world Node app was just as simple.
            We installed the
            <code>nodejs</code> and
            <code>npm</code> packages and linked NodeJS to Node:</p>
        <pre><code class="lang-bash">ln -s /usr/bin/nodejs /usr/bin/node
</code></pre>
        <p>Then we created a simple node
            <em>hello-world</em> program listening on port 8080:</p>
        <pre><code class="lang-javascript">#!/usr/bin/env nodejs

var http = require(&#39;http&#39;);
http.createServer(function (req, res) {
        res.writeHead(200, {&#39;Content-Type&#39;: &#39;text/plain&#39;});
        res.end(&#39;Hello World\n&#39;);
}).listen(8080, &#39;localhost&#39;);

console.log(&#39;Server running at http://localhost:8080/&#39;);
</code></pre>
        <p>Then we installed Node&#39;s package manager,
            <a href="http://pm2.keymetrics.io/">PM2</a>, to be able to daemonize our app:</p>
        <pre><code class="lang-bash">sudo npm install -g pm2

pm2 start index.js
</code></pre>
        <p>aAnd voila --- our Node app was being served by Varnish:</p>
        <p>
            <img src="../images/1511065933node-pingdom-1024x292.jpg" alt="Node Hello World - Pingdom">
        </p>
        <h2 id="other-tips">Other Tips</h2>
        <p>To be able to control whether or not our request is being cached in our browser inspector, we ought to add the following
            snippet to our Varnish config file, into the
            <code>sub vcl_deliver</code> block:</p>
        <pre><code class="lang-javascript">sub vcl_deliver {
    if (obj.hits &gt; 0) {
    set resp.http.X-Cache = &quot;HIT&quot;;
    } else {
    set resp.http.X-Cache = &quot;MISS&quot;;
    }
}
</code></pre>
        <p>Then we can see the feedback in our response headers as
            <em>HIT or MISS</em>:</p>
        <p>
            <img src="../images/1511065974response-headers.jpg" alt="Response Headers">
        </p>
        <p>One more warning: Varnish (or at least the open-source version)
            <a href="https://varnish-cache.org/docs/trunk/phk/ssl.html">doesn&#39;t support SSL</a>, reiterated
            <a href="https://varnish-cache.org/docs/trunk/phk/ssl_again.html">again</a> by its creator Poul-Henning Kamp (who is
            <a href="http://queue.acm.org/detail.cfm?id=2716278">not shy to voice his opinions</a>). So when you need to use Varnish and HTTPS, consider using another proxy in
            front of it for
            <a href="https://komelin.com/articles/https-varnish">SSL termination</a> --- such as
            <a href="https://z0z0.me/create-your-own-cdn-with-haproxy-and-varnish/">haproxy</a>, or Varnish&#39;s own
            <a href="https://github.com/varnish/hitch">hitch</a>.</p>
        <p>Or, if that&#39;s getting too involved, just use
            <a href="https://www.scalescale.com/tips/nginx/configure-nginx-fastcgi-cache/#">Nginx and FastCGI Cache</a>.</p>
        <h2 id="conclusion">Conclusion</h2>
        <p>In this article we tried to give a brief introduction to Varnish Cache without going too deeply into its setup, monitoring
            and administration.</p>
        <p>Tuning server performance is a science of its own, and presenting the full scope of use cases and setups requires
            another article. I&#39;ll be diving a bit deeper into this subject in another article, so stay tuned for a future
            installment, where I&#39;ll add Varnish in front of a real app.</p>
    </div>
    <div class="chapter">
        <div class="ch-head">Chapter</div>
        <h1 class="chaptertitle"> How to Process Server Logs</h1>
        <h3 class="author">Daniel Berman</h3>
        <p>When things go south with our applications -- as they sometimes do, whether we like it or not -- our log files are
            normally among the first places where we go when we start the troubleshooting process. The big “but” here is
            that despite the fact that log files contain a wealth of helpful information about events, they are usually extremely
            difficult to decipher.</p>
        <p>A modern web application environment consists of multiple log sources, which collectively output thousands of log
            lines written in unintelligible machine language. If you, for example, have a LAMP stack set up, then you have
            PHP, Apache, and MySQL logs to go through. Add system and environment logs into the fray -- together with framework-specific
            logs such as Laravel logs -- and you end up with an endless pile of machine data.</p>
        <p>Talk about a needle in a haystack.</p>
        <p>
            <img src="../images/1462437187elk-logo.png" alt="ELK logo">
        </p>
        <p>The ELK Stack (
            <a href="https://github.com/elastic/elasticsearch">Elasticsearch</a>,
            <a href="https://github.com/elastic/logstash">Logstash</a>, and
            <a href="https://github.com/elastic/kibana">Kibana</a>) is quickly becoming the most popular way to handle this challenge. Already the most popular open-source
            log analysis platform -- with 500,000 downloads a month, according to Elastic -- ELK is a great way to centralize
            logs from multiple sources, identify correlations, and perform deep-data analysis.</p>
        <p>Elasticsearch is a search-and-analytics engine based on Apache Lucene that allows users to search and analyze large
            amounts of data in almost real time. Logstash can ingest and forward logs from anywhere to anywhere. Kibana is
            the stack’s pretty face -- a user interface that allows you to query, visualize, and explore Elasticsearch data
            easily.</p>
        <p>This article will describe how to set up the ELK Stack on a local development environment, ship web server logs (Apache
            logs in this case) into Elasticsearch using Logstash, and then analyze the data in Kibana.</p>
        <h2 id="installing-java">Installing Java</h2>
        <p>The ELK Stack requires Java 7 and higher (only Oracle’s Java and the OpenJDK are supported), so as an initial step,
            update your system and run the following:</p>
        <pre><code class="lang-bash">sudo apt-get install default-jre
</code></pre>
        <h2 id="installing-elk">Installing ELK</h2>
        <p>There are numerous ways of installing the ELK Stack -- you can use Docker, Ansible, Vagrant, Microsoft Azure, AWS,
            or a hosted ELK solution -- just take your pick. There is a vast number of tutorials and guides that will help
            you along the way, one being this
            <a href="http://logz.io/learn/complete-guide-elk-stack/">ELK Stack guide</a> that we at
            <a href="http://logz.io">Logz.io</a> put together.</p>
        <h3 id="installing-elasticsearch">Installing Elasticsearch</h3>
        <p>We’re going to start the installation process with installing Elasticsearch. There are various ways of setting up
            Elasticsearch but we will use Apt.</p>
        <p>First, download and install Elastic’s public signing key:</p>
        <pre><code class="lang-bash">wget -qO - https://packages.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
</code></pre>
        <p>Next, save the repository definition to
            <code>/etc/apt/sources.list.d/elasticsearch-2.x.list</code>:</p>
        <pre><code class="lang-bash">echo &quot;deb http://packages.elastic.co/elasticsearch/2.x/debian stable main&quot; | sudo tee -a /etc/apt/sources.list.d/elasticsearch-2.x.list
</code></pre>
        <p>Last but not least, update the repository cache and install Elasticsearch:</p>
        <pre><code class="lang-bash">sudo apt-get update &amp;&amp; sudo apt-get install elasticsearch
</code></pre>
        <p>Elasticsearch is now installed. Before we continue to the next components, we’re going to tweak the configuration
            file a bit:</p>
        <pre><code class="lang-bash">sudo nano /etc/elasticsearch/elasticsearch.yml
</code></pre>
        <p>Some common configurations involve the restriction of external access to Elasticsearch, so data cannot be hacked
            or deleted via HTTP API:</p>
        <pre><code class="lang-yaml">network.host: localhost
</code></pre>
        <p>You can now restart Elasticsearch:</p>
        <pre><code class="lang-bash">sudo service elasticsearch restart
</code></pre>
        <p>To verify that Elasticsearch is running properly, query the following URL using the cURL command:</p>
        <pre><code class="lang-bash">sudo curl &#39;http://localhost:9200&#39;
</code></pre>
        <p>You should see the following output in your terminal:</p>
        <pre><code class="lang-javascript">{
    &quot;name&quot; : &quot;Jebediah Guthrie&quot;,
    &quot;cluster_name&quot; : &quot;elasticsearch&quot;,
    &quot;version&quot; : {
    &quot;number&quot; : &quot;2.3.1&quot;,
    &quot;build_hash&quot; : &quot;bd980929010aef404e7cb0843e61d0665269fc39&quot;,
    &quot;build_timestamp&quot; : &quot;2016-04-04T12:25:05Z&quot;,
    &quot;build_snapshot&quot; : false,
    &quot;lucene_version&quot; : &quot;5.5.0&quot;
    },
    &quot;tagline&quot; : &quot;You Know, for Search&quot;
}
</code></pre>
        <p>To make the service start on boot, run:</p>
        <pre><code class="lang-bash">sudo update-rc.d elasticsearch defaults 95 10
</code></pre>
        <h3 id="installing-logstash">Installing Logstash</h3>
        <p>Logstash, the &quot;L&quot; in the &quot;ELK Stack&quot;, is used at the beginning of the log pipeline, ingesting
            and collecting data before sending it on to Elasticsearch.</p>
        <p>To install Logstash, add the repository definition to your
            <code>/etc/apt/sources.list</code> file:</p>
        <pre><code class="lang-bash">echo &quot;deb http://packages.elastic.co/logstash/2.2/debian stable main&quot; | sudo tee -a /etc/apt/sources.list
</code></pre>
        <p>Update your system so that the repository will be ready for use and then install Logstash:</p>
        <pre><code class="lang-bash">sudo apt-get update &amp;&amp; sudo apt-get install logstash
</code></pre>
        <p>We’ll be returning to Logstash later to configure log shipping into Elasticsearch.</p>
        <h3 id="installing-kibana">Installing Kibana</h3>
        <p>The final piece of the puzzle is Kibana - the ELK Stack&#39;s pretty face. First, create the Kibana source list:</p>
        <pre><code class="lang-bash">echo &quot;deb http://packages.elastic.co/kibana/4.5/debian stable main&quot; | sudo tee -a /etc/apt/sources.list
</code></pre>
        <p>Then, update and install Kibana:</p>
        <pre><code class="lang-bash">sudo apt-get update &amp;&amp; apt-get install kibana
</code></pre>
        <p>Configure the Kibana configuration file at
            <code>/opt/kibana/config/kibana.yml</code>:</p>
        <pre><code class="lang-bash">sudo vi /opt/kibana/config/kibana.yml
</code></pre>
        <p>Uncomment the following lines:</p>
        <pre><code class="lang-yaml">server.port: 5601
server.host: “0.0.0.0”
</code></pre>
        <p>Last but not least, start Kibana:</p>
        <pre><code class="lang-bash">sudo service kibana start
</code></pre>
        <p>You can access Kibana in your browser at
            <code>http://localhost:5601/</code> (change the URL if you&#39;re using a VM like
            <a href="http://www.sitepoint.com/quick-tip-get-homestead-vagrant-vm-running/">Homestead Improved</a> to whichever host/port you configured):</p>
        <p>
            <img src="../images/1461657638kibana_loaded.png" alt="Kibana interface">
        </p>
        <p>To start analyzing logs in Kibana, at least one index pattern needs to be defined. An index is how Elasticsearch
            organizes data, and it can be compared to a database in the world of RDBMS, with mapping defining multiple types.</p>
        <p>You will notice that since we have not yet shipped any logs, Kibana is unable to fetch mapping (as indicated by the
            grey button at the bottom of the page). We will take care of this in the next few steps.</p>
        <p>
            <strong>Tip:</strong> By default, Kibana connects to the Elasticsearch instance running on localhost, but you can connect
            to a different Elasticsearch instance. Simply modify the Elasticsearch URL in the Kibana configuration file that
            you had edited earlier and then restart Kibana.</p>
        <h2 id="shipping-logs">Shipping Logs</h2>
        <p>Our next step is to set up a log pipeline into Elasticsearch for indexing and analysis using Kibana. There are various
            ways of forwarding data into Elasticsearch, but we’re going to use Logstash.</p>
        <p>Logstash configuration files are written in JSON format and reside in
            <code>/etc/logstash/conf.d</code>. The configuration consists of three plugin sections: input, filter, and output.</p>
        <p>Create a configuration file called
            <code>apache-logs.conf</code>:</p>
        <pre><code class="lang-bash">sudo vi /etc/logstash/conf.d/apache-logs.conf
</code></pre>
        <p>Our first task is to configure the input section, which defines where data is being pulled from.</p>
        <p>In this case, we’re going to define the path to our Apache access log, but you could enter a path to any other set
            of log files (e.g. the path to your PHP error logs).</p>
        <p>
            <em>Before doing so, however, I recommend doing some research into supported input plugins and how to define them.
                In some cases, other log forwarders such as
                <a href="https://www.elastic.co/products/beats/filebeat">Filebeat</a> and
                <a href="http://www.fluentd.org/">Fluentd</a> are recommended.</em>
        </p>
        <p>The input configuration:</p>
        <pre><code class="lang-conf">input {
    file {
        path =&gt; &quot;/var/log/apache2/access.log&quot;
        type =&gt; &quot;apache-access&quot;
    }
}
</code></pre>
        <p>Our next task is to configure a filter.</p>
        <p>Filter plugins allow us to take our raw data and try to make sense of it. One of these plugins is grok -- a plugin
            used to derive structure out of unstructured data. Using grok, you can define a search and extract part of your
            log lines into structured fields.</p>
        <pre><code class="lang-conf">filter {
    if [type] == &quot;apache-access&quot; {
    grok {
        match =&gt; { &quot;message&quot; =&gt; &quot;%{COMBINEDAPACHELOG}&quot; }
    }
    }
}
</code></pre>
        <p>The last section of the Logstash configuration file is the Output section, which defines the location to where the
            logs are sent. In our case, it is our local Elasticsearch instance on our localhost:</p>
        <pre><code class="lang-javascript">output {
    elasticsearch {}
}
</code></pre>
        <p>That’s it. Once you’re done, start Logstash with the new configuration:</p>
        <pre><code class="lang-bash">/opt/logstash/bin/logstash -f /etc/logstash/conf.d/apache-logs.conf
</code></pre>
        <p>You should see the following JSON output from Logstash indicating that all is in order:</p>
        <pre><code class="lang-javascript">{
        &quot;message&quot; =&gt; &quot;127.0.0.1 - - [24/Apr/2016:11:41:59 +0000] \&quot;GET / HTTP/1.1\&quot; 200 11764 \&quot;-\&quot; \&quot;curl/7.35.0\&quot;&quot;,
        &quot;@version&quot; =&gt; &quot;1&quot;,
        &quot;@timestamp&quot; =&gt; &quot;2016-04-24T11:43:34.245Z&quot;,
            &quot;path&quot; =&gt; &quot;/var/log/apache2/access.log&quot;,
            &quot;host&quot; =&gt; &quot;ip-172-31-46-40&quot;,
            &quot;type&quot; =&gt; &quot;apache-access&quot;,
        &quot;clientip&quot; =&gt; &quot;127.0.0.1&quot;,
            &quot;ident&quot; =&gt; &quot;-&quot;,
            &quot;auth&quot; =&gt; &quot;-&quot;,
        &quot;timestamp&quot; =&gt; &quot;24/Apr/2016:11:41:59 +0000&quot;,
            &quot;verb&quot; =&gt; &quot;GET&quot;,
        &quot;request&quot; =&gt; &quot;/&quot;,
    &quot;httpversion&quot; =&gt; &quot;1.1&quot;,
        &quot;response&quot; =&gt; &quot;200&quot;,
            &quot;bytes&quot; =&gt; &quot;11764&quot;,
        &quot;referrer&quot; =&gt; &quot;\&quot;-\&quot;&quot;,
            &quot;agent&quot; =&gt; &quot;\&quot;curl/7.35.0\&quot;&quot;
}
</code></pre>
        <p>Refresh Kibana in your browser, and you’ll notice that the index pattern for our Apache logs was identified:</p>
        <p>
            <img src="../images/1461657663kibana_loaded_with_index_defined.png" alt="Kibana interface">
        </p>
        <p>Click the
            <strong>Create</strong> button, and then select the Discover tab:</p>
        <p>
            <img src="../images/1461657631kibana_apache_logs_loaded.png" alt="Kibana interface">
        </p>
        <p>From this point onwards, Logstash is tailing the Apache access log for messages so that any new entries will be forwarded
            into Elasticsearch.</p>
        <h2 id="analyzing-logs">Analyzing Logs</h2>
        <p>Now that our pipeline is up and running, it’s time to have some fun.</p>
        <p>To make things a bit more interesting, let’s simulate some noise on our web server. To do this I’m going to download
            some
            <a href="http://logz.io/sample-data">sample Apache logs</a> and insert them into the Apache access log. Logstash is already tailing this log, so these
            messages will be indexed into Elasticsearch and displayed in Kibana:</p>
        <pre><code class="lang-bash">wget http://logz.io/sample-data
sudo -i
cat /home/ubuntu/sample-data &gt;&gt; /var/log/apache2/access.log
exit
</code></pre>
        <p>
            <img src="../images/1461657653kibana_loaded_sample_data.png" alt="Kibana interface">
        </p>
        <h3 id="searching">Searching</h3>
        <p>Searching is the bread and butter of the ELK Stack, and it’s an art unto itself. There is a large amount of documentation
            available online, but I thought I’d cover the essentials so that you will have a solid base from which to start
            your exploration work.</p>
        <p>Let’s start with some simple searches.</p>
        <p>The most basic search is the “free text” search that is performed against all indexed fields. For example, if you’re
            analyzing web server logs, you could search for a specific browser type (searching is performed using the wide
            search box at the top of the page):</p>
        <pre><code>Chrome
</code></pre>
        <p>It’s important to note that free text searches are NOT case-sensitive unless you use double quotes, in which case
            the search results show exact matches to your query.</p>
        <pre><code>“Chrome”
</code></pre>
        <p>Next up are the field-level searches.</p>
        <p>To search for a value in a specific field, you need to add the name of the field as a prefix to the value:</p>
        <pre><code>type:apache-access
</code></pre>
        <p>Say, for example, that you’re looking for a specific web server response. Enter
            <code>response:200</code> to limit results to those containing that response.</p>
        <p>You can also search for a range within a field. If you use brackets [], the results will be inclusive. If you use
            curly braces {}, the results will exclude the specified values in the query.</p>
        <p>Now, it’s time to take it up a notch.</p>
        <p>The next types of searches involve using logical statements. These are quite intuitive but require some finesse because
            they are extremely syntax-sensitive.</p>
        <p>These statements include the use of the Boolean operators AND, OR, and NOT:</p>
        <pre><code>type:apache-access AND (response:400 OR response:500)
</code></pre>
        <p>In the above search, I’m looking for Apache access logs with only a 400 or 500 response. Note the use of parentheses
            as an example of how more complex queries can be constructed.</p>
        <p>There are many more search options available (I recommend referring to Logz.io&#39;s
            <a href="http://logz.io/blog/kibana-tutorial/">Kibana tutorial</a> for more information) such as regular expressions, fuzzy searches, and proximity searches,
            but once you’ve pinpointed the required data, you can save the search for future reference and as the basis to
            create Kibana visualizations.</p>
        <h3 id="visualizing">Visualizing</h3>
        <p>One of the most prominent features in the ELK Stack in general and Kibana in particular is the ability to create
            beautiful visualizations with the ingested data. These visualizations can then be aggregated into a dashboard
            that you can use to get a comprehensive view of all the various log files coming into Elasticsearch.</p>
        <p>To create a visualization, select the Visualize tab in Kibana:</p>
        <p>
            <img src="../images/1461657624creating_a_visualization1.png" alt="Create a visualization">
        </p>
        <p>There are a number of visualization types that you can select, and which type you will choose will greatly depend
            on the purpose and end-result you are trying to achieve. In this case, I’m going to select the good ol’ pie chart.</p>
        <p>We then have another choice -- we can create the visualization from either a saved search or a new search. In this
            case, we’re going with the latter.</p>
        <p>Our next step is to configure the various metrics and aggregations for the graph’s X and Y axes. In this case, we’re
            going to use the entire index as our search base (by not entering a search query in the search box) and then
            cross reference the data with browser type: Chrome, Firefox, Internet Explorer, and Safari:</p>
        <p>
            <img src="../images/1461657670new_visualization_browser.png" alt="Pie chart visualization">
        </p>
        <p>Once you are finished, save the visualization. You can then add it to a custom dashboard in the Dashboard tab in
            Kibana.</p>
        <p>Visualizations are incredibly rich tools to have, and they are the best way to understand the trends within your
            data.</p>
        <h2 id="conclusion">Conclusion</h2>
        <p>The ELK Stack is becoming THE way to analyze and manage logs. The fact that the stack is open source and that it’s
            backed by a strong community and a fast growing ecosystem is driving its popularity.</p>
        <p>DevOps is not the sole realm of log analysis, and ELK is being used by developers, sysadmins, SEO experts, and marketers
            as well. Log-driven development -- the development process in which code is monitored using metrics, alerts,
            and logs -- is gaining traction within more and more R&amp;D teams, and it would not be a stretch of the imagination
            to tie this to the growing popularity of ELK.</p>
        <p>Of course, no system is perfect and there are pitfalls that users need to avoid, especially when handling big production
            operations. But this should not deter you from trying it out, especially because there are numerous sources of
            information that will guide you through the process.</p>
        <p>Good luck, and happy indexing!</p>
        <p>
            <em>This article was
                <a href="http://www.sitepoint.com/introduction-to-sitepoints-peer-review/">peer reviewed</a> by
                <a href="http://www.sitepoint.com/author/cthomas/">Christopher Thomas</a>,
                <a href="http://www.sitepoint.com/author/yrafie/">Younes Rafie</a>, and
                <a href="http://www.sitepoint.com/author/smolinari/">Scott Molinari</a>. Thanks to all of SitePoint’s peer reviewers for making SitePoint content the best it
                can be!</em>
        </p>
    </div>
    <div class="chapter">
        <div class="ch-head">Chapter</div>
        <h1 class="chaptertitle"> Web App Performance Testing with Siege: Plan, Test, Learn</h1>
        <h3 class="author">Zoran Antolovic</h3>
        <p>
            <em>This article was peer reviewed by
                <a href="https://github.com/Hywan">Ivan Enderlin</a> and
                <a href="https://www.sitepoint.com/author/wancheta">Wern Ancheta</a>. Thanks to all of SitePoint’s peer reviewers for making SitePoint content the best it can
                be!</em>
        </p>
        <p>
            <strong>Building a simple web application today isn’t that hard. The web development community is friendly, and there
                are lots of discussions on Stack Overflow or similar platforms, and various sites with
                <a href="https://www.sitepoint.com/premium/topics/all?q=&amp;limit=24">lessons and tutorials</a>.</strong>
        </p>
        <p>Almost anyone can
            <a href="http://bit.ly/phpenv-spp">build an app locally</a>, deploy it to a server, and proudly show it to your friends. I hope you’ve already done
            all of this, and your project went viral, so you’re obviously here because you want to learn how to make sure
            your app is ready for some high traffic.</p>
        <p>If we think about a web app as a black box, things are quite simple: the app waits for a request, processes it, and
            returns the response presentation of a resource (HTML, JSON, XML, etc.). One could think: “Yeah, that’s simple,
            we should be able to scale our app with ease.” Sadly, the world of web development ain&#39;t all sunshine and
            rainbows, and you’ll encounter a lot of performance issues while your traffic grows! You&#39;ll learn and improve
            both your skills and the app over time. In this article, designed to speed this process up, I&#39;ll cover the
            basic concepts of testing the app (regression, load, and stress testing) with
            <a href="https://www.joedog.org/siege-home/">Siege</a> and some tips and tricks I like to use when I’m testing my own web apps.</p>
        <p>
            <img src="../images/1495571919Fotolia_119566077_Subscription_Monthly_M-1024x1024.jpg" alt="Vector image of catapult">
        </p>
        <h2 id="the-types-of-testing">The Types of Testing</h2>
        <p>Let’s say we want to achieve the daily goal of 1 million unique users. How should we prepare for that amount of traffic?
            How do we make sure nothing will break under normal traffic or peaks? This type of testing is called
            <em>load</em> testing, since we know exactly how much traffic we want our app to endure --- the load. If you want
            to push your app to its limits and beyond until it breaks, you’re
            <em>stress</em> testing your app.</p>
        <p>In addition to those, are you aware of how code changes you’ve deployed might affect performance? One simple update
            can degrade or improve the performance of a high traffic web app to a large extent, and you won’t even know what
            happened or why until the storm is over. To make sure an app is performing the same before and after a change,
            we’re doing
            <em>regression</em> testing.</p>
        <p>Another great motivation for regression testing is infrastructure change: for whatever reason, you might want to
            move from provider A to provider B (or switch from Apache to nginx). You know that your app is usually handling
            N requests per minute (on average) and what its normal traffic is (a quick look at analytics would do the job).
            You’re expecting that your app will behave the same (or better) once deployed provider B&#39;s server. Are you
            sure, though? Would you take that risk? You already have all the data you need, don’t guess! Test your new setup
            before deploying and sleep better!</p>
        <p>Before you start randomly hitting your app with virtual requests, you should know that testing is not an easy job,
            and the numbers you’ll get from Siege or any other testing tool should be used
            <strong>as a reference to analyze relative changes</strong>. Running both Siege and the app locally for five minutes
            and concluding your app can handle a few hundred or thousand requests within a few seconds is not something I’d
            recommend.</p>
        <h2 id="the-steps-for-successful-testing">The Steps for Successful Testing</h2>
        <ul>
            <li>
                <p>Plan</p>
                <p>Think about what you want to test. and what you expect. How much traffic, on which URLs, with what payload?
                    Define parameters up front, don’t just randomly hit your app.</p>
            </li>
            <li>
                <p>Prepare</p>
                <p>Make sure your environments are as isolated as possible: use the same environment for testing, for every
                    test run. A good guide on how to accomplish this can be found in
                    <a href="http://bit.ly/phpenv-spp">this book about setting up PHP environments</a>.</p>
            </li>
            <li>
                <p>Analyze and Learn</p>
                <p>Learn something from the numbers and make educated decisions. Results should always be evaluated within their
                    context: don’t jump to conclusions; check everything at least twice.</p>
            </li>
        </ul>
        <h2 id="getting-started-with-siege">Getting Started with Siege</h2>
        <p>
            <a href="https://github.com/JoeDog/siege">Siege</a> is an awesome tool for benchmarking and testing web apps. It simulates concurrent users requesting
            resources at a given URL (or multiple URLs) and lets the user heavily customize the testing parameters. Run
            <code>siege --help</code> to see all available options; we’ll cover some of them in detail below.</p>
        <h3 id="preparing-the-test-app">Preparing the Test App</h3>
        <p>With Siege, you can test an app&#39;s stability, performance, and improvements between code (or infrastructure) changes.
            You can also use it to make sure your Wordpress website can handle the peak you’re expecting after publishing
            a viral photo of a cat, or to set up and evaluate the benefits of an
            <a href="https://www.sitepoint.com/getting-started-with-varnish/">HTTP cache system such as Varnish</a>.</p>
        <p>For the tests in this article, I&#39;ll be using a slightly modified
            <a href="http://symfony.com/blog/introducing-the-symfony-demo-application">Symfony Demo application</a> deployed to one Digital Ocean node in Frankfurt, and SIEGE 4.0.2 installed on a
            second Digital Ocean node in New York.</p>
        <p>As I said earlier, it’s crucial to have both the app and test server isolated whenever possible. If you’re running
            any of them on your local machine, you can’t guarantee the same environment because there are other processes
            (email client, messaging tools, daemons) running which may affect performance; even with high quality virtual
            machines like
            <a href="http://www.sitepoint.com/quick-tip-get-homestead-vagrant-vm-running/">Homestead Improved</a>, resource availability isn&#39;t 100% guaranteed (though these isolated VMs are a valid
            option if you don&#39;t feel like spending money on the load testing phase of your app).</p>
        <p>The Symfony Demo application is pretty simple and fast when used out of the box. In real life, we’re dealing with
            complex and slow apps, so I decided to add two modules to the sidebar of a single post page:
            <strong>Recent posts</strong> (10 latest posts) and
            <strong>Popular posts</strong> (10 posts with most comments). By doing so, I’ve added more complexity to the app which
            is now querying the DB at least three times. The idea is to get as real a situation as possible. The database
            has been populated with 62,230 dummy articles and ~1,445,505 comments.</p>
        <h3 id="learning-the-basics">Learning the Basics</h3>
        <p>Running the command
            <code>siege SOME-URL</code> will make Siege start testing the URL with default parameters. After the initial message
            …</p>
        <pre><code class="lang-bash">** Preparing 25 concurrent users for battle.
The server is now under siege…
</code></pre>
        <p>… the screen will start to fill with information about sent requests. Your immediate reflex would probably be to
            stop execution by pressing
            <code>CTRL + C</code>, at which point it will stop testing and give output results.</p>
        <p>Before we go on, there is one thing you should keep in mind when testing/benchmarking a web app. Think about the
            lifecycle of a single HTTP request sent towards the Symfony demo app blog page ---
            <code>/en/blog/</code>. The server will generate an HTTP response with a status 200 (OK) and HTML in the body with
            content and references to images and other assets (stylesheets, JavaScript files, …). The web browser processes
            those references, and requests all assets needed to render a web page in the background. How many HTTP requests
            in total do we need?</p>
        <p>To get the answer, let&#39;s ask Siege to run a single test and analyze the result. I’m having my app&#39;s access
            log open (
            <code>tail -f var/logs/access.log</code>) in the terminal as I run
            <code>siege -c=1 --reps=1 http://sfdemo.loc/en/blog/</code>. Basically, I’m telling Siege: “Run test once (--reps=1)
            with one user (-c=1) for URL
            <a href="http://sfdemo.loc/en/blog/”">http://sfdemo.loc/en/blog/”</a>. I can see the requests in both the log and Siege&#39;s output.</p>
        <pre><code class="lang-bash">siege -c=1 --reps=1 http://sfdemo.loc/en/blog/

** Preparing 1 concurrent users for battle.
The server is now under siege...
HTTP/1.1 200     1.85 secs:   22367 bytes ==&gt; GET  /en/blog/
HTTP/1.1 200     0.17 secs:    2317 bytes ==&gt; GET  /js/main.js
HTTP/1.1 200     0.34 secs:   49248 bytes ==&gt; GET  /js/bootstrap-tagsinput.min.js
HTTP/1.1 200     0.25 secs:   37955 bytes ==&gt; GET  /js/bootstrap-datetimepicker.min.js
HTTP/1.1 200     0.26 secs:   21546 bytes ==&gt; GET  /js/highlight.pack.js
HTTP/1.1 200     0.26 secs:   37045 bytes ==&gt; GET  /js/bootstrap-3.3.7.min.js
HTTP/1.1 200     0.44 secs:  170649 bytes ==&gt; GET  /js/moment.min.js
HTTP/1.1 200     0.36 secs:   85577 bytes ==&gt; GET  /js/jquery-2.2.4.min.js
HTTP/1.1 200     0.16 secs:    6160 bytes ==&gt; GET  /css/main.css
HTTP/1.1 200     0.18 secs:    4583 bytes ==&gt; GET  /css/bootstrap-tagsinput.css
HTTP/1.1 200     0.17 secs:    1616 bytes ==&gt; GET  /css/highlight-solarized-light.css
HTTP/1.1 200     0.17 secs:    7771 bytes ==&gt; GET  /css/bootstrap-datetimepicker.min.css
HTTP/1.1 200     0.18 secs:     750 bytes ==&gt; GET  /css/font-lato.css
HTTP/1.1 200     0.26 secs:   29142 bytes ==&gt; GET  /css/font-awesome-4.6.3.min.css
HTTP/1.1 200     0.44 secs:  127246 bytes ==&gt; GET  /css/bootstrap-flatly-3.3.7.min.css

Transactions:                  15 hits
Availability:              100.00 %
Elapsed time:                5.83 secs
Data transferred:            0.58 MB
Response time:                0.37 secs
Transaction rate:            2.57 trans/sec
Throughput:                0.10 MB/sec
Concurrency:                0.94
Successful transactions:          15
Failed transactions:               0
Longest transaction:            1.85
Shortest transaction:            0.16
</code></pre>
        <p>The access log looks like this:</p>
        <pre><code class="lang-shell">107.170.85.171 - - [04/May/2017:05:35:15 +0000] &quot;GET /en/blog/ HTTP/1.1&quot; 200 22701 &quot;-&quot; &quot;Mozilla/5.0 (unknown-x86_64-linux-gnu) Siege/4.0.2&quot;
107.170.85.171 - - [04/May/2017:05:35:17 +0000] &quot;GET /js/main.js HTTP/1.1&quot; 200 2602 &quot;-&quot; &quot;Mozilla/5.0 (unknown-x86_64-linux-gnu) Siege/4.0.2&quot;
107.170.85.171 - - [04/May/2017:05:35:17 +0000] &quot;GET /js/bootstrap-tagsinput.min.js HTTP/1.1&quot; 200 49535 &quot;-&quot; &quot;Mozilla/5.0 (unknown-x86_64-linux-gnu) Siege/4.0.2&quot;
107.170.85.171 - - [04/May/2017:05:35:17 +0000] &quot;GET /js/bootstrap-datetimepicker.min.js HTTP/1.1&quot; 200 38242 &quot;-&quot; &quot;Mozilla/5.0 (unknown-x86_64-linux-gnu) Siege/4.0.2&quot;
107.170.85.171 - - [04/May/2017:05:35:18 +0000] &quot;GET /js/highlight.pack.js HTTP/1.1&quot; 200 21833 &quot;-&quot; &quot;Mozilla/5.0 (unknown-x86_64-linux-gnu) Siege/4.0.2&quot;
107.170.85.171 - - [04/May/2017:05:35:18 +0000] &quot;GET /js/bootstrap-3.3.7.min.js HTTP/1.1&quot; 200 37332 &quot;-&quot; &quot;Mozilla/5.0 (unknown-x86_64-linux-gnu) Siege/4.0.2&quot;
107.170.85.171 - - [04/May/2017:05:35:18 +0000] &quot;GET /js/moment.min.js HTTP/1.1&quot; 200 170938 &quot;-&quot; &quot;Mozilla/5.0 (unknown-x86_64-linux-gnu) Siege/4.0.2&quot;
107.170.85.171 - - [04/May/2017:05:35:19 +0000] &quot;GET /js/jquery-2.2.4.min.js HTTP/1.1&quot; 200 85865 &quot;-&quot; &quot;Mozilla/5.0 (unknown-x86_64-linux-gnu) Siege/4.0.2&quot;
107.170.85.171 - - [04/May/2017:05:35:19 +0000] &quot;GET /css/main.css HTTP/1.1&quot; 200 6432 &quot;-&quot; &quot;Mozilla/5.0 (unknown-x86_64-linux-gnu) Siege/4.0.2&quot;
107.170.85.171 - - [04/May/2017:05:35:19 +0000] &quot;GET /css/bootstrap-tagsinput.css HTTP/1.1&quot; 200 4855 &quot;-&quot; &quot;Mozilla/5.0 (unknown-x86_64-linux-gnu) Siege/4.0.2&quot;
107.170.85.171 - - [04/May/2017:05:35:19 +0000] &quot;GET /css/highlight-solarized-light.css HTTP/1.1&quot; 200 1887 &quot;-&quot; &quot;Mozilla/5.0 (unknown-x86_64-linux-gnu) Siege/4.0.2&quot;
107.170.85.171 - - [04/May/2017:05:35:20 +0000] &quot;GET /css/bootstrap-datetimepicker.min.css HTTP/1.1&quot; 200 8043 &quot;-&quot; &quot;Mozilla/5.0 (unknown-x86_64-linux-gnu) Siege/4.0.2&quot;
107.170.85.171 - - [04/May/2017:05:35:20 +0000] &quot;GET /css/font-lato.css HTTP/1.1&quot; 200 1020 &quot;-&quot; &quot;Mozilla/5.0 (unknown-x86_64-linux-gnu) Siege/4.0.2&quot;
107.170.85.171 - - [04/May/2017:05:35:20 +0000] &quot;GET /css/font-awesome-4.6.3.min.css HTTP/1.1&quot; 200 29415 &quot;-&quot; &quot;Mozilla/5.0 (unknown-x86_64-linux-gnu) Siege/4.0.2&quot;
107.170.85.171 - - [04/May/2017:05:35:20 +0000] &quot;GET /css/bootstrap-flatly-3.3.7.min.css HTTP/1.1&quot; 200 127521 &quot;-&quot; &quot;Mozilla/5.0 (unknown-x86_64-linux-gnu) Siege/4.0.2&quot;
</code></pre>
        <p>We can see that even though we told Siege to test the URL once, 15 transactions (requests) were executed. If you’re
            wondering what a transaction is, you should check out
            <a href="https://github.com/JoeDog/siege">the GitHub page</a>:</p>
        <blockquote>
            <p>A transaction is characterized by the server opening a socket for the client, handling a request, serving data
                over the wire and closing the socket upon completion.</p>
        </blockquote>
        <p>Siege thus won’t only send a single HTTP GET request for the URL provided, it will generate HTTP GET requests for
            all related assets referenced by the resource at the given URL. We should still be aware that Siege isn’t evaluating
            JavaScript, therefore AJAX requests aren’t included here. We should also keep in mind that browsers are capable
            of caching static files (images, fonts, JS files).</p>
        <p>This behavior can be changed since
            <a href="https://www.joedog.org/2016/03/10/siege-release-4-0-0/">version 4.0</a> by updating the Siege configuration file located at
            <code>~/.siege/siege.conf</code> and setting
            <code>parser = false</code>.</p>
        <p>
            <em>
                <strong>Note:</strong> Default behavior may be different depending on the version of Seige you’re using, or even
                on the tool. If you’re using something other than Siege, check what exactly it considers to be a single test
                (is it single request for given URL, or a request for a given URL and sub-requests for all its resources?)
                before you come to conclusions.</em>
        </p>
        <p>From the test output above, we can see that Siege has generated 15 requests (transactions) in ~6 seconds, resulting
            in 0.58 MB of transferred data with 100% availability or 15/15 successful transactions - “
            <em>Successful transactions is the number of times the server returned a code less than 400. Accordingly, redirects
                are considered successful transactions</em>”.</p>
        <p>Response time is the average time needed to complete all requests and get responses. Transaction rate and throughput
            are telling us what the capacity of our app is (how much traffic our app can handle at a given time).</p>
        <p>Let’s repeat the test with 15 users:</p>
        <pre><code class="lang-bash">siege --concurrent=15 --reps=1 sfdemo.loc/en/blog/

Transactions:                 225 hits
Availability:              100.00 %
Elapsed time:                6.16 secs
Data transferred:            8.64 MB
Response time:                0.37 secs
Transaction rate:           36.53 trans/sec
Throughput:                1.40 MB/sec
Concurrency:               13.41
Successful transactions:         225
Failed transactions:               0
Longest transaction:            1.74
Shortest transaction:            0.16
</code></pre>
        <p>By increasing the test load, we’re allowing our app to show its full power. We can see that our app can handle a
            single request from 15 users on a blog page very well, with an average response time of 0.37 seconds. Siege by
            default delays requests randomly in the interval from 1 to 3 seconds. By setting the
            <code>--delay=N</code> parameter, we can affect the randomness of delays between requests (setting the maximum delay).</p>
        <h3 id="concurrency">Concurrency</h3>
        <p>Concurrency is probably the most confusing result attribute, so let&#39;s explain it. The documentation says:</p>
        <blockquote>
            <p>Concurrency is an average number of simultaneous connections, a number which rises as server performance decreases.</p>
        </blockquote>
        <p>From the
            <a href="https://www.joedog.org/siege-faq/#a17a">FAQ</a> section, we can see how concurrency is calculated:</p>
        <blockquote>
            <p>Concurrency is the total transactions divided by total elapsed time. So if we completed 100 transactions in 10
                seconds, our concurrency was 10.00.</p>
        </blockquote>
        <p>Another great explanation of concurrency is available on the
            <a href="https://www.joedog.org/2012/02/17/concurrency-single-siege/">official website</a>:</p>
        <blockquote>
            <p>We can illustrate this point with an obvious example. I ran Siege against a two-node clustered website. My concurrency
                was
                <strong>6.97</strong>. Then I took a node away and ran the same run against the same page. My concurrency rose to
                <strong>18.33</strong>. At the same time, my elapsed time was extended
                <strong>65%</strong>.</p>
        </blockquote>
        <p>Let’s look at it from a different perspective: if you’re a restaurant owner aiming to measure business performance
            before doing some changes, you could measure the
            <strong>average number of open orders</strong> (i.e. orders waiting to be delivered --- requests) over time. In the first
            example above, an average nu
            <strong>mb</strong>er of open orders was
            <strong>7</strong>, but if you fire half of your kitchen staff (i.e. take a node away), your concurrency will rise to
            <strong>18</strong>. Remember, we’re expecting tests to be conducted in an identical environment, so the number of guests
            and intensity of ordering should be the same. Waiters can accept orders at a high rate (like web servers can)
            but processing time is slow, and your kitchen staff (your app) is overloaded and sweating to deliver orders.</p>
        <h2 id="performance-testing-with-siege">Performance Testing with Siege</h2>
        <p>To get a real overview of our app’s performance, I&#39;ll run Siege for 5 minutes with a different number of concurrent
            users and compare the results. Since the blog home page is a simple endpoint with a single database query, I&#39;ll
            be testing single post pages in the following tests as they are slower and more complex.</p>
        <pre><code class="lang-bash">siege --concurrent=5 --time=5M http://sfdemo.loc/en/blog/posts/vero-iusto-fugit-sed-totam.\`
</code></pre>
        <p>While tests are running, I can take a look at
            <code>top</code> on my app&#39;s server to see the status. MySQL is working hard:</p>
        <pre><code>%CPU %MEM     TIME+ COMMAND
96.3 53.2   1:23.80 mysqld
</code></pre>
        <p>That was expected, since every time the app renders a single post page, it’s executing several non-trivial DB queries:</p>
        <ol>
            <li>Fetch article with associated comments.</li>
            <li>Fetch top 10 posts sorted descending by publication time.</li>
            <li>A
                <code>SELECT</code> query with a join between posts and the large comments table with
                <code>COUNT</code> to get the 10 most popular articles.</li>
        </ol>
        <p>The first test with five concurrent users is done, and the numbers aren’t that impressive:</p>
        <pre><code class="lang-bash">siege --concurrent=5 --time=5M http://sfdemo.loc/en/blog/posts/vero-iusto-fugit-sed-totam.

Transactions:                1350 hits
Availability:              100.00 %
Elapsed time:              299.54 secs
Data transferred:           51.92 MB
Response time:                1.09 secs
Transaction rate:            4.51 trans/sec
Throughput:                0.17 MB/sec
Concurrency:                4.91
Successful transactions:        1350
Failed transactions:               0
Longest transaction:           15.55
Shortest transaction:            0.16
</code></pre>
        <p>Siege was able to get 1350 transactions complete in 5 minutes. Since we have 15 transactions/page load, we can easily
            calculate that our app was able to handle 90 page loads within 5 minutes or 18 page loads / 1 minute or
            <strong>0,3 page loads / second</strong>. We calculate the same by dividing the transaction rate with a number of transactions
            per page 4,51 / 15 = 0,3.</p>
        <p>Well… that’s not such a great throughput, but at least now we know where the bottlenecks are (DB queries) and we
            have reference to compare with once we optimize our app.</p>
        <p>Let’s run a few more tests to see how our app works under more pressure. This time we have the concurrent users set
            to 10 and within the first few minutes of testing we can see lots of HTTP 500 errors: the app started to fall
            apart under slightly bigger traffic. Let’s now compare how the app is performing under siege with 5, 10 and 15
            concurrent users.</p>
        <pre><code class="lang-bash">siege --concurrent=10 --time=5M http://sfdemo.loc/en/blog/posts/vero-iusto-fugit-sed-totam.

Lifting the server siege…
Transactions:                 450 hits
Availability:               73.89 %
Elapsed time:              299.01 secs
Data transferred:           18.23 MB
Response time:                6.17 secs
Transaction rate:            1.50 trans/sec
Throughput:                0.06 MB/sec
Concurrency:                9.29
Successful transactions:         450
Failed transactions:             159
Longest transaction:           32.68
Shortest transaction:            0.16
</code></pre>
        <pre><code class="lang-bash">siege --concurrent=10 --time=5M http://sfdemo.loc/en/blog/posts/vero-iusto-fugit-sed-totam.

Transactions:                   0 hits
Availability:                0.00 %
Elapsed time:              299.36 secs
Data transferred:            2.98 MB
Response time:                0.00 secs
Transaction rate:            0.00 trans/sec
Throughput:                0.01 MB/sec
Concurrency:               14.41
Successful transactions:           0
Failed transactions:             388
Longest transaction:           56.85
Shortest transaction:            0.00
</code></pre>
        <p>
            <img src="../images/1495631993siege-comparison-no-cache.png" alt="siege-comparison-no-cache">
        </p>
        <p>
            <em>Siege result comparison with concurrency set to 5, 10 and 15</em>
        </p>
        <p>Notice how concurrency rose as our app&#39;s performance was dropping. The app was completely dead under siege with
            15 concurrent users --- i.e. 15 wild users is all it takes to take your fortress down! We’re engineers, and we’re
            not going to cry over challenges, we’re going to solve them!</p>
        <p>Keep in mind that these tests are automated, and we’re putting our app under pressure. In reality, users aren’t just
            hitting the refresh button like maniacs, they are processing (i.e. reading) the content you present and therefore
            some delay between requests exists.</p>
        <h2 id="cache-to-the-rescue">Cache to the Rescue</h2>
        <p>We’re now aware of some problems with our app --- we’re querying the database too much. Do we really need to get
            the list of popular and recent articles from the database on every single request? Probably not, so we can add
            a cache layer at the application level (e.g. Redis) and cache a list of popular and recent articles. This article
            isn’t about caching (for that, see
            <a href="https://www.sitepoint.com/speeding-up-existing-apps-with-a-redis-cache/">this one</a>), so we’re going to a add full response cache for the single post page.</p>
        <p>The demo app already comes with
            <a href="http://symfony.com/doc/current/http_cache.html">Symfony HTTP Cache</a> enabled, we just need to set the TTL header for the HTTP response we’re returning.</p>
        <pre><code class="lang-php">$response-&gt;setTtl(60);
</code></pre>
        <p>Let’s repeat the tests with 5, 10, and 15 concurrent users and see how adding a cache affects the performance. Obviously,
            we’re expecting app performance to increase after the cache warms up. We’re also waiting at least 1 minute for
            the cache to expire between the tests.</p>
        <p>
            <strong>Note:</strong> Be careful with caching, especially IRT protected areas of a web app (
            <a href="https://steamdb.info/blog/recent-caching-issues-on-steam/">oops example</a>) and always remember that it&#39;s
            <a href="https://martinfowler.com/bliki/TwoHardThings.html">one of the two hard things about computer science</a>.</p>
        <p>On to results: by adding 60s of cache, the app&#39;s stability and performance improved dramatically. Take a look
            at the results in the table and charts below.</p>
        <table>

            <thead>

                <tr>

                    <th>C=5</th>

                    <th>C=10</th>

                    <th>C=15</th>

                </tr>

            </thead>

            <tbody>

                <tr>

                    <td>Transactions</td>

                    <td>4566 hits</td>

                    <td>8323 hits</td>

                    <td>12064 hits</td>

                </tr>

                <tr>

                    <td>Availability</td>

                    <td>100.00 %</td>

                    <td>100.00 %</td>

                    <td>100.00 %</td>

                </tr>

                <tr>

                    <td>Elapsed time</td>

                    <td>299.86 secs</td>

                    <td>299.06 secs</td>

                    <td>299.35 secs</td>

                </tr>

                <tr>

                    <td>Data transferred</td>

                    <td>175.62 MB</td>

                    <td>320.42 MB</td>

                    <td>463.78 MB</td>

                </tr>

                <tr>

                    <td>Response time</td>

                    <td>0.31 secs</td>

                    <td>0.34 secs</td>

                    <td>0.35 secs</td>

                </tr>

                <tr>

                    <td>Transaction rate</td>

                    <td>15.23 trans/sec</td>

                    <td>27.83 trans/sec</td>

                    <td>40.30 trans/sec</td>

                </tr>

                <tr>

                    <td>Throughput</td>

                    <td>0.59 MB/sec</td>

                    <td>1.07 MB/sec</td>

                    <td>1.55 MB/sec</td>

                </tr>

                <tr>

                    <td>Concurrency</td>

                    <td>4.74</td>

                    <td>9.51</td>

                    <td>14.31</td>

                </tr>

                <tr>

                    <td>Successful transactions</td>

                    <td>4566</td>

                    <td>8323</td>

                    <td>12064</td>

                </tr>

                <tr>

                    <td>Failed transactions</td>

                    <td>0</td>

                    <td>0</td>

                    <td>0</td>

                </tr>

                <tr>

                    <td>Longest transaction</td>

                    <td>4.32</td>

                    <td>5.73</td>

                    <td>4.93</td>

                </tr>

            </tbody>

        </table>

        <p>
            <em>Siege of a single post URL with HTTP cache ttl set to 60 seconds</em>
        </p>
        <p>
            <img src="../images/1495631991siege-transaction-rate.png" alt="Transactions before and after adding cache">
        </p>
        <p>
            <em>The application was able to handle way more transactions with cache</em>
        </p>
        <p>
            <img src="../images/1495631992siege-response.png" alt="Response time before and after adding cache">
        </p>
        <p>
            <em>The response time after adding cache was decreased and stable regardless of traffic, as expected</em>
        </p>
        <h3 id="the-real-feel">The Real Feel</h3>
        <p>If you want to get the real feel of using your app when it is under pressure, you can run Siege and use your app
            in the browser. Siege will put the app under pressure, and you’ll be able to the actual user experience. Even
            though this is a subjective method, I think it’s an eye-opening experience for a majority of developers. Try
            it.</p>
        <h2 id="alternative-tools">Alternative Tools</h2>
        <p>Siege isn’t the only tool for load testing and benchmarking of web apps. Let’s quickly test the app with ab.</p>
        <h3 id="ab">Ab</h3>
        <p>ab or
            <a href="http://httpd.apache.org/docs/current/programs/ab.html">Apache HTTP server benchmarking tool</a> is another great tool. It is well documented and has lots of options,
            though it doesn’t support using URL files, parsing, and requesting referenced assets, nor random delays like
            Siege does.</p>
        <p>If I run ab against single post page (without cache), the result is:</p>
        <pre><code class="lang-shell">ab -c 5 -t 300 http://sfdemo.loc/en/blog/posts/vero-iusto-fugit-sed-totam.
This is ApacheBench, Version 2.3 &lt;$Revision: 1706008 $&gt;
Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
Licensed to The Apache Software Foundation, http://www.apache.org/

Benchmarking sfdemo.loc (be patient)
Finished 132 requests

Server Software:        Apache/2.4.18
Server Hostname:        sfdemo.loc
Server Port:            80

Document Path:          /en/blog/posts/vero-iusto-fugit-sed-totam.
Document Length:        23291 bytes

Concurrency Level:      5
Time taken for tests:   300.553 seconds
Complete requests:      132
Failed requests:        0
Total transferred:      3156000 bytes
HTML transferred:       3116985 bytes
Requests per second:    0.44 [#/sec] (mean)
Time per request:       11384.602 [ms] (mean)
Time per request:       2276.920 [ms] (mean, across all concurrent requests)
Transfer rate:          10.25 [Kbytes/sec] received

Connection Times (ms)
                min  mean[+/-sd] median   max
Connect:       81   85   2.0     85      91
Processing:  9376 11038 1085.1  10627   13217
Waiting:     9290 10953 1084.7  10542   13132
Total:       9463 11123 1085.7  10712   13305

Percentage of the requests served within a certain time (ms)
    50%  10712
    66%  11465
    75%  12150
    80%  12203
    90%  12791
    95%  13166
    98%  13302
    99%  13303
    100%  13305 (longest request)
</code></pre>
        <p>And after we turn cache on, the result is:</p>
        <pre><code class="lang-shell">
ab -c 5 -t 300 http://sfdemo.loc/en/blog/posts/vero-iusto-fugit-sed-totam.
This is ApacheBench, Version 2.3 &lt;$Revision: 1706008 $&gt;
Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
Licensed to The Apache Software Foundation, http://www.apache.org/

Benchmarking sfdemo.loc (be patient)
Completed 5000 requests
Finished 5373 requests

Server Software:        Apache/2.4.18
Server Hostname:        sfdemo.loc
Server Port:            80

Document Path:          /en/blog/posts/vero-iusto-fugit-sed-totam.
Document Length:        23351 bytes

Concurrency Level:      5
Time taken for tests:   300.024 seconds
Complete requests:      5373
Failed requests:        0
Total transferred:      127278409 bytes
HTML transferred:       125479068 bytes
Requests per second:    17.91 [#/sec] (mean)
Time per request:       279.196 [ms] (mean)
Time per request:       55.839 [ms] (mean, across all concurrent requests)
Transfer rate:          414.28 [Kbytes/sec] received

Connection Times (ms)
                min  mean[+/-sd] median   max
Connect:       81   85   2.1     85     106
Processing:   164  194 434.8    174   13716
Waiting:       83  109 434.8     89   13632
Total:        245  279 434.8    259   13803

Percentage of the requests served within a certain time (ms)
    50%    259
    66%    262
    75%    263
    80%    265
    90%    268
    95%    269
    98%    272
    99%    278
    100%  13803 (longest request)
</code></pre>
        <p>I love the way ab shows the timing breakdown and stats in the report! I can immediately see that 50% of our requests
            were served under 259ms (vs 10.712ms without cache) and 99% of them were under 278ms (vs 13.305ms without cache)
            which is acceptable. Again, the test results are evaluated within their context and relative to the previous
            state.</p>
        <h2 id="advanced-load-testing-with-siege">Advanced Load Testing with Siege</h2>
        <p>Now that we have the basics of load and regression testing covered, it is time to take the next step. So far, we
            were hitting single URLs with generated requests, and we saw that once the response is cached, our app can handle
            lots of traffic with ease.</p>
        <p>In real life, things are a bit more complex: users randomly navigate through the site, visiting URLs and processing
            the content. The thing I love the most about Siege is the possibility of using a URL file into which I can place
            multiple URLs to be randomly used during the test.</p>
        <h3 id="step-1-plan-the-test">Step 1: Plan the Test</h3>
        <p>We need to conduct the relevant test on a list of top URLs visited by our users. The second thing we should consider
            is the dynamic of users’ behavior --- i.e. how fast they click links.</p>
        <p>I would create a URL file based on data from my analytics tool or server access log file. One could use access log
            parsing tools such as
            <a href="https://github.com/kassner/log-parser">Web server access Log Parser</a> to parse Apache access logs, and generate a list of URLs sorted by popularity.
            I would take the top N (20, 50, 100 …) URLs and place them in the file. If some of the URLs (e.g. landing page
            or viral article) are visited more often than others, we should adjust the probabilities so that siege requests
            those URLs more often.</p>
        <p>Let say we have following URLs with these visit counts over the last N days:</p>
        <ul>
            <li>Landing page / Home page - 30.000</li>
            <li>Article A - 10.000</li>
            <li>Article B - 2.000</li>
            <li>Article C - 50.000</li>
            <li>About us - 3.000</li>
        </ul>
        <p>We can normalize the visits count and get a list like this:</p>
        <ul>
            <li>Landing page / Home page - 32% (30.000 / 95.000)</li>
            <li>Article A - 11% (10.000 / 95.000)</li>
            <li>Article B - 2% (2.000 / 95.000)</li>
            <li>Article C - 52% (50.000 / 95.000)</li>
            <li>About us - 3% (3.000 / 95.000)</li>
        </ul>
        <p>Now we can create a URL file with 100 URLs (lines) with 32 x Homepage URLs, 52 x Article C URLs etc. You can shuffle
            the final file to get more randomness, and save it.</p>
        <p>Take the average session time and pages per session from your analytics tool to calculate the average delay between
            two requests. If average session duration is 2 minutes and users are visiting 8 pages per session on average,
            simple math gives us an average delay of 15 seconds (120 seconds / 8 pages = 15 seconds / page).</p>
        <p>Finally, we disable parsing and requesting of assets as I am caching static files in production and serving them
            from a different server. As mentioned above, the parser is turned off by setting
            <code>parser = false</code> in Siege&#39;s config located at
            <code>~/.siege/siege.conf</code>
        </p>
        <h3 id="step-2-prepare-and-run-tests">Step 2: Prepare and Run Tests</h3>
        <p>Since we’re now dealing with randomness, it would be a good idea to increase the duration of test so that we get
            more relevant results. I will be running Siege for 20 minutes with a maximum delay set to 15 seconds and 50 concurrent
            users. I will test the blog homepage and 10 articles with different probabilities.</p>
        <p>Since I don’t expect that amount of traffic to hit the app with an empty cache, I will warm up the app’s cache by
            requesting every URL at least once before testing with</p>
        <pre><code class="lang-bash">siege -b --file=urls.txt -t 30S -c 1
</code></pre>
        <p>Now we’re ready to put our app under some serious pressure. If we use the
            <code>--internet</code> switch, Siege will select URL from the file randomly. Without the switch, Siege is selecting
            URLs sequentially. Let’s start:</p>
        <pre><code class="lang-bash">siege --file=urls.txt --internet --delay=15 -c 50 -t 30M

Lifting the server siege...
Transactions:               10931 hits
Availability:               98.63 %
Elapsed time:             1799.88 secs
Data transferred:          351.76 MB
Response time:                0.67 secs
Transaction rate:            6.07 trans/sec
Throughput:                0.20 MB/sec
Concurrency:                4.08
Successful transactions:       10931
Failed transactions:             152
Longest transaction:           17.71
Shortest transaction:            0.24
</code></pre>
        <p>Or with 60 concurrent users during the siege:</p>
        <pre><code class="lang-bash">siege --file=urls.txt --delay=15 -c 60 -t 30M
Transactions:               12949 hits
Availability:               98.10 %
Elapsed time:             1799.20 secs
Data transferred:          418.04 MB
Response time:                0.69 secs
Transaction rate:            7.20 trans/sec
Throughput:                0.23 MB/sec
Concurrency:                4.99
Successful transactions:       12949
Failed transactions:             251
Longest transaction:           15.75
Shortest transaction:            0.21
</code></pre>
        <p>We can see that the modified Symfony Demo app (with cache turned on) handled tests pretty well. On average, it was
            able to serve
            <strong>7.2 requests per second</strong> within
            <strong>0.7 seconds</strong> (note that we’re using a single core Digital Ocean droplet with only 512 MB of RAM). The
            availability was
            <strong>98.10%</strong> due to 251 out of 13.200 requests having failed (the connection with the DB failed a few times).</p>
        <h2 id="submitting-data-with-siege">Submitting Data with Siege</h2>
        <p>So far we’ve been sending only HTTP GET requests, which is usually enough to get an overview of an app’s performance.
            Often, it makes sense to submit data during load tests (e.g. testing API endpoints). With Siege, you can easily
            send data to your endpoint:</p>
        <pre><code class="lang-bash">siege --reps=1 -c 1 &#39;http://sfdemo.loc POST foo=bar&amp;baz=bash&#39;
</code></pre>
        <p>You can also send the data in JSON format. By using
            <code>--content-type</code> parameter, we can specify the content type of a request.</p>
        <pre><code class="lang-bash">siege --reps=1 -c 1 --content-type=&quot;application/json&quot; &#39;http://sfdemo.loc POST {&quot;foo&quot;:&quot;bar&quot;,&quot;baz&quot;:&quot;bash&quot;}&#39;
</code></pre>
        <p>We can also change the default user agent with
            <code>--user-agent=&quot;MY USER AGENT&quot;</code> or specify multiple HTTP headers with
            <code>--header=&quot;MY HEADER VALUE&quot;</code>.</p>
        <p>Siege also can read the payload data from a file:</p>
        <pre><code class="lang-bash">cat payload.json
{
    &quot;foo&quot;:&quot;bar&quot;,
    &quot;baz&quot;:&quot;bash&quot;
}

siege --reps=1 -c 1 --content-type=&quot;application/json&quot; ⤶
&#39;http://sfdemo.loc POST &lt; payload.json&#39;
</code></pre>
        <p>You can also send cookies within tests by using the
            <code>--header</code> option:</p>
        <pre><code class="lang-bash">siege --reps=1 -c 1 --content-type=&quot;application/json&quot; ⤶
--header=&quot;Cookie: my_cookie=abc123&quot; &#39;http://sfdemo.loc POST &lt; payload.json&#39;
</code></pre>
        <h2 id="conclusion">Conclusion</h2>
        <p>Siege is a very powerful tool when it comes to load, stress, and regression testing of a web app. There are plenty
            of options you can use to make your tests behave as close as possible to a real life environment, which makes
            Siege my preferred tool over something like ab. You can combine different options Siege provides and even run
            multiple Siege processes in parallel if you want to test your app thoroughly.</p>
        <p>It’s always a good idea to automate the testing process (a simple bash script will do the job) and visualize the
            results. I usually run multiple Siege processes in parallel testing read-only endpoints (i.e. sending only GET
            requests) at a high rate and submitting the data (i.e. posting comments, invalidating the cache, etc.) at a lower
            rate, according to real life ratios. Since you can’t specify dynamic payloads within one Siege test, you can
            set a bigger delay between two requests and run more Siege commands with different parameters.</p>
        <p>I’m also thinking about adding simple load testing to my CI pipeline just to make sure my app’s performance wouldn’t
            drop below an acceptable level for critical endpoints.</p>
    </div>
</body>
<script src="../assets/js/book.js"></script>
<script src="../assets/js/prism.js"></script>

</html>